{"pages":[{"title":"About Chris Albon","loc":"http://chrisalbon.com/pages/about.html","text":"I am a data scientist, and the co-founder and Chief Science Officer at Popily . I am also the co-host of the data science podcast, Partially Derivative . Previously, I led Ushahidi's work on crisis and humanitarian data and launched CrisisNET . Prior to Ushahidi, I was Director of the Governance Project at FrontlineSMS . I earned a Ph.D. in Political Science from the University of California, Davis researching the quantitative impact of civil wars on health care systems. In 2008, I founded Conflict Health , a blog on the defense of health and health workers in armed conflict and political violence. I also wrote for the Daily Dot , United States Naval Institute Blog , TheAtlantic.com, ForeignPolicy.com, UN Dispatch , and elsewhere. I earned a B.A. from the University of Miami, where I triple majored in political science, international studies, and religious studies. I code in R and Python , and write in Markdown . Email: cralbon@gmail.com Twitter: @chrisalbon Curriculum Vitae Education Ph.D., Political Science , University of California, Davis. 2012 Dissertation: \"Civil Wars And Health Systems\", a quantitative analysis of the determinants of rebel and government behavior towards health system destruction and reconstruction using original data Fields: International Relations, Quantitative Methodology, and Epidemiology M.A., Political Science , University of California, Davis. 2010 Thesis: \"U.N. Peace Operations And Public Health After Civil War\" B.A. , University of Miami, Miami, FL. 2006 Triple majored in political science, international studies, and religious studies Experience Co-founder & Chief Science Officer , Popily , 2015 - Present In charge of everything data science and product. Co-founder & Co-host , Partially Derivative , 2014 - Present Co-founded a podcast on data and data science. Volunteer Data Scientist , DataKind , 2015 - Present Director of CrisisNET , Ushahidi , 2014 - 2015 Launched a pipeline for global humanitarian crisis data. Director of Data Projects , Ushahidi , 2013 - 2014 The non-profit's first data science hire; led all data science efforts. Project Director , FrontlineSMS , 2012 - 2013 Led FrontlineSMS's Governance Project, an effort improve the transparency and accountability of governments through mobile technology Contributor , Daily Dot , 2012 - 2013 Write opinion pieces on the politics of data and the internet Contributor , United Nations Dispatch , 2011 - 2013 Write news, opinion, and analysis on global affairs, particularly relating to health during conflict, global health politics, and the role of social media Blogger , Conflict Health , 2008 - 2012 Designed and launched blog on defending health and health workers against persecution, violence, and armed conflict Wrote 485 posts over four years Cited by major publications including The Atlantic, Harpers, Wired, The Economist, Time, The Guardian, and The American Prospect Contributor , United States Naval Institute Blog , 2009 - 2011 Wrote posts on the U.S. Navy's role in disaster relief, humanitarian assistance, and health diplomacy for one of America's most prestigious professional military associations Research Assistant , U.C. Davis Department Of Political Science , 2008 - 2009 Researched the effect of U.S. defense policy on military suicides Founder , Serve Your World 2002 - 2006 An information site on overseas volunteering","tags":"pages","url":"http://chrisalbon.com/pages/about.html"},{"title":"The Beauty Of The Central Limit Theorem","loc":"http://chrisalbon.com/pages/test.html","text":"Lorem ipsum dolor sit amet, case rationibus ea eos, simul ceteros deserunt eum ea. Eam et hinc unum vocibus, has movet sonet ullamcorper id. Eam erat volumus id, munere omittantur eu est. An mea posse viris, ne scaevola euripidis qui. Cum ea adipiscing reprehendunt, mei ubique quidam voluptatibus ex. Cu nec doming civibus, eam in euismod blandit definiebas. Ipsum facete his id, in mei sumo vulputate. Cibo electram erroribus ad ius, iudico consulatu appellantur duo ne. Facete iisque diceret his eu . Dolore adipisci liberavisse est at, est et aliquam urbanitas. Timeam fierent commune vim in, soluta principes pri te. Header 2 Cum ullum utinam maiestatis et, mei facer quidam te. Nostrud maiorum cu ius, usu congue omittam moderatius te, quo choro nostrum adipisci et. Denique suscipit eos ad , nec suas ullum mundi cu, duo brute tempor reformidans ex. Vidit errem reprehendunt te sit, cum et qualisque elaboraret. Est ei esse \\(\\sum_{i=0}&#94;n i&#94;2 = \\frac{(n&#94;2+n)(2n+1)}{6}\\) nemore cetero, sumo oblique euismod in eos, nam idque facilisis an. Sale nobis affert eum no. Cu mel habemus scribentur , id persius nibh aliquid perpetua has . Ius cu esse nonumy recteque, veri voluptatibus cum ei. Est persius imperdiet rationibus ut, minim volumus dissentias qui et, vix indoctum assentior ea. Cu eum hinc dolorem, quo at ceteros insolens complectitur. Ad has albucius repudiare, ut mea altera possim scripserit. No qui fuisset accusamus efficiantur ea enim aeque nam, per mucius forensibus argumentum at Mei ne dicit labore. Solet labore interesset sit ut quo zril causae persius ea, cu eligendi iudicabit sea. Lorem ipsum dolor sit amet, case rationibus ea eos, simul ceteros deserunt eum ea. Eam et hinc unum vocibus, has movet sonet ullamcorper id. Eam erat volumus id, munere omittantur eu est. An mea posse viris, ne scaevola euripidis qui. Cum ea adipiscing reprehendunt, mei ubique quidam voluptatibus ex. # Map the lambda function x() over casualties list ( map (( lambda x : x + 100 ), casualties )) Cum ullum utinam maiestatis et, mei facer quidam te. Nostrud maiorum cu ius, usu congue omittam moderatius te, quo choro nostrum adipisci et. Denique suscipit eos ad, nec suas ullum mundi cu, duo brute tempor reformidans ex. Vidit errem reprehendunt te sit, cum et qualisque elaboraret. \\(\\sum_{i=0}&#94;\\infty i&#94;2\\) Est ei esse nemore cetero, sumo oblique euismod in eos, nam idque facilisis an. Sale nobis affert eum no. $$\\sum_{i=0}&#94;n i&#94;2 = \\frac{(n&#94;2+n)(2n+1)}{6}$$ Cu mel habemus scribentur, id nibh aliquid perpetua has. Ius cu esse nonumy recteque, veri voluptatibus cum ei. Est persius imperdiet rationibus ut, minim volumus dissentias qui et, vix indoctum assentior ea. Cu eum hinc dolorem, quo at ceteros insolens complectitur. Ad has albucius repudiare, ut mea altera possim scripserit. # Map the lambda function x() over casualties list(map((lambda x: x + 100), casualties)) No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. # Create a values as dictionary of lists raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} # Create a dataframe raw_df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) # View a dataframe raw_df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 5 rows × 5 columns Cu mel habemus scribentur, id nibh aliquid perpetua has. Ius cu esse nonumy recteque, veri voluptatibus cum ei. Est persius imperdiet rationibus ut, minim volumus dissentias qui et, vix indoctum assentior ea. Cu eum hinc dolorem, quo at ceteros insolens complectitur. Ad has albucius repudiare, ut mea altera possim scripserit. Header 3 No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. Header 4 No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. Header 5 No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. No qui fuisset accusamus efficiantur, ea enim aeque nam, per mucius forensibus argumentum at. Mei ne dicit labore. Solet labore interesset sit ut, quo zril causae persius ea, cu eligendi iudicabit sea. Ut tractatos ocurreret constituam mei, dolorum splendide te qui. Cu noster laboramus intellegat quo. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"pages","url":"http://chrisalbon.com/pages/test.html"},{"title":"Nested Cross Validation","loc":"http://chrisalbon.com/machine-learning/nested_cross_validation.html","text":"Often we want to tune the parameters of a model (for example, C in a support vector machine). That is, we want to find the value of a parameter that minimizes our loss function. The best way to do this is cross validation: Set the parameter you want to tune to some value. Split your data into K 'folds' (sections). Train your model using K-1 folds using the parameter value. Test your model on the remaining fold. Repeat steps 3 and 4 so that every fold is the test data once. Repeat steps 1 to 5 for every possible value of the parameter. Report the parameter that produced the best result. However, as Cawley and Talbot point out in their 2010 paper, since we used the test set to both select the values of the parameter and evaluate the model, we risk optimistically biasing our model evaluations. For this reason, if a test set is used to select model parameters, then we need a different test set to get an unbiased evaluation of that selected model. One way to overcome this problem is to have nested cross validations. First, an inner cross validation is used to tune the parameters and select the best model. Second, an outer cross validation is used to evaluate the model selected by the inner cross validation. Preliminaries # Load required packages from sklearn import datasets from sklearn.model_selection import GridSearchCV , cross_val_score from sklearn.preprocessing import StandardScaler import numpy as np from sklearn.svm import SVC Get Data The data for this tutorial is beast cancer data with 30 features and a binary target variable . # Load the data dataset = datasets . load_breast_cancer () # Create X from the features X = dataset . data # Create y from the target y = dataset . target Standardize Data # Create a scaler object sc = StandardScaler () # Fit the scaler to the feature data and transform X_std = sc . fit_transform ( X ) Create Inner Cross Validation (For Parameter Tuning) This is our inner cross validation. We will use this to hunt for the best parameters for C , the penalty for misclassifying a data point. GridSearchCV will conduct steps 1-6 listed at the top of this tutorial. # Create a list of 10 candidate values for the C parameter C_candidates = dict ( C = np . logspace ( - 4 , 4 , 10 )) # Create a gridsearch object with the support vector classifier and the C value candidates clf = GridSearchCV ( estimator = SVC (), param_grid = C_candidates ) The code below isn't necessary for parameter tuning using nested cross validation, however to demonstrate that our inner cross validation grid search can find the best value for the parameter C , we will run it once here: # Fit the cross validated grid search on the data clf . fit ( X_std , y ) # Show the best value for C clf . best_estimator_ . C 2.7825594022071258 Create Outer Cross Validation (For Model Evaluation) With our inner cross validation constructed, we can use cross_val_score to evaluate the model with a second (outer) cross validation. The code below splits the data into three folds, running the inner cross validation on two of the folds (merged together) and then evaluating the model on the third fold. This is repeated three times so that every fold is used for testing once. cross_val_score ( clf , X_std , y ) array([ 0.94736842, 0.97894737, 0.98412698]) Each the values above is an unbiased evaluation of the model's accuracy, once for each of the three test folds. Averaged together, they would represent the average accuracy of the model found in the inner cross validated grid search.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/nested_cross_validation.html"},{"title":"Feature Selection Using Random Forest","loc":"http://chrisalbon.com/machine-learning/feature_selection_using_random_forest.html","text":"Often in data science we have hundreds or even millions of features and we want a way to create a model that only includes the most important features. This has three benefits. First, we make our model more simple to interpret. Second, we can reduce the variance of the model, and therefore overfitting. Finally, we can reduce the computational cost (and time) of training a model. The process of identifying only the most relevant features is called \"feature selection.\" Random Forests are often used for feature selection in a data science workflow. The reason is because the tree-based strategies used by random forests naturally ranks by how well they improve the purity of the node. This mean decrease in impurity over all trees (called gini impurity ). Nodes with the greatest decrease in impurity happen at the start of the trees, while notes with the least decrease in impurity occur at the end of trees. Thus, by pruning trees below a particular node, we can create a subset of the most important features. In this tutorial we will: Prepare the dataset Train a random forest classifier Identify the most important features Create a new 'limited featured' dataset containing only those features Train a second classifier on this new dataset Compare the accuracy of the 'full featured' classifier to the accuracy of the 'limited featured' classifier Note: There are other definitions of importance, however in this tutorial we limit our discussion to gini importance. Preliminaries import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.feature_selection import SelectFromModel from sklearn.metrics import accuracy_score Create The Data The dataset used in this tutorial is the famous iris dataset . The Iris target data contains 50 samples from three species of Iris, y and four feature variables, X . # Load the iris dataset iris = datasets . load_iris () # Create a list of feature names feat_labels = [ 'Sepal Length' , 'Sepal Width' , 'Petal Length' , 'Petal Width' ] # Create X from the features X = iris . data # Create y from output y = iris . target View The Data # View the features X [ 0 : 5 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2]]) # View the target data y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Split The Data Into Training And Test Sets # Split the data into 40% test and 60% training X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.4 , random_state = 0 ) Train A Random Forest Classifier # Create a random forest classifier clf = RandomForestClassifier ( n_estimators = 10000 , random_state = 0 , n_jobs =- 1 ) # Train the classifier clf . fit ( X_train , y_train ) # Print the name and gini importance of each feature for feature in zip ( feat_labels , clf . feature_importances_ ): print ( feature ) ('Sepal Length', 0.11024282328064565) ('Sepal Width', 0.016255033655398394) ('Petal Length', 0.45028123999239533) ('Petal Width', 0.42322090307156124) The scores above are the importance scores for each variable. There are two things to note. First, all the importance scores add up to 100%. Second, Petal Length and Petal Width are far more important than the other two features. Combined, Petal Length and Petal Width have an importance of ~0.86! Clearly these are the most importance features. Identify And Select Most Important Features # Create a selector object that will use the random forest classifier to identify # features that have an importance of more than 0.15 sfm = SelectFromModel ( clf , threshold = 0.15 ) # Train the selector sfm . fit ( X_train , y_train ) SelectFromModel(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False), prefit=False, threshold=0.15) # Print the names of the most important features for feature_list_index in sfm . get_support ( indices = True ): print ( feat_labels [ feature_list_index ]) Petal Length Petal Width Create A Data Subset With Only The Most Important Features # Transform the data to create a new dataset containing only the most important features # Note: We have to apply the transform to both the training X and test X data. X_important_train = sfm . transform ( X_train ) X_important_test = sfm . transform ( X_test ) Train A New Random Forest Classifier Using Only Most Important Features # Create a new random forest classifier for the most important features clf_important = RandomForestClassifier ( n_estimators = 10000 , random_state = 0 , n_jobs =- 1 ) # Train the new classifier on the new dataset containing the most important features clf_important . fit ( X_important_train , y_train ) RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10000, n_jobs=-1, oob_score=False, random_state=0, verbose=0, warm_start=False) Compare The Accuracy Of Our Full Feature Classifier To Our Limited Feature Classifier # Apply The Full Featured Classifier To The Test Data y_pred = clf . predict ( X_test ) # View The Accuracy Of Our Full Feature (4 Features) Model accuracy_score ( y_test , y_pred ) 0.93333333333333335 # Apply The Full Featured Classifier To The Test Data y_important_pred = clf_important . predict ( X_important_test ) # View The Accuracy Of Our Limited Feature (2 Features) Model accuracy_score ( y_test , y_important_pred ) 0.8833333333333333 As can be seen by the accuracy scores, our original model which contained all four features is 93.3% accurate while the our 'limited' model which contained only two features is 88.3% accurate. Thus, for a small cost in accuracy we halved the number of features in the model.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/feature_selection_using_random_forest.html"},{"title":"Logistic Regression With L1 Regularization","loc":"http://chrisalbon.com/machine-learning/logistic_regression_with_l1_regularization.html","text":"L1 regularization (also called least absolute deviations) is a powerful tool in data science. There are many tutorials out there explaining L1 regularization and I will not try to do that here. Instead, this tutorial is show the effect of the regularization parameter C on the coefficients and model accuracy. Preliminaries import numpy as np from sklearn.linear_model import LogisticRegression from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler Create The Data The dataset used in this tutorial is the famous iris dataset . The Iris target data contains 50 samples from three species of Iris, y and four feature variables, X . The dataset contains three categories (three species of Iris), however for the sake of simplicity it is easier if the target data is binary. Therefore we will remove the data from the last species of Iris. # Load the iris dataset iris = datasets . load_iris () # Create X from the features X = iris . data # Create y from output y = iris . target # Remake the variable, keeping all data where the category is not 2. X = X [ y != 2 ] y = y [ y != 2 ] View The Data # View the features X [ 0 : 5 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2]]) # View the target data y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) Split The Data Into Training And Test Sets # Split the data into test and training sets, with 30% of samples being put into the test set X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 0 ) Standardize Features Because the regularization penalty is comprised of the sum of the absolute value of the coefficients, we need to scale the data so the coefficients are all based on the same scale. # Create a scaler object sc = StandardScaler () # Fit the scaler to the training data and transform X_train_std = sc . fit_transform ( X_train ) # Apply the scaler to the test data X_test_std = sc . transform ( X_test ) Run Logistic Regression With A L1 Penalty With Various Regularization Strengths The usefulness of L1 is that it can push feature coefficients to 0, creating a method for feature selection. In the code below we run a logistic regression with a L1 penalty four times, each time decreasing the value of C . We should expect that as C decreases, more coefficients become 0. C = [ 10 , 1 , . 1 , . 001 ] for c in C : clf = LogisticRegression ( penalty = 'l1' , C = c ) clf . fit ( X_train , y_train ) print ( 'C:' , c ) print ( 'Coefficient of each feature:' , clf . coef_ ) print ( 'Training accuracy:' , clf . score ( X_train , y_train )) print ( 'Test accuracy:' , clf . score ( X_test , y_test )) print ( '' ) C: 10 Coefficient of each feature: [[-0.0855264 -3.75409972 4.40427765 0. ]] Training accuracy: 1.0 Test accuracy: 1.0 C: 1 Coefficient of each feature: [[ 0. -2.28800472 2.5766469 0. ]] Training accuracy: 1.0 Test accuracy: 1.0 C: 0.1 Coefficient of each feature: [[ 0. -0.82310456 0.97171847 0. ]] Training accuracy: 1.0 Test accuracy: 1.0 C: 0.001 Coefficient of each feature: [[ 0. 0. 0. 0.]] Training accuracy: 0.5 Test accuracy: 0.5 Notice that as C decreases the model coefficients become smaller (for example from 4.36276075 when C=10 to 0.0.97175097 when C=0.1 ), until at C=0.001 all the coefficients are zero. This is the effect of the regularization penalty becoming more prominent.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/logistic_regression_with_l1_regularization.html"},{"title":"Convert Pandas Categorical Data For Scikit-Learn","loc":"http://chrisalbon.com/machine-learning/convert_pandas_categorical_column_into_integers_for_scikit-learn.html","text":"Preliminaries # Import required packages from sklearn import preprocessing import pandas as pd Create DataFrame raw_data = { 'patient' : [ 1 , 1 , 1 , 2 , 2 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 'strong' , 'weak' , 'normal' , 'weak' , 'strong' ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) Fit The Label Encoder # Create a label (category) encoder object le = preprocessing . LabelEncoder () # Fit the encoder to the pandas column le . fit ( df [ 'score' ]) LabelEncoder() View The Labels # View the labels (if you want) list ( le . classes_ ) ['normal', 'strong', 'weak'] Transform Categories Into Integers # Apply the fitted encoder to the pandas column le . transform ( df [ 'score' ]) array([1, 2, 0, 2, 1]) Transform Integers Into Categories # Convert some integers into their category names list ( le . inverse_transform ([ 2 , 2 , 1 ])) ['weak', 'weak', 'strong']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/convert_pandas_categorical_column_into_integers_for_scikit-learn.html"},{"title":"Impute Missing Values With Means","loc":"http://chrisalbon.com/machine-learning/impute_missing_values_with_means.html","text":"Mean imputation replaces missing values with the mean value of that feature/variable. Mean imputation is one of the most 'naive' imputation methods because unlike more complex methods like k-nearest neighbors imputation, it does not use the information we have about an observation to estimate a value for it. Preliminaries import pandas as pd import numpy as np from sklearn.preprocessing import Imputer Create Data # Create an empty dataset df = pd . DataFrame () # Create two variables called x0 and x1. Make the first value of x1 a missing value df [ 'x0' ] = [ 0.3051 , 0.4949 , 0.6974 , 0.3769 , 0.2231 , 0.341 , 0.4436 , 0.5897 , 0.6308 , 0.5 ] df [ 'x1' ] = [ np . nan , 0.2654 , 0.2615 , 0.5846 , 0.4615 , 0.8308 , 0.4962 , 0.3269 , 0.5346 , 0.6731 ] # View the dataset df x0 x1 0 0.3051 NaN 1 0.4949 0.2654 2 0.6974 0.2615 3 0.3769 0.5846 4 0.2231 0.4615 5 0.3410 0.8308 6 0.4436 0.4962 7 0.5897 0.3269 8 0.6308 0.5346 9 0.5000 0.6731 Fit Imputer # Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0) mean_imputer = Imputer ( missing_values = 'NaN' , strategy = 'mean' , axis = 0 ) # Train the imputor on the df dataset mean_imputer = mean_imputer . fit ( df ) Apply Imputer # Apply the imputer to the df dataset imputed_df = mean_imputer . transform ( df . values ) View Data # View the data imputed_df array([[ 0.3051 , 0.49273333], [ 0.4949 , 0.2654 ], [ 0.6974 , 0.2615 ], [ 0.3769 , 0.5846 ], [ 0.2231 , 0.4615 ], [ 0.341 , 0.8308 ], [ 0.4436 , 0.4962 ], [ 0.5897 , 0.3269 ], [ 0.6308 , 0.5346 ], [ 0.5 , 0.6731 ]]) Notice that 0.49273333 is the imputed value, replacing the np.NaN value.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/impute_missing_values_with_means.html"},{"title":"SVC Parameters When Using RBF Kernel","loc":"http://chrisalbon.com/machine-learning/svc_parameters_using_rbf_kernel.html","text":"In this tutorial we will visually explore the effects of the two parameters from the support vector classifier (SVC) when using the radial basis function kernel (RBF). This tutorial draws heavily on the code used in Sebastian Raschka's book Python Machine Learning . Preliminaries # Import packages to visualize the classifer from matplotlib.colors import ListedColormap import matplotlib.pyplot as plt import warnings # Import packages to do the classifying import numpy as np from sklearn.svm import SVC Create Function To Visualize Classification Regions You can ignore the code below. It is used to visualize the the decision regions of the classifier. However it is unimportant to this tutorial to understand how the function works. def versiontuple ( v ): return tuple ( map ( int , ( v . split ( \".\" )))) def plot_decision_regions ( X , y , classifier , test_idx = None , resolution = 0.02 ): # setup marker generator and color map markers = ( 's' , 'x' , 'o' , '&#94;' , 'v' ) colors = ( 'red' , 'blue' , 'lightgreen' , 'gray' , 'cyan' ) cmap = ListedColormap ( colors [: len ( np . unique ( y ))]) # plot the decision surface x1_min , x1_max = X [:, 0 ] . min () - 1 , X [:, 0 ] . max () + 1 x2_min , x2_max = X [:, 1 ] . min () - 1 , X [:, 1 ] . max () + 1 xx1 , xx2 = np . meshgrid ( np . arange ( x1_min , x1_max , resolution ), np . arange ( x2_min , x2_max , resolution )) Z = classifier . predict ( np . array ([ xx1 . ravel (), xx2 . ravel ()]) . T ) Z = Z . reshape ( xx1 . shape ) plt . contourf ( xx1 , xx2 , Z , alpha = 0.4 , cmap = cmap ) plt . xlim ( xx1 . min (), xx1 . max ()) plt . ylim ( xx2 . min (), xx2 . max ()) for idx , cl in enumerate ( np . unique ( y )): plt . scatter ( x = X [ y == cl , 0 ], y = X [ y == cl , 1 ], alpha = 0.8 , c = cmap ( idx ), marker = markers [ idx ], label = cl ) # highlight test samples if test_idx : # plot all samples if not versiontuple ( np . __version__ ) >= versiontuple ( '1.9.0' ): X_test , y_test = X [ list ( test_idx ), :], y [ list ( test_idx )] warnings . warn ( 'Please update to NumPy 1.9.0 or newer' ) else : X_test , y_test = X [ test_idx , :], y [ test_idx ] plt . scatter ( X_test [:, 0 ], X_test [:, 1 ], c = '' , alpha = 1.0 , linewidths = 1 , marker = 'o' , s = 55 , label = 'test set' ) Generate Data Here we are generating some non-linearly separable data that we will train our classifier on. This data would be akin to your training dataset. There are two classes in our y vector: blue x's and red squares. np . random . seed ( 0 ) X_xor = np . random . randn ( 200 , 2 ) y_xor = np . logical_xor ( X_xor [:, 0 ] > 0 , X_xor [:, 1 ] > 0 ) y_xor = np . where ( y_xor , 1 , - 1 ) plt . scatter ( X_xor [ y_xor == 1 , 0 ], X_xor [ y_xor == 1 , 1 ], c = 'b' , marker = 'x' , label = '1' ) plt . scatter ( X_xor [ y_xor == - 1 , 0 ], X_xor [ y_xor == - 1 , 1 ], c = 'r' , marker = 's' , label = '-1' ) plt . xlim ([ - 3 , 3 ]) plt . ylim ([ - 3 , 3 ]) plt . legend ( loc = 'best' ) plt . tight_layout () plt . show () Classify Using a Linear Kernel The most basic way to use a SVC is with a linear kernel, which means the decision boundary is a straight line (or hyperplane in higher dimensions). Linear kernels are rarely used in practice, however I wanted to show it here since it is the most basic version of SVC. As can been seen below, it is not very good at classifying (which can be seen by all the blue X's in the red region) because the data is not linear. # Create a SVC classifier using a linear kernel svm = SVC ( kernel = 'linear' , C = 1 , random_state = 0 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Classify Using a RBF Kernel Radial Basis Function is a commonly used kernel in SVC: $$K(\\mathbf {x} ,\\mathbf {x'} )=\\exp \\left(-{\\frac {||\\mathbf {x} -\\mathbf {x'} ||&#94;{2}}{2\\sigma &#94;{2}}}\\right)$$ where \\(||\\mathbf {x} -\\mathbf {x'} ||&#94;{2}\\) is the squared Euclidean distance between two data points \\(\\mathbf {x}\\) and \\(\\mathbf {x'}\\) . If this doesn't make sense, Sebastian's book has a full description. However, for this tutorial, it is only important to know that an SVC classifier using an RBF kernel has two parameters: gamma and C . Gamma gamma is a parameter of the RBF kernel and can be thought of as the 'spread' of the kernel and therefore the decision region. When gamma is low, the 'curve' of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the 'curve' of the decision boundary is high, which creates islands of decision-boundaries around data points. We will see this very clearly below. C C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance). Gamma In the four charts below, we apply the same SVC-RBF classifier to the same data while holding C constant. The only difference between each chart is that each time we will increase the value of gamma . By doing so, we can visually see the effect of gamma on the decision boundary. Gamma = 0.01 In the case of our SVC classifier and data, when using a low gamma like 0.01, the decision boundary is not very 'curvy', rather it is just one big sweeping arch. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Gamma = 1.0 You can see a big difference when we increase the gamma to 1. Now the decision boundary is starting to better cover the spread of the data. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma = 1 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Gamma = 10.0 At gamma = 10 the spread of the kernel is less pronounced. The decision boundary starts to be highly effected by individual data points (i.e. variance). # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma = 10 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () Gamma = 100.0 With high gamma , the decision boundary is almost entirely dependent on individual data points, creating \"islands\". This data is clearly overfitted. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma = 100 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C - The Penalty Parameter Now we will repeat the process for C : we will use the same classifier, same data, and hold gamma constant. The only thing we will change is the C , the penalty for misclassification. C = 1 With C = 1 , the classifier is clearly tolerant of misclassified data point. There are many red points in the blue region and blue points in the red region. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 1 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 10 At C = 10 , the classifier is less tolerant to misclassified data points and therefore the decision boundary is more severe. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 10 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 1000 When C = 1000 , the classifier starts to become very intolerant to misclassified data points and thus the decision boundary becomes less biased and has more variance (i.e. more dependent on the individual data points). # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 1000 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 10000 At C = 10000 , the classifier \"works really hard\" to not misclassify data points and we see signs of overfitting. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 10000 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () C = 100000 At C = 100000 , the classifier is heavily penalized for any misclassified data points and therefore the margins are small. # Create a SVC classifier using an RBF kernel svm = SVC ( kernel = 'rbf' , random_state = 0 , gamma =. 01 , C = 100000 ) # Train the classifier svm . fit ( X_xor , y_xor ) # Visualize the decision boundaries plot_decision_regions ( X_xor , y_xor , classifier = svm ) plt . legend ( loc = 'upper left' ) plt . tight_layout () plt . show () if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/svc_parameters_using_rbf_kernel.html"},{"title":"Run Project Jupyter Notebooks On Amazon EC2","loc":"http://chrisalbon.com/jupyter/run_project_jupyter_on_amazon_ec2.html","text":"This is tutorial on running Project Jupyter Notebook on an Amazon EC2 instance. It is based on a tutorial by Piyush Agarwal which did not work for me immediately, but I tweaked a few things and got it working. Note: This is not a beginner's tutorial. I don't explain some of the steps fully and don't explain some concepts. There are other tutorials out there for that. Create an AWS account An EC2 instance requires an AWS account. You can make an account here . Navigate to EC2 Log into AWS and go to the EC2 main page. Then you will see a 'Launch Instance' button. Launch a new instance Select Ubuntu Select t2.micro Check out your new instance Create a new security group Create and download a new key pair View connect instructions Set permissions on key pair chmod 400 tutorial.pem Open terminal Connect using ssh ssh -i \"tutorial.pem\" ubuntu@ec2-52-39-239-66.us-west-2.compute.amazonaws.com Are you sure you want to continue connecting (yes/no)? Download Anaconda to instance Visit Anaconda's download page and right click to get the url of the latest version of the Linux 64-bit version. In my case this url was: https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh Now, back in the terminal, tell the EC2 instance to download that file. Note: You aren't downloading the file to your computer, you are downloading it to the EC2 instance and installing it from there. wget https://repo.continuum.io/archive/Anaconda3-4.2.0-Linux-x86_64.sh Install Anaconda bash Anaconda3-4.2.0-Linux-x86_64.sh Press enter a few times Type 'yes' to agree 'Press ENTER to confirm the location' Do you wish the installer to prepend the Anaconda3 install location to PATH in your /home/ubuntu/.bashrc ? [yes|no] [no] >>> yes Set Anaconda as the preferred environment which python [ubuntu@ip-172-31-43-70:~$ which python /usr/bin/python source .bashrc Create a password for jupyter notebook ipython from IPython.lib import passwd passwd() 'sha1:98ff0e580111:12798c72623a6eecd54b51c006b1050f0ac1a62d' exit Create config profile jupyter notebook --generate-config Create certificates for https mkdir certs cd certs sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem Answer questions Configure jupyter cd ~/.jupyter/ vi jupyter_notebook_config.py c = get_config() # Kernel config c.IPKernelApp.pylab = 'inline' # if you want plotting support always in your notebook # Notebook config c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' #location of your certificate file c.NotebookApp.ip = '*' c.NotebookApp.open_browser = False #so that the ipython notebook does not opens up a browser by default c.NotebookApp.password = u'sha1:98ff0e580111:12798c72623a6eecd54b51c006b1050f0ac1a62d' #the encrypted password we generated above # It is a good idea to put it on a known, fixed port c.NotebookApp.port = 8888 Remember to replace sha1:98ff0e580111:12798c72623a6eecd54b51c006b1050f0ac1a62d with your password! Press esc Press shift-z Press shift-z Create folder for notebooks cd ~ mkdir Notebooks cd Notebooks Create new screen screen Start Jupyter notebook jupyter notebook Detach from screen Command: Ctrl-a and then d Other useful commands: Create new window: Ctrl-a c . Switch windows: Ctrl-a n Reattach to Screen: screen -r Visit Jupyter notebook in browser Your EC2 instance will have a long url, like this: ec2-52-39-239-66.us-west-2.compute.amazonaws.com Visit that URL in your browser: https://ec2-52-39-239-66.us-west-2.compute.amazonaws.com:8888/","tags":"Jupyter","url":"http://chrisalbon.com/jupyter/run_project_jupyter_on_amazon_ec2.html"},{"title":"Training A Perceptron","loc":"http://chrisalbon.com/machine-learning/training_a_perceptron.html","text":"A perceptron learner was one of the earliest machine learning techniques and still from the foundation of many modern neural networks. In this tutorial we use a perceptron learner to classify the famous iris dataset . This tutorial was inspired by Python Machine Learning by Sebastian Raschka . Preliminaries # Load required libraries from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.linear_model import Perceptron from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import numpy as np Load The Iris Data # Load the iris dataset iris = datasets . load_iris () # Create our X and y data X = iris . data y = iris . target View The Iris Data # View the first five observations of our y data y [: 5 ] array([0, 0, 0, 0, 0]) # View the first five observations of our x data. # Notice that there are four independent variables (features) X [: 5 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2], [ 4.6, 3.1, 1.5, 0.2], [ 5. , 3.6, 1.4, 0.2]]) Split The Iris Data Into Training And Test # Split the data into 70% training data and 30% test data X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 ) Preprocess The X Data By Scaling # Train the scaler, which standarizes all the features to have mean=0 and unit variance sc = StandardScaler () sc . fit ( X_train ) StandardScaler(copy=True, with_mean=True, with_std=True) # Apply the scaler to the X training data X_train_std = sc . transform ( X_train ) # Apply the SAME scaler to the X test data X_test_std = sc . transform ( X_test ) Train A Perceptron Learner # Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1 ppn = Perceptron ( n_iter = 40 , eta0 = 0.1 , random_state = 0 ) # Train the perceptron ppn . fit ( X_train_std , y_train ) Perceptron(alpha=0.0001, class_weight=None, eta0=0.1, fit_intercept=True, n_iter=40, n_jobs=1, penalty=None, random_state=0, shuffle=True, verbose=0, warm_start=False) Apply The Trained Learner To Test Data # Apply the trained perceptron on the X data to make predicts for the y test data y_pred = ppn . predict ( X_test_std ) Compare The Predicted Y With The True Y # View the predicted y test data y_pred array([0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1]) # View the true y test data y_test array([0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 1, 1, 1, 0, 2, 2, 2, 1, 0, 0, 0, 0, 2, 2, 1, 1, 0, 2, 1, 1, 1, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1]) Examine Accuracy Metric # View the accuracy of the model, which is: 1 - (observations predicted wrong / total observations) print ( 'Accuracy: %.2f ' % accuracy_score ( y_test , y_pred )) Accuracy: 0.87","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/training_a_perceptron.html"},{"title":"Mine Twitter's Stream For Hashtags Or Words","loc":"http://chrisalbon.com/python/mine_a_twitter_hashtags_and_words.html","text":"This is a script which monitor's Twitter for tweets containing certain hashtags, words, or phrases. When one of those appears, it saves that tweet, and the user's information to a csv file. A similar version of this script is available on GitHub here . The main difference between the code presented here and the repo is that here I am added extensive comments in the code explaining what is happening. Also, the code below runs as a Jupyter notebook. To get the code below to run, you need to added your own Twitter API credentials. Preliminaries #Import libraries from tweepy.streaming import StreamListener from tweepy import OAuthHandler from tweepy import Stream import time import csv import sys Create A Twitter Stream Miner # Create a streamer object class StdOutListener ( StreamListener ): # Define a function that is initialized when the miner is called def __init__ ( self , api = None ): # That sets the api self . api = api # Create a file with 'data_' and the current time self . filename = 'data' + '_' + time . strftime ( '%Y%m %d -%H%M%S' ) + '.csv' # Create a new file with that filename csvFile = open ( self . filename , 'w' ) # Create a csv writer csvWriter = csv . writer ( csvFile ) # Write a single row with the headers of the columns csvWriter . writerow ([ 'text' , 'created_at' , 'geo' , 'lang' , 'place' , 'coordinates' , 'user.favourites_count' , 'user.statuses_count' , 'user.description' , 'user.location' , 'user.id' , 'user.created_at' , 'user.verified' , 'user.following' , 'user.url' , 'user.listed_count' , 'user.followers_count' , 'user.default_profile_image' , 'user.utc_offset' , 'user.friends_count' , 'user.default_profile' , 'user.name' , 'user.lang' , 'user.screen_name' , 'user.geo_enabled' , 'user.profile_background_color' , 'user.profile_image_url' , 'user.time_zone' , 'id' , 'favorite_count' , 'retweeted' , 'source' , 'favorited' , 'retweet_count' ]) # When a tweet appears def on_status ( self , status ): # Open the csv file created previously csvFile = open ( self . filename , 'a' ) # Create a csv writer csvWriter = csv . writer ( csvFile ) # If the tweet is not a retweet if not 'RT @' in status . text : # Try to try : # Write the tweet's information to the csv file csvWriter . writerow ([ status . text , status . created_at , status . geo , status . lang , status . place , status . coordinates , status . user . favourites_count , status . user . statuses_count , status . user . description , status . user . location , status . user . id , status . user . created_at , status . user . verified , status . user . following , status . user . url , status . user . listed_count , status . user . followers_count , status . user . default_profile_image , status . user . utc_offset , status . user . friends_count , status . user . default_profile , status . user . name , status . user . lang , status . user . screen_name , status . user . geo_enabled , status . user . profile_background_color , status . user . profile_image_url , status . user . time_zone , status . id , status . favorite_count , status . retweeted , status . source , status . favorited , status . retweet_count ]) # If some error occurs except Exception as e : # Print the error print ( e ) # and continue pass # Close the csv file csvFile . close () # Return nothing return # When an error occurs def on_error ( self , status_code ): # Print the error code print ( 'Encountered error with status code:' , status_code ) # If the error code is 401, which is the error for bad credentials if status_code == 401 : # End the stream return False # When a deleted tweet appears def on_delete ( self , status_id , user_id ): # Print message print ( \"Delete notice\" ) # Return nothing return # When reach the rate limit def on_limit ( self , track ): # Print rate limiting error print ( \"Rate limited, continuing\" ) # Continue mining tweets return True # When timed out def on_timeout ( self ): # Print timeout message print ( sys . stderr , 'Timeout...' ) # Wait 10 seconds time . sleep ( 10 ) # Return nothing return Create A Wrapper For The Miner # Create a mining function def start_mining ( queries ): ''' Inputs list of strings. Returns tweets containing those strings. ''' #Variables that contains the user credentials to access Twitter API consumer_key = \"YOUR_CREDENTIALS\" consumer_secret = \"YOUR_CREDENTIALS\" access_token = \"YOUR_CREDENTIALS\" access_token_secret = \"YOUR_CREDENTIALS\" # Create a listener l = StdOutListener () # Create authorization info auth = OAuthHandler ( consumer_key , consumer_secret ) auth . set_access_token ( access_token , access_token_secret ) # Create a stream object with listener and authorization stream = Stream ( auth , l ) # Run the stream object using the user defined queries stream . filter ( track = queries ) Run The Stream Miner # Start the miner start_mining ([ 'python' , '#Python' ]) Encountered error with status code: 401","tags":"Python","url":"http://chrisalbon.com/python/mine_a_twitter_hashtags_and_words.html"},{"title":"Generate Tweets Using Markov Chains","loc":"http://chrisalbon.com/python/generate_tweets_using_markov_chain.html","text":"Preliminaries import markovify Load Corpus The corpus I am using is just one I found online. The corpus you choose is central to generating realistic text. # Get raw text as string with open ( \"brown.txt\" ) as f : text = f . read () Build Markov Chain # Build the model. text_model = markovify . Text ( text ) Generate One Tweet # Print three randomly-generated sentences of no more than 140 characters for i in range ( 3 ): print ( text_model . make_short_sentence ( 140 )) Within a month, calls were still productive and most devotees of baseball attended the dozens of them. Even death, therefore, has a leather bolo drawn through a local rajah in 1949. He had a rather sharp and confident.","tags":"Python","url":"http://chrisalbon.com/python/generate_tweets_using_markov_chain.html"},{"title":"Lasso Regression In Scikit-Learn","loc":"http://chrisalbon.com/machine-learning/lasso_regression_in_scikit.html","text":"Often we want conduct a process called regularization , wherein we penalize the number of features in a model in order to only keep the most important features. This can be particularly important when you have a dataset with 100,000+ features. Lasso regression is a common modeling technique to do regularization. The math behind it is pretty interesting, but practically, what you need to know is that Lasso regression comes with a parameter, alpha , and the higher the alpha , the most feature coefficients are zero. That is, when alpha is 0 , Lasso regression produces the same coefficients as a linear regression. When alpha is very very large, all coefficients are zero. In this tutorial, I run three lasso regressions, with varying levels of alpha, and show the resulting effect on the coefficients. Preliminaries from sklearn.linear_model import Lasso from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_boston import pandas as pd Load Data boston = load_boston () scaler = StandardScaler () X = scaler . fit_transform ( boston [ \"data\" ]) Y = boston [ \"target\" ] names = boston [ \"feature_names\" ] Run Three Lasso Regressions, Varying Alpha Levels # Create a function called lasso, def lasso ( alphas ): ''' Takes in a list of alphas. Outputs a dataframe containing the coefficients of lasso regressions from each alpha. ''' # Create an empty data frame df = pd . DataFrame () # Create a column of feature names df [ 'Feature Name' ] = names # For each alpha value in the list of alpha values, for alpha in alphas : # Create a lasso regression with that alpha value, lasso = Lasso ( alpha = alpha ) # Fit the lasso regression lasso . fit ( X , Y ) # Create a column name for that alpha value column_name = 'Alpha = %f ' % alpha # Create a column of coefficient values df [ column_name ] = lasso . coef_ # Return the datafram return df # Run the function called, Lasso lasso ([ . 0001 , . 5 , 10 ]) Feature Name Alpha = 0.000100 Alpha = 0.500000 Alpha = 10.000000 0 CRIM -0.920130 -0.106977 -0.0 1 ZN 1.080498 0.000000 0.0 2 INDUS 0.142027 -0.000000 -0.0 3 CHAS 0.682235 0.397399 0.0 4 NOX -2.059250 -0.000000 -0.0 5 RM 2.670814 2.973323 0.0 6 AGE 0.020680 -0.000000 -0.0 7 DIS -3.104070 -0.169378 0.0 8 RAD 2.656950 -0.000000 -0.0 9 TAX -2.074110 -0.000000 -0.0 10 PTRATIO -2.061921 -1.599574 -0.0 11 B 0.856553 0.545715 0.0 12 LSTAT -3.748470 -3.668884 -0.0 Notice that as the alpha value increases, more features have a coefficient of 0.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/lasso_regression_in_scikit.html"},{"title":"Loading Features From Dictionaries","loc":"http://chrisalbon.com/machine-learning/loading_features_from_dictionaries.html","text":"Preliminaries from sklearn.feature_extraction import DictVectorizer Create A Dictionary staff = [{ 'name' : 'Steve Miller' , 'age' : 33. }, { 'name' : 'Lyndon Jones' , 'age' : 12. }, { 'name' : 'Baxter Morth' , 'age' : 18. }] Convert Dictionary To Feature Matrix # Create an object for our dictionary vectorizer vec = DictVectorizer () # Fit then transform the staff dictionary with vec, then output an array vec . fit_transform ( staff ) . toarray () array([[ 33., 0., 0., 1.], [ 12., 0., 1., 0.], [ 18., 1., 0., 0.]]) View Feature Names # Get Feature Names vec . get_feature_names () ['age', 'name=Baxter Morth', 'name=Lyndon Jones', 'name=Steve Miller']","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/loading_features_from_dictionaries.html"},{"title":"Nested For Loops Using List Comprehension","loc":"http://chrisalbon.com/python/nested_for_loops_using_list_comprehension.html","text":"# Create two lists squads = [ \"1st Squad\" , '2nd Squad' , '3rd Squad' ] regiments = [ \"51st Regiment\" , '15th Regiment' , '12th Regiment' ] # Create a tuple for each regiment in regiments, for each squad in sqauds [( regiment , squad ) for regiment in regiments for squad in squads ] [('51st Regiment', '1st Squad'), ('51st Regiment', '2nd Squad'), ('51st Regiment', '3rd Squad'), ('15th Regiment', '1st Squad'), ('15th Regiment', '2nd Squad'), ('15th Regiment', '3rd Squad'), ('12th Regiment', '1st Squad'), ('12th Regiment', '2nd Squad'), ('12th Regiment', '3rd Squad')]","tags":"Python","url":"http://chrisalbon.com/python/nested_for_loops_using_list_comprehension.html"},{"title":"Preprocessing Categorical Features","loc":"http://chrisalbon.com/machine-learning/preprocessing_categorical_features.html","text":"Often, machine learning methods (e.g. logistic regression, SVM with a linear kernel, etc) will require that categorical variables be converted into dummy variables (also called OneHot encoding). For example, a single feature Fruit would be converted into three features, Apples , Oranges , and Bananas , one for each category in the categorical feature. There are common ways to preprocess categorical features: using pandas or scikit-learn. Preliminaries from sklearn import preprocessing from sklearn.pipeline import Pipeline import pandas as pd Create Data raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'city' : [ 'San Francisco' , 'Baltimore' , 'Miami' , 'Douglas' , 'Boston' ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'city' ]) df first_name last_name age city 0 Jason Miller 42 San Francisco 1 Molly Jacobson 52 Baltimore 2 Tina Ali 36 Miami 3 Jake Milner 24 Douglas 4 Amy Cooze 73 Boston Convert Nominal Categorical Feature Into Dummy Variables Using Pandas # Create dummy variables for every unique category in df.city pd . get_dummies ( df [ \"city\" ]) Baltimore Boston Douglas Miami San Francisco 0 0.0 0.0 0.0 0.0 1.0 1 1.0 0.0 0.0 0.0 0.0 2 0.0 0.0 0.0 1.0 0.0 3 0.0 0.0 1.0 0.0 0.0 4 0.0 1.0 0.0 0.0 0.0 Convert Nominal Categorical Data Into Dummy (OneHot) Features Using Scikit # Convert strings categorical names to integers integerized_data = preprocessing . LabelEncoder () . fit_transform ( df [ \"city\" ]) # View data integerized_data array([4, 0, 3, 2, 1]) # Convert integer categorical representations to OneHot encodings preprocessing . OneHotEncoder () . fit_transform ( integerized_data . reshape ( - 1 , 1 )) . toarray () array([[ 0., 0., 0., 0., 1.], [ 1., 0., 0., 0., 0.], [ 0., 0., 0., 1., 0.], [ 0., 0., 1., 0., 0.], [ 0., 1., 0., 0., 0.]]) Note that the output of pd.get_dummies() and the scikit methods produces the same output matrix.","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/preprocessing_categorical_features.html"},{"title":"Store API Credentials For Open Source Projects","loc":"http://chrisalbon.com/python/store_api_credentials_for_open_source_projects.html","text":"One issue which repeated comes up is how to manage private API credentials when the project is available on GitHub. This is the method I use for my own projects. I store all credentials in a JSON file and tell gitignore to not upload that file. Then when I am running that code locally, load the API credentials from the JSON file. Preliminaries import json Step 1: Create a JSON with the API credentials credentials = { 'access_secret' : '392n39d93' , 'access_token' : 'sdf424f' , 'consumer_key' : 'sdf3223' , 'consumer_secret' : 'dsf2344' } with open ( 'credentials.json' , 'w' ) as f : json . dump ( credentials , f , ensure_ascii = False ) Step 2: Add File To gitignore Follow the instructions here . Here is an example of a good gitignore file. Step 3: Retrieve The Credentials From The JSON File This step should be the one done inside your project or script. Load JSON File # Import API Keys with open ( 'credentials.json' ) as creds : credentials = json . load ( creds ) Retrieve The Credentials credentials [ 'consumer_key' ] 'sdf3223'","tags":"Python","url":"http://chrisalbon.com/python/store_api_credentials_for_open_source_projects.html"},{"title":"Cross Validation Pipeline","loc":"http://chrisalbon.com/machine-learning/cross_validation_pipeline.html","text":"The code below does a lot in only a few lines. To help explain things, here are the steps that code is doing: Split the raw data into three folds. Select one for testing and two for training. Preprocess the data by scaling the training features. Train a support vector classifier on the training data. Apply the classifier to the test data. Record the accuracy score. Repeat steps 1-5 two more times, once for each fold. Calculate the mean score for all the folds. Preliminaries from sklearn.datasets import load_iris from sklearn.pipeline import make_pipeline from sklearn import preprocessing from sklearn import cross_validation from sklearn import svm Load Data For this tutorial we will use the famous iris dataset . The iris data contains four measurements of 150 iris flowers and their species. We will use a support vector classifier to predict the species of the iris flowers. # Load the iris test data iris = load_iris () # View the iris data features for the first three rows iris . data [ 0 : 3 ] array([[ 5.1, 3.5, 1.4, 0.2], [ 4.9, 3. , 1.4, 0.2], [ 4.7, 3.2, 1.3, 0.2]]) # View the iris data target for first three rows. '0' means it flower is of the setosa species. iris . target [ 0 : 3 ] array([0, 0, 0]) Create Classifier Pipeline Now we create a pipeline for the data. First, the pipeline preprocesses the data by scaling the feature variable's values to mean zero and unit variance. Second, the pipeline trains a support classifier on the data with C=1 . C is the cost function for the margins. The higher the C, the less tolerant the model is for observations being on the wrong side of the hyperplane. # Create a pipeline that scales the data then trains a support vector classifier classifier_pipeline = make_pipeline ( preprocessing . StandardScaler (), svm . SVC ( C = 1 )) Cross Validation Scikit provides a great helper function to make it easy to do cross validation. Specifically, the code below splits the data into three folds, then executes the classifier pipeline on the iris data. Important note from the scikit docs : For integer/None inputs, if y is binary or multiclass, StratifiedKFold used. If the estimator is a classifier or if y is neither binary nor multiclass, KFold is used. # KFold/StratifiedKFold cross validation with 3 folds (the default) # applying the classifier pipeline to the feature and target data scores = cross_validation . cross_val_score ( classifier_pipeline , iris . data , iris . target , cv = 3 ) Evaluate Model Here is the output of our 3 KFold cross validation. Each value is the accuracy score of the support vector classifier when leaving out a different fold. There are three values because there are three folds. A higher accuracy score, the better. scores array([ 0.98039216, 0.90196078, 0.97916667]) To get an good measure of the model's accuracy, we calculate the mean of the three scores. This is our measure of model accuracy. scores . mean () 0.95383986928104569","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/cross_validation_pipeline.html"},{"title":"Saving Machine Learning Models","loc":"http://chrisalbon.com/machine-learning/saving_machine_learning_models.html","text":"In scikit there are two main ways to save a model for future use: a pickle string and a pickled model as a file. Preliminaries from sklearn.linear_model import LogisticRegression from sklearn import datasets import pickle from sklearn.externals import joblib Load Data # Load the iris data iris = datasets . load_iris () # Create a matrix, X, of features and a vector, y. X , y = iris . data , iris . target Train Model # Train a naive logistic regression model clf = LogisticRegression ( random_state = 0 ) clf . fit ( X , y ) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=0, solver='liblinear', tol=0.0001, verbose=0, warm_start=False) Save To String Using Pickle # Save the trained model as a pickle string. saved_model = pickle . dumps ( clf ) # View the pickled model saved_model b'\\x80\\x03csklearn.linear_model.logistic\\nLogisticRegression\\nq\\x00)\\x81q\\x01}q\\x02(X\\x07\\x00\\x00\\x00penaltyq\\x03X\\x02\\x00\\x00\\x00l2q\\x04X\\x0b\\x00\\x00\\x00multi_classq\\x05X\\x03\\x00\\x00\\x00ovrq\\x06X\\x08\\x00\\x00\\x00max_iterq\\x07KdX\\x08\\x00\\x00\\x00classes_q\\x08cnumpy.core.multiarray\\n_reconstruct\\nq\\tcnumpy\\nndarray\\nq\\nK\\x00\\x85q\\x0bC\\x01bq\\x0c\\x87q\\rRq\\x0e(K\\x01K\\x03\\x85q\\x0fcnumpy\\ndtype\\nq\\x10X\\x02\\x00\\x00\\x00i8q\\x11K\\x00K\\x01\\x87q\\x12Rq\\x13(K\\x03X\\x01\\x00\\x00\\x00<q\\x14NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq\\x15b\\x89C\\x18\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00q\\x16tq\\x17bX\\x07\\x00\\x00\\x00n_iter_q\\x18h\\th\\nK\\x00\\x85q\\x19h\\x0c\\x87q\\x1aRq\\x1b(K\\x01K\\x01\\x85q\\x1ch\\x10X\\x02\\x00\\x00\\x00i4q\\x1dK\\x00K\\x01\\x87q\\x1eRq\\x1f(K\\x03h\\x14NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq b\\x89C\\x04\\x07\\x00\\x00\\x00q!tq\"bX\\x06\\x00\\x00\\x00n_jobsq#K\\x01X\\x11\\x00\\x00\\x00intercept_scalingq$K\\x01X\\x03\\x00\\x00\\x00tolq%G?\\x1a6\\xe2\\xeb\\x1cC-X\\x07\\x00\\x00\\x00verboseq&K\\x00X\\x04\\x00\\x00\\x00dualq\\'\\x89X\\x0c\\x00\\x00\\x00random_stateq(K\\x00X\\x05\\x00\\x00\\x00coef_q)h\\th\\nK\\x00\\x85q*h\\x0c\\x87q+Rq,(K\\x01K\\x03K\\x04\\x86q-h\\x10X\\x02\\x00\\x00\\x00f8q.K\\x00K\\x01\\x87q/Rq0(K\\x03h\\x14NNNJ\\xff\\xff\\xff\\xffJ\\xff\\xff\\xff\\xffK\\x00tq1b\\x88C`\\x9a\\x1c\\x904+\\x8f\\xda?v5\\xf6\\x7f9\\xaa\\xda?FVL\\xe5\\x05R\\xfb\\xbf\\xf6\\xad\\xd9&#94;ya\\xf7?\\x89\\x86\\x10B\\x03\\x9d\\xf9\\xbf\\x7f\\xa7x\\xf5\\\\\\x8c\\xf8\\xbf\\x8b$8y\\xdd\\x18\\x02\\xc0\\xac\\x8f\\xee\\xd9+|\\xe2?\\\\\\x10\\xf2\\xcc\\x8c\\xc4\\x03@\\xda\\xb0;l,w\\xf0\\xbf8_\\xe7W*+\\xf6\\xbf\\xefT`-lq\\x04@q2tq3bX\\n\\x00\\x00\\x00intercept_q4h\\th\\nK\\x00\\x85q5h\\x0c\\x87q6Rq7(K\\x01K\\x03\\x85q8h0\\x89C\\x18\\xd4\\x86D\\x03\\xb1\\xff\\xd0?\\xa2\\xcc=I\\xe5]\\xf1?\\x84\\'\\xad\\x8dxo\\xf3\\xbfq9tq:bX\\n\\x00\\x00\\x00warm_startq;\\x89X\\x01\\x00\\x00\\x00Cq<G?\\xf0\\x00\\x00\\x00\\x00\\x00\\x00X\\r\\x00\\x00\\x00fit_interceptq=\\x88X\\x06\\x00\\x00\\x00solverq>X\\t\\x00\\x00\\x00liblinearq?X\\x0c\\x00\\x00\\x00class_weightq@Nub.' # Load the pickled model clf_from_pickle = pickle . loads ( saved_model ) # Use the loaded pickled model to make predictions clf_from_pickle . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Save To Pickled File Using joblib # Save the model as a pickle in a file joblib . dump ( clf , 'filename.pkl' ) ['filename.pkl', 'filename.pkl_01.npy', 'filename.pkl_02.npy', 'filename.pkl_03.npy', 'filename.pkl_04.npy'] # Load the model from the file clf_from_joblib = joblib . load ( 'filename.pkl' ) # Use the loaded model to make predictions clf_from_joblib . predict ( X ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/saving_machine_learning_models.html"},{"title":"Geolocate A City Or Country","loc":"http://chrisalbon.com/python/geolocate_a_city_or_country.html","text":"This tutorial creates a function that attempts to take a city and country and return its latitude and longitude. But when the city is unavailable (which is often be the case), the returns the latitude and longitude of the center of the country. Preliminaries from geopy.geocoders import Nominatim geolocator = Nominatim () import numpy as np Create Geolocation Function def geolocate ( city = None , country = None ): ''' Inputs city and country, or just country. Returns the lat/long coordinates of either the city if possible, if not, then returns lat/long of the center of the country. ''' # If the city exists, if city != None : # Try try : # To geolocate the city and country loc = geolocator . geocode ( str ( city + ',' + country )) # And return latitude and longitude return ( loc . latitude , loc . longitude ) # Otherwise except : # Return missing value return np . nan # If the city doesn't exist else : # Try try : # Geolocate the center of the country loc = geolocator . geocode ( country ) # And return latitude and longitude return ( loc . latitude , loc . longitude ) # Otherwise except : # Return missing value return np . nan Geolocate A City And Country # Geolocate a city and country geolocate ( city = 'Austin' , country = 'USA' ) (30.2711286, -97.7436994) Geolocate Just A Country # Geolocate just a country geolocate ( country = 'USA' ) (39.7837304, -100.4458824)","tags":"Python","url":"http://chrisalbon.com/python/geolocate_a_city_or_country.html"},{"title":"Preprocessing Iris Data","loc":"http://chrisalbon.com/machine-learning/preprocessing_iris_data.html","text":"Preliminaries from sklearn import datasets import numpy as np from sklearn.cross_validation import train_test_split from sklearn.preprocessing import StandardScaler Load Data # Load the iris data iris = datasets . load_iris () # Create a variable for the feature data X = iris . data # Create a variable for the target data y = iris . target Split Data For Cross Validation # Random split the data into four new datasets, training features, training outcome, test features, # and test outcome. Set the size of the test data to be 30% of the full dataset. X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.3 , random_state = 42 ) Standardize Feature Data # Load the standard scaler sc = StandardScaler () # Compute the mean and standard deviation based on the training data sc . fit ( X_train ) # Scale the training data to be of mean 0 and of unit variance X_train_std = sc . transform ( X_train ) # Scale the test data to be of mean 0 and of unit variance X_test_std = sc . transform ( X_test ) # Feature Test Data, non-standardized X_test [ 0 : 5 ] array([[ 6.1, 2.8, 4.7, 1.2], [ 5.7, 3.8, 1.7, 0.3], [ 7.7, 2.6, 6.9, 2.3], [ 6. , 2.9, 4.5, 1.5], [ 6.8, 2.8, 4.8, 1.4]]) # Feature Test Data, standardized. X_test_std [ 0 : 5 ] array([[ 0.3100623 , -0.49582097, 0.48403749, -0.05143998], [-0.17225683, 1.92563026, -1.26851205, -1.26670948], [ 2.23933883, -0.98011121, 1.76924049, 1.43388941], [ 0.18948252, -0.25367584, 0.36720086, 0.35364985], [ 1.15412078, -0.49582097, 0.54245581, 0.21861991]])","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/preprocessing_iris_data.html"},{"title":"Random Forest Classifier Example","loc":"http://chrisalbon.com/machine-learning/random_forest_classifier_example_scikit.html","text":"This tutorial is based on Yhat's 2013 tutorial on Random Forests in Python . If you want a good summary of the theory and uses of random forests, I suggest you check out their guide. In the tutorial below, I annotate, correct, and expand on a short code example of random forests they present at the end of the article. Specifically, I 1) update the code so it runs in the latest version of pandas and Python, 2) write detailed comments explaining what is happening in each step, and 3) expand the code in a number of ways. Let's get started! A Note About The Data The data for this tutorial is famous. Called, the iris dataset , it contains four variables measuring various parts of iris flowers of three related species, and then a fourth variable with the species name. The reason it is so famous in machine learning and statistics communities is because the data requires very little preprocessing (i.e. no missing values, all features are floating numbers, etc.). Preliminaries # Load the library with the iris dataset from sklearn.datasets import load_iris # Load scikit's random forest classifier library from sklearn.ensemble import RandomForestClassifier # Load pandas import pandas as pd # Load numpy import numpy as np Load Data # Create an object called iris with the iris data iris = load_iris () # Create a dataframe with the four feature variables df = pd . DataFrame ( iris . data , columns = iris . feature_names ) # View the top 5 rows df . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 # Add a new column with the species names, this is what we are going to try to predict df [ 'species' ] = pd . Categorical . from_codes ( iris . target , iris . target_names ) # View the top 5 rows df . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 setosa 1 4.9 3.0 1.4 0.2 setosa 2 4.7 3.2 1.3 0.2 setosa 3 4.6 3.1 1.5 0.2 setosa 4 5.0 3.6 1.4 0.2 setosa Create Training And Test Data # Create a new column that for each row, generates a random number between 0 and 1, and # if that value is less than or equal to .75, then sets the value of that cell as True # and false otherwise. This is a quick and dirty way of randomly assigning some rows to # be used as the training data and some as the test data. df [ 'is_train' ] = np . random . uniform ( 0 , 1 , len ( df )) <= . 75 # View the top 5 rows df . head () sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species is_train 0 5.1 3.5 1.4 0.2 setosa True 1 4.9 3.0 1.4 0.2 setosa True 2 4.7 3.2 1.3 0.2 setosa True 3 4.6 3.1 1.5 0.2 setosa True 4 5.0 3.6 1.4 0.2 setosa True # Create two new dataframes, one with the training rows, one with the test rows train , test = df [ df [ 'is_train' ] == True ], df [ df [ 'is_train' ] == False ] # Show the number of observations for the test and training dataframes print ( 'Number of observations in the training data:' , len ( train )) print ( 'Number of observations in the test data:' , len ( test )) Number of observations in the training data: 107 Number of observations in the test data: 43 Preprocess Data # Create a list of the feature column's names features = df . columns [: 4 ] features Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], dtype='object') # train['species'] contains the actual species names. Before we can use it, # we need to convert each species name into a digit. So, in this case there # are three species, which have been coded as 0, 1, or 2. y = pd . factorize ( train [ 'species' ])[ 0 ] y array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) Train The Random Forest Classifier # Create a random forest classifier. By convention, clf means 'classifier' clf = RandomForestClassifier ( n_jobs = 2 ) # Train the classifier to take the training features and learn how they relate # to the training y (the species) clf . fit ( train [ features ], y ) RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None, max_features='auto', max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=2, oob_score=False, random_state=None, verbose=0, warm_start=False) Huzzah! We have done it! We have officially trained our random forest classifier! Now let's play with it. The classifier model itself is stored in the clf variable. Apply classifier To Test Data If you have been following along, you will know we only trained our classifier on part of the data, leaving the rest out. This is, in my humble opinion, the most important part of machine learning. Why? Because by leaving out a portion of the data, we have a set of data to test the accuracy of our model! Let's do that now. # Apply the classifier we trained to the test data (which, remember, it has never seen before) clf . predict ( test [ features ]) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]) What are you looking at above? Remember that we coded each of the three species of plant as 0, 1, or 2. What the list of numbers above is showing you is what species our model predicts each plant is based on the the sepal length, sepal width, petal length, and petal width. How confident is the classifier about each plant? We can see that too. # View the predicted probabilities of the first 10 observations clf . predict_proba ( test [ features ])[ 0 : 10 ] array([[ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 0.9, 0.1, 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ], [ 1. , 0. , 0. ]]) There are three species of plant, thus [ 1. , 0. , 0. ] tells us that the classifier is certain that the plant is the first class. Taking another example, [ 0.9, 0.1, 0. ] tells us that the classifier gives a 90% probability the plant belongs to the first class and a 10% probability the plant belongs to the second class. Because 90 is greater than 10, the classifier predicts the plant is the first class. Evaluate classifier Now that we have predicted the species of all plants in the test data, we can compare our predicted species with the that plant's actual species. # Create actual english names for the plants for each predicted plant class preds = iris . target_names [ clf . predict ( test [ features ])] # View the PREDICTED species for the first five observations preds [ 0 : 5 ] array(['setosa', 'setosa', 'setosa', 'setosa', 'setosa'], dtype='<U10') # View the ACTUAL species for the first five observations test [ 'species' ] . head () 14 setosa 18 setosa 20 setosa 22 setosa 25 setosa Name: species, dtype: category Categories (3, object): [setosa, versicolor, virginica] That looks pretty good! At least for the first five observations. Now let's use look at all the data. Create a confusion matrix A confusion matrix can be, no pun intended, a little confusing to interpret at first, but it is actually very straightforward. The columns are the species we predicted for the test data and the rows are the actual species for the test data. So, if we take the top row, we can wee that we predicted all 20 setosa plants in the test data perfectly. However, in the next row, we predicted 17 of the versicolor plants correctly, but mis-predicted two of the versicolor plants as virginica. The short explanation of how to interpret a confusion matrix is: anything on the diagonal was classified correctly and anything off the diagonal was classified incorrectly. # Create confusion matrix pd . crosstab ( test [ 'species' ], preds , rownames = [ 'Actual Species' ], colnames = [ 'Predicted Species' ]) Predicted Species setosa versicolor virginica Actual Species setosa 11 0 0 versicolor 0 16 1 virginica 0 0 15 View Feature Importance While we don't get regression coefficients like with OLS, we do get a score telling us how important each feature was in classifying. This is one of the most powerful parts of random forests, because we can clearly see that petal width was more important in classification than sepal width. # View a list of the features and their importance scores list ( zip ( train [ features ], clf . feature_importances_ )) [('sepal length (cm)', 0.13356069065846765), ('sepal width (cm)', 0.04486948688226873), ('petal length (cm)', 0.37067096905488794), ('petal width (cm)', 0.45089885340437574)]","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/random_forest_classifier_example_scikit.html"},{"title":"Construct A Dictionary From Multiple Lists","loc":"http://chrisalbon.com/python/construct_a_dictionary_from_multiple_lists_python.html","text":"Create Two Lists # Create a list of theofficer's name officer_names = [ 'Sodoni Dogla' , 'Chris Jefferson' , 'Jessica Billars' , 'Michael Mulligan' , 'Steven Johnson' ] # Create a list of the officer's army officer_armies = [ 'Purple Army' , 'Orange Army' , 'Green Army' , 'Red Army' , 'Blue Army' ] Construct A Dictionary From The Two Lists # Create a dictionary that is the zip of the two lists dict ( zip ( officer_names , officer_armies )) {'Chris Jefferson': 'Orange Army', 'Jessica Billars': 'Green Army', 'Michael Mulligan': 'Red Army', 'Sodoni Dogla': 'Purple Army', 'Steven Johnson': 'Blue Army'}","tags":"Python","url":"http://chrisalbon.com/python/construct_a_dictionary_from_multiple_lists_python.html"},{"title":"Cross Validation With Parameter Tuning Using Grid Search","loc":"http://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html","text":"In machine learning, two tasks are commonly done at the same time in data pipelines: cross validation and (hyper)parameter tuning. Cross validation is the process of training learners using one set of data and testing it using a different set. Parameter tuning is the process to selecting the values for a model's parameters that maximize the accuracy of the model. In this tutorial we work through an example which combines cross validation and parameter tuning using scikit-learn. Note: This tutorial is based on examples given in the scikit-learn documentation . I have combined a few examples in the documentation, simplified the code, and added extensive explanations/code comments. Preliminaries import numpy as np from sklearn.grid_search import GridSearchCV from sklearn import datasets , svm import matplotlib.pyplot as plt Create Two Datasets In the code below, we load the digits dataset , which contains 64 feature variables. Each feature denotes the darkness of a pixel in an 8 by 8 image of a handwritten digit. We can see these features for the first observation: # Load the digit data digits = datasets . load_digits () # View the features of the first observation digits . data [ 0 : 1 ] array([[ 0., 0., 5., 13., 9., 1., 0., 0., 0., 0., 13., 15., 10., 15., 5., 0., 0., 3., 15., 2., 0., 11., 8., 0., 0., 4., 12., 0., 0., 8., 8., 0., 0., 5., 8., 0., 0., 9., 8., 0., 0., 4., 11., 0., 1., 12., 7., 0., 0., 2., 14., 5., 10., 12., 0., 0., 0., 0., 6., 13., 10., 0., 0., 0.]]) The target data is a vector containing the image's true digit. For example, the first observation is a handwritten digit for '0'. # View the target of the first observation digits . target [ 0 : 1 ] array([0]) To demonstrate cross validation and parameter tuning, first we are going to divide the digit data into two datasets called data1 and data2 . data1 contains the first 1000 rows of the digits data, while data2 contains the remaining ~800 rows. Note that this split is separate to the cross validation we will conduct and is done purely to demonstrate something at the end of the tutorial. In other words, don't worry about data2 for now, we will come back to it. # Create dataset 1 data1_features = digits . data [: 1000 ] data1_target = digits . target [: 1000 ] # Create dataset 2 data2_features = digits . data [ 1000 :] data2_target = digits . target [ 1000 :] Create Parameter Candidates Before looking for which combination of parameter values produces the most accurate model, we must specify the different candidate values we want to try. In the code below we have a number of candidate parameter values, including four different values for C ( 1, 10, 100, 1000 ), two values for gamma ( 0.001, 0.0001 ), and two kernels ( linear, rbf ). The grid search will try all combinations of parameter values and select the set of parameters which provides the most accurate model. parameter_candidates = [ { 'C' : [ 1 , 10 , 100 , 1000 ], 'kernel' : [ 'linear' ]}, { 'C' : [ 1 , 10 , 100 , 1000 ], 'gamma' : [ 0.001 , 0.0001 ], 'kernel' : [ 'rbf' ]}, ] Conduct Grid Search To Find Parameters Producing Highest Score Now we are ready to conduct the grid search using scikit-learn's GridSearchCV which stands for grid search cross validation. By default, the GridSearchCV 's cross validation uses 3-fold KFold or StratifiedKFold depending on the situation. # Create a classifier object with the classifier and parameter candidates clf = GridSearchCV ( estimator = svm . SVC (), param_grid = parameter_candidates , n_jobs =- 1 ) # Train the classifier on data1's feature and target data clf . fit ( data1_features , data1_target ) GridSearchCV(cv=None, error_score='raise', estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=None, degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False), fit_params={}, iid=True, n_jobs=-1, param_grid=[{'kernel': ['linear'], 'C': [1, 10, 100, 1000]}, {'kernel': ['rbf'], 'gamma': [0.001, 0.0001], 'C': [1, 10, 100, 1000]}], pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0) Success! We have our results! First, let's look at the accuracy score when we apply the model to the data1 's test data. # View the accuracy score print ( 'Best score for data1:' , clf . best_score_ ) Best score for data1: 0.942 Which parameters are the best? We can tell scikit-learn to display them: # View the best parameters for the model found using grid search print ( 'Best C:' , clf . best_estimator_ . C ) print ( 'Best Kernel:' , clf . best_estimator_ . kernel ) print ( 'Best Gamma:' , clf . best_estimator_ . gamma ) Best C: 10 Best Kernel: rbf Best Gamma: 0.001 This tells us that the most accurate model uses C=10 , the rbf kernel, and gamma=0.001 . Sanity Check Using Second Dataset Remember the second dataset we created? Now we will use it to prove that those parameters are actually used by the model. First, we apply the classifier we just trained to the second dataset. Then we will train a new support vector classifier from scratch using the parameters found using the grid search. We should get the same results for both models. # Apply the classifier trained using data1 to data2, and view the accuracy score clf . score ( data2_features , data2_target ) 0.96988707653701378 # Train a new classifier using the best parameters found by the grid search svm . SVC ( C = 10 , kernel = 'rbf' , gamma = 0.001 ) . fit ( data1_features , data1_target ) . score ( data2_features , data2_target ) 0.96988707653701378 Success!","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html"},{"title":"Exiting A Loop","loc":"http://chrisalbon.com/python/exiting_a_loop_python.html","text":"Create A List # Create a list: armies = [ 'Red Army' , 'Blue Army' , 'Green Army' ] Breaking Out Of A For Loop for army in armies : print ( army ) if army == 'Blue Army' : print ( 'Blue Army Found! Stopping.' ) break Red Army Blue Army Blue Army Found! Stopping. Notice that the loop stopped after the conditional if statement was satisfied. Exiting If Loop Completed A loop will exit when completed, but using an else statement we can add an action at the conclusion of the loop if it hasn't been exited earlier. for army in armies : print ( army ) if army == 'Orange Army' : break else : print ( 'Looped Through The Whole List, No Orange Army Found' ) Red Army Blue Army Green Army Looped Through The Whole List, No Orange Army Found","tags":"Python","url":"http://chrisalbon.com/python/exiting_a_loop_python.html"},{"title":"Impute Missing Values Using K-Nearest Neighbors","loc":"http://chrisalbon.com/machine-learning/impute_missing_values_with_k-nearest_neighbors.html","text":"Nearest neighbor imputations which weights samples using the mean squared difference on features for which two rows both have observed data. In this example, we use 3 nearest rows which have a feature to fill in each row's missing features. Preliminaries import pandas as pd import numpy as np from fancyimpute import KNN import numpy as np import matplotlib.pyplot as plt % matplotlib inline Using Theano backend. Create Data df = pd . DataFrame () df [ 'x0' ] = [ 0.3051 , 0.4949 , 0.6974 , 0.3769 , 0.2231 , 0.341 , 0.4436 , 0.5897 , 0.6308 , 0.5 ] df [ 'x1' ] = [ np . nan , 0.2654 , 0.2615 , 0.5846 , 0.4615 , 0.8308 , 0.4962 , 0.3269 , 0.5346 , 0.6731 ] df . head () x0 x1 0 0.3051 NaN 1 0.4949 0.2654 2 0.6974 0.2615 3 0.3769 0.5846 4 0.2231 0.4615 View The Raw Data # Create data, with the first observation containing a missing value X = df . as_matrix ( columns = [ 'x0' , 'x1' ]) # View data X array([[ 0.3051, nan], [ 0.4949, 0.2654], [ 0.6974, 0.2615], [ 0.3769, 0.5846], [ 0.2231, 0.4615], [ 0.341 , 0.8308], [ 0.4436, 0.4962], [ 0.5897, 0.3269], [ 0.6308, 0.5346], [ 0.5 , 0.6731]]) # Plot data plt . scatter ( X [:, 0 ], X [:, 1 ]) <matplotlib.collections.PathCollection at 0x1191cf780> Impute Using K-Nearest Neighbors # Imput missing values from three closest observations X_imputed = KNN ( k = 3 ) . complete ( X ) # View new data X_imputed Computing pairwise distances between 10 samples Computing distances for sample #1/10, elapsed time: 0.000 Imputing row 1/10 with 1 missing columns, elapsed time: 0.002 array([[ 0.3051 , 0.73900744], [ 0.4949 , 0.2654 ], [ 0.6974 , 0.2615 ], [ 0.3769 , 0.5846 ], [ 0.2231 , 0.4615 ], [ 0.341 , 0.8308 ], [ 0.4436 , 0.4962 ], [ 0.5897 , 0.3269 ], [ 0.6308 , 0.5346 ], [ 0.5 , 0.6731 ]]) Notice that the first observation previously contained a missing value. However, now it contains a new value: 0.73900744 View Imported Value n = [ ' Imputed X1' , '' , '' , '' , '' , '' , '' , '' , '' , '' ] fig , ax = plt . subplots () ax . scatter ( X_imputed [:, 0 ], X_imputed [:, 1 ]) for i , txt in enumerate ( n ): ax . annotate ( txt , ( X_imputed [:, 0 ][ i ], X_imputed [:, 1 ][ i ]))","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/impute_missing_values_with_k-nearest_neighbors.html"},{"title":"Iterating Over Dictionary Keys","loc":"http://chrisalbon.com/python/iterating_over_dictionary_keys_python.html","text":"Create A Dictionary Officers = { 'Michael Mulligan' : 'Red Army' , 'Steven Johnson' : 'Blue Army' , 'Jessica Billars' : 'Green Army' , 'Sodoni Dogla' : 'Purple Army' , 'Chris Jefferson' : 'Orange Army' } Officers {'Chris Jefferson': 'Orange Army', 'Jessica Billars': 'Green Army', 'Michael Mulligan': 'Red Army', 'Sodoni Dogla': 'Purple Army', 'Steven Johnson': 'Blue Army'} Use Dictionary Comprehension # Display all dictionary entries where the key doesn't start with 'Chris' { keys : Officers [ keys ] for keys in Officers if not keys . startswith ( 'Chris' )} {'Jessica Billars': 'Green Army', 'Michael Mulligan': 'Red Army', 'Sodoni Dogla': 'Purple Army', 'Steven Johnson': 'Blue Army'} Notice that the entry for 'Chris Jefferson' is not returned.","tags":"Python","url":"http://chrisalbon.com/python/iterating_over_dictionary_keys_python.html"},{"title":"Looping Over Two Lists","loc":"http://chrisalbon.com/python/looping_over_two_lists_using_Python.html","text":"# Create a list of length 3: armies = [ 'Red Army' , 'Blue Army' , 'Green Army' ] # Create a list of length 4: units = [ 'Red Infantry' , 'Blue Armor' , 'Green Artillery' , 'Orange Aircraft' ] # For each element in the first list, for army , unit in zip ( armies , units ): # Display the corresponding index element of the second list: print ( army , 'has the following options:' , unit ) Red Army has the following options: Red Infantry Blue Army has the following options: Blue Armor Green Army has the following options: Green Artillery Notice that the fourth item of the second list, orange aircraft , did not display.","tags":"Python","url":"http://chrisalbon.com/python/looping_over_two_lists_using_Python.html"},{"title":"K-Nearest Neighbors Classification","loc":"http://chrisalbon.com/machine-learning/k-nearest_neighbors_using_scikit_pandas.html","text":"K-nearest neighbors classifier (KNN) is a simple and powerful classification learner. KNN has three basic parts: \\(y_i\\) : The class of an observation (what we are trying to predict in the test data). \\(X_i\\) : The predictors/IVs/attributes of an observation. \\(K\\) : A positive number specified by the researcher. K denotes the number of observations closest to a particular observation that define its \"neighborhood\". For example, K=2 means that each observation's has a neighorhood comprising of the two other observations closest to it. Imagine we have an observation where we know its independent variables \\(x_{test}\\) but do not know its class \\(y_{test}\\) . The KNN learner finds the K other observations that are closest to \\(x_{test}\\) and uses their known classes to assign a classes to \\(x_{test}\\) . Preliminaries import pandas as pd from sklearn import neighbors import numpy as np % matplotlib inline import seaborn Create Dataset Here we create three variables, test_1 and test_2 are our independent variables, 'outcome' is our dependent variable. We will use this data to train our learner. training_data = pd . DataFrame () training_data [ 'test_1' ] = [ 0.3051 , 0.4949 , 0.6974 , 0.3769 , 0.2231 , 0.341 , 0.4436 , 0.5897 , 0.6308 , 0.5 ] training_data [ 'test_2' ] = [ 0.5846 , 0.2654 , 0.2615 , 0.4538 , 0.4615 , 0.8308 , 0.4962 , 0.3269 , 0.5346 , 0.6731 ] training_data [ 'outcome' ] = [ 'win' , 'win' , 'win' , 'win' , 'win' , 'loss' , 'loss' , 'loss' , 'loss' , 'loss' ] training_data . head () test_1 test_2 outcome 0 0.3051 0.5846 win 1 0.4949 0.2654 win 2 0.6974 0.2615 win 3 0.3769 0.4538 win 4 0.2231 0.4615 win Plot the data This is not necessary, but because we only have three variables, we can plot the training dataset. The X and Y axes are the independent variables, while the colors of the points are their classes. seaborn . lmplot ( 'test_1' , 'test_2' , data = training_data , fit_reg = False , hue = \"outcome\" , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) <seaborn.axisgrid.FacetGrid at 0x11553bf98> Convert Data Into np.arrays The scikit-learn library requires the data be formatted as a numpy array. Here are doing that reformatting. X = training_data . as_matrix ( columns = [ 'test_1' , 'test_2' ]) y = np . array ( training_data [ 'outcome' ]) Train The Learner This is our big moment. We train a KNN learner using the parameters that an observation's neighborhood is its three closest neighors. weights = 'uniform' can be thought of as the voting system used. For example, uniform means that all neighbors get an equally weighted \"vote\" about an observation's class while weights = 'distance' would tell the learner to weigh each observation's \"vote\" by its distance from the observation we are classifying. clf = neighbors . KNeighborsClassifier ( 3 , weights = 'uniform' ) trained_model = clf . fit ( X , y ) View The Model's Score How good is our trained model compared to our training data? trained_model . score ( X , y ) 0.80000000000000004 Our model is 80% accurate! Note: that in any real world example we'd want to compare the trained model to some holdout test data. But since this is a toy example I used the training data . Apply The Learner To A New Data Point Now that we have trained our model, we can predict the class any new observation, \\(y_{test}\\) . Let us do that now! # Create a new observation with the value of the first independent variable, 'test_1', as .4 # and the second independent variable, test_1', as .6 x_test = np . array ([[ . 4 , . 6 ]]) # Apply the learner to the new, unclassified observation. trained_model . predict ( x_test ) array(['loss'], dtype=object) Huzzah! We can see that the learner has predicted that the new observation's class is loss . We can even look at the probabilities the learner assigned to each class: trained_model . predict_proba ( x_test ) array([[ 0.66666667, 0.33333333]]) According to this result, the model predicted that the observation was loss with a ~67% probability and win with a ~33% probability. Because the observation had a greater probability of being loss , it predicted that class for the observation. Notes The choice of K has major affects on the classifer created. The greater the K, more linear (high bias and low variance) the decision boundary. There are a variety of ways to measure distance, two popular being simple euclidean distance and cosine similarity. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/k-nearest_neighbors_using_scikit_pandas.html"},{"title":"Bessel's Correction","loc":"http://chrisalbon.com/frequentist-statistics/bessels_correction.html","text":"Bessel's correction is the reason we use \\(n-1\\) instead of \\(n\\) in the calculations of sample variance and sample standard deviation. Sample variance: $$ s&#94;2 = \\frac {1}{n-1} \\sum_{i=1}&#94;n \\left(x_i - \\overline{x} \\right)&#94; 2 $$ When we calculate sample variance, we are attempting to estimate the population variance, an unknown value. To make this estimate, we estimate this unknown population variance from the mean of the squared deviations of samples from the overall sample mean. A negative sideffect of this estimation technique is that, because we are taking a sample, we are a more likely to observe observations with a smaller deviation because they are more common (e.g. they are the center of the distribution). The means that by definiton we will underestimate the population variance. Friedrich Bessel figured out that by multiplying a biased (uncorrected) sample variance \\(s_n&#94;2 = \\frac {1}{n} \\sum_{i=1}&#94;n \\left(x_i - \\overline{x} \\right)&#94; 2\\) by \\(\\frac{n}{n-1}\\) we will be able to reduce that bias and thus be able to make an accurate estimate of the population variance and standard deviation. The end result of that multiplication is the unbiased sample variance. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Frequentist Statistics","url":"http://chrisalbon.com/frequentist-statistics/bessels_correction.html"},{"title":"Linear Regression In scikit-learn","loc":"http://chrisalbon.com/machine-learning/linear_regression_scikit-learn.html","text":"Sources: scikit-learn , DrawMyData . The purpose of this tutorial is to give a brief introduction into the logic of statistical model building used in machine learning. If you want to read more about the theory behind this tutorial, check out An Introduction To Statistical Learning . Let us get started. Preliminary import pandas as pd from sklearn import linear_model import random import numpy as np % matplotlib inline Load Data With those libraries added, let us load the dataset (the dataset is avaliable in his site's GitHub repo). # Load the data df = pd . read_csv ( '../data/simulated_data/battledeaths_n300_cor99.csv' ) # Shuffle the data's rows (This is only necessary because of the way I created # the data using DrawMyData. This would not normally be necessary with a real analysis). df = df . sample ( frac = 1 ) hi Explore Data Let us take a look at the first few rows of the data just to get an idea about it. # View the first few rows df . head () friendly_battledeaths enemy_battledeaths 173 28.4615 26.5385 257 9.7436 7.6923 162 20.0000 10.3846 12 12.8205 10.3846 111 63.8462 61.9231 Now let us plot the data so we can see it's structure. # Plot the two variables against eachother df . plot ( x = 'friendly_battledeaths' , y = 'enemy_battledeaths' , kind = 'scatter' ) <matplotlib.axes._subplots.AxesSubplot at 0x11a9fc710> Break Data Up Into Training And Test Datasets Now for the real work. To judge how how good our model is, we need something to test it against. We can accomplish this using a technique called cross-validation. Cross-validation can get much more complicated and powerful, but in this example we are going do the most simple version of this technique. Steps Divide the dataset into two datasets: A 'training' dataset that we will use to train our model and a 'test' dataset that we will use to judge the accuracy of that model. Train the model on the 'training' data. Apply that model to the test data's X variable, creating the model's guesses for the test data's Ys. Compare how close the model's predictions for the test data's Ys were to the actual test data Ys. # Create our predictor/independent variable # and our response/dependent variable X = df [ 'friendly_battledeaths' ] y = df [ 'enemy_battledeaths' ] # Create our test data from the first 30 observations X_test = X [ 0 : 30 ] . reshape ( - 1 , 1 ) y_test = y [ 0 : 30 ] # Create our training data from the remaining observations X_train = X [ 30 :] . reshape ( - 1 , 1 ) y_train = y [ 30 :] Train The Linear Model Let us train the model using our training data. # Create an object that is an ols regression ols = linear_model . LinearRegression () # Train the model using our training data model = ols . fit ( X_train , y_train ) View The Results Here are some basic outputs of the model, notably the coefficient and the R-squared score. # View the training model's coefficient model . coef_ array([ 0.9770556]) # View the R-Squared score model . score ( X_test , y_test ) 0.98719951914847326 Now that we have used the training data to train a model, called model , we can apply it to the test data's Xs to make predictions of the test data's Ys. Previously we used X_train and y_train to train a linear regression model, which we stored as a variable called model . The code model.predict(X_test) applies the trained model to the X_test data, data the model has never seen before to make predicted values of Y. This can easily be seen by simply running the code: # Run the model on X_test and show the first five results list ( model . predict ( X_test )[ 0 : 5 ]) [27.238901424783169, 8.9504723858776192, 18.971545454685064, 11.956774765407825, 61.811720758841012] This array of values is the model's best guesses for the values of the test data's Ys. Compare them to the actual test data Y values: # View the first five test Y values list ( y_test )[ 0 : 5 ] [26.538499999999999, 7.6923000000000004, 10.384600000000001, 10.384600000000001, 61.923099999999998] Our model predicts that the first observation in the test data will have a Y value of 27.24. Actual Y value for that observation was 26.54. That means the model's prediction was \\(27.24 - 26.54 = 0.8\\) off, this is called the residual of that observation. The difference between the model's predicted values and the actual values is how is we judge as model's accuracy, because a perfectly accurate model would have residuals of zero. However, to judge a model, we want a single statistic (number) that we can use as a measure. We want this measure to capture the difference between the predicted values and the actual values across all observations in the data. The most common statistic used for quantitative Ys is the residual sum of squares : $$ RSS = \\sum_{i=1}&#94;{n}(y_{i}-f(x_{i}))&#94;{2} $$ Don't let the mathematical notation throw you off: \\(f(x_{i})\\) is the model we trained: model.predict(X_test) \\(y_{i}\\) is the test data's y: y_test \\(&#94;{2}\\) is the exponent: **2 \\(\\sum_{i=1}&#94;{n}\\) is the summation: .sum() In the residual sum of squares, for each observation we find the difference between the model's predicted Y and the actual Y, then square that difference to make all the values positive. Then we add all those squared differences together to get a single number. The final result is a statistic representing how far the model's predictions were from the real values. # Apply the model we created using the training data # to the test data, and calculate the RSS. (( y_test - model . predict ( X_test )) ** 2 ) . sum () 352.1634102396179 Note: You can also use Mean Squared Error, which is RSS divided by the degrees of freedom. But I find it helpful to think in terms of RSS. # Calculate the MSE np . mean (( model . predict ( X_test ) - y_test ) ** 2 ) 11.738780341320597 What does the model's RSS of 352.16 mean? Mathematically, it is the sum of the squared errors (obviously). But substantly 352.16 has little real meaning. Then why is RSS so fundamental to everything we do? Because it lets us compare models. Does 352.16 mean our model is good? On it's own we don't realy have a good answer. But what if we trained a second model -- with different independent variables -- and applied that model to the same test data and got an RSS of 200? Then we would know that the second model is better! And that hunt for the best model is very often our goal. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Machine Learning","url":"http://chrisalbon.com/machine-learning/linear_regression_scikit-learn.html"},{"title":"Basic Mathematical Terms In Python","loc":"http://chrisalbon.com/mathematics/basic_mathematical_terms_in_python.html","text":"import math Ratio Two quantities divided by eachother, \\(\\frac{x}{y}\\) . Often written as \\(x:y\\) . x = 2 y = 3 x / y 0.6666666666666666 Proportion One variable divided by the sum of itself and another variable: \\(\\left | \\frac{x}{x+y} \\right |\\) . Proportions range from 0 to 1. x = 2 y = 3 abs ( x / ( x + y )) 0.4 Percentage Proportions multiplied by 100: $\\left | \\frac{x}{x+y} \\right | \\times 100 $ x = 2 y = 3 abs ( x / ( x + y )) * 100 40.0 Functions Functions assigns something to each element of a domain. \\(f\\left ( x \\right ): A \\rightarrow B\\) means \"Function x maps A to B\" A = 4 # Create a function called x def x ( _range ): B = 5 return _range + B x ( A ) 9 Exponential Function Multiplication of a number by itself. Example: \\(y&#94;{3} = y \\times y \\times y\\) y = 3 # y*y*y y ** 3 27 Logarithmic Function Inverse of exponential function, transforming exponential functions to linear functions. Logarithms tell you how many times to multiple a base to get a value. Example: \\(log_{a}a&#94;{x} = x\\) a = 5.0 x = 4.0 math . log ( a ** x , a ) 4.0 Natural Log Logarithmic Function with a base of \\(e\\) , where \\(e\\approx 2.7183\\) x = 4 math . log ( x ) 1.3862943611198906 Sequence An ordered list of numbers. Example: \\(x = \\left \\{ 1,5,100,2,5 \\right \\}\\) . The \\(ith\\) individual member of a sequence, \\(x\\) is denoted \\(x_{i}\\) . x = [ 1 , 5 , 100 , 2 , 5 ] x [1, 5, 100, 2, 5] Series The sum of a sequence, or put another way, a sequence with plus signs between elements. Example: \\(\\sum_{i=1}&#94;{n}x_{i}\\) is the sum of the first to the \\(nth\\) member of a sequence, \\(x\\) . x = [ 1 , 5 , 100 , 2 , 5 ] sum ( x ) 113 Derivative The instanteous rate of change of a function. Can be thought of as the slope between two points, \\(a\\) and \\(b\\) as the distance between them approaches being infinitely small. Definite integral The area under a curve. Local extrema, local maxima, local minima A high or low point of a function. Global extrema, global maxima, global minima The highest or lowest point of a function. Taylor Series Describing a function by using the information contained in the deriviatives of the function. Critical Point Any point in a function where the first derivative is 0 or doesn't exist. Basically any point where something interesting might happen. Inflection Point A point of a function where the curve switches from concave to convex or vice versa. Stationary Point A point of a function where the first derivative is 0. Can be thought of as either a peak or a valley floor. Saddle Point A point of a functon where both the first and second derivative is 0. Concave A function where the rate of increase slows as the value of the function gets bigger. Another way to think about it: if plotted in 2d, a convex function curves towards the ground. Convex A function where the rate of increase speeds up as the value of the function gets bigger. Another way to think about it: if plotted in 2d, a convex function curves towards the sky. Conditional Probability The probability an event occurs given whether or not another event occurs. \\(Pr\\left ( y\\mid x = 4 \\right )\\) is the probability y occurs given that the event x=4 occurs. Combination From n items, choose k items, ignoring the order in which you choose them: \\(\\binom{n}{k}\\) Odds and Odds Ratios The odds of an event are the probability of an event occurs over the probability of an event not occuring: \\(\\frac{Pr(x)}{Pr(~x)}\\) The odds ratio of two events, \\(x\\) and \\(y\\) are the odds of the two events: \\(\\frac{Pr(x)/Pr(~x)}{Pr(y)/Pr(~y)}\\) Random Variable A random variable is a variable that can take an array of values, with the probability it takes a particular value determined by some random process. A distribution of a random variable is the probability of each possible value being realized. Probability Mass Function & Probability Distribution Function A function that defines the probability of each possible value occuring in a discrete (PMF) or continous (PDF) function. Bernoulli Distribution Applies only to binary random variables only (e.g. coin flips). It's PMF is written as: $$ Pr(Y = y\\mid p)=\\begin{cases} 1-p & \\text{ for } y = 0\\\\ p & \\text{ for } y= 1 \\end{cases} $$ The Bernoulli distribution describes the ferquencey of two outcomes over repeated observations. It is built upon the assumption that the events of independent of eachother, meaning the outcome of one coin flip doesn't not affect the outcome of the second coin flip (from Moore and Siegel). Binomial Distribution The PMF of the binomial distribution is: $$ Pr(Y = y\\mid n,p) = \\binom{n}{y}p&#94;{y}(1-p)&#94;{n-y}$$ The binomial distribution describes any discrete distribution with three or more observations where (1) each observation is composed of a binary outcome, (2) the observations are independent, and (3) we have a record of the number of times one value was obtained (from Moore and Siegel). Poisson Distribution The Poisson distribution the number of times you observe one event, two events, three events, etc over a fixed period of time. Put another way, it describes the distribution of event counts for rare, random events (from Moore and Siegel). $$Pr(Y=y|u)=\\frac{u&#94;y}{y!\\times e&#94;u}$$ For example, the Poisson distribution would describe the probability of observing 10 wars in a century. Negative Binomial Distribution The NB distribution describes the number of events occuring prior to observing the kth non-event. For example, the number of successful surgeries performed before the 6th failed surgery. Expectation Of Random Variables The most likely value a random variable takes. Written \\(E[X]\\) . Moments Of A Distribution Moments of a distribution are the parameters used to describe a distribution. First moment: Mean Second moment: Variance Third moment: Skewness Fourth moment: Kurtosis (flatness or peakedness) Uniform Distribution Assigns an equal probability to all possible events. Gaussian-Normal Distribution The bell curve. Defined by two moments: mean and variance. Logistic Distribution Often used to model binary outcomes. Defined by two moments: mean and variance. Exponential Distribution The exponential distribution describes events produced by a process with a constant risk of failure (from Moore and Siegel). Pareto Distribution A distribution in which prior values affect later values. Gamma Distribution While the exponential distribution assumes contant risk, the Gamma distribution allows risk to vary over a number of periods. Weibull Distribution \"[Weibull distribution] can be interpreted as if several processes are running in parallel, with the first to stop ending the duration. This is the weakest link mechanism, as when the failure of some part causes a machine to break down and the total operating time of the machine is the duration.\" (Lindsey 1995, p. 133) Chi-squared Distribution \"The sum of squares of n independent variables each distributed according to a standard normal distribution is distributed according to a chi-squared distribution.\" (Moore and Siegel) Student's t Distribution This distribution looks like a normal distribution when N is big enough, but with thicker tails when N is small. Vector An 'arrow' in n-dimensional space. For example, in a 2d coordinate plane, the vector (2,3) is x=2 and y=3. Length Of A Vector If you think of a 2d coordinate plane, and have a vector (x,y), then you have a point at x,y. The distance from 0 (the origin) to x,y is the vector's length. The length of vector \\(a\\) is denoted \\(\\left \\| a \\right \\|\\) and is calculated \\(\\sqrt{a_{1}&#94;{2}+a_{2}&#94;{2}+...+a_{n}&#94;{2}}\\) Vector Addition To add two vectors, add each corresponding element. For example, for vectors \\(\\boldsymbol{a}\\) and \\(\\boldsymbol{b}\\) , the sum would be \\((a_{1}+b_{1}, a_{2}+b_{2}, ... a_{n}+b_{n})\\) To add the lengths of two vectors, the process is the same: $$\\left \\| \\boldsymbol{a} + \\boldsymbol{b} \\right \\| = \\sqrt{(a_{1}+b_{1})&#94;{2}+(a_{2}+b_{2})&#94;{2}+...+(a_{n}+b_{n})&#94;{2}}$$ Multiply A Scalar By A Vector Simply multiply each element of the vector by the scalar. For example: $$c\\boldsymbol{x} = (cx_{1}, cx_{2}, ... cx_{x})$$ Dot Product One way of thinking about the dot product is that if two vectors are imagined as two arrows pointing out into a 2nd coordinate plane, the dot product is the vector starting at one endpoint and going to the other endpoint. Because if two vectors are perpendicular then the dot product is 0 (because the cosine of the angle they create is 0), the dot product is used to tell you if two vectors are perpendicular/orthogonal/independent. $$\\boldsymbol{a} \\cdot \\boldsymbol{b} = a_{1}b_{1} + a_{2}b_{2} + ... a_{n}b_{n}$$ Matrix A matrix is a rectangular table of numbers of variables that contains rows and columns. It is typically described as an \\(n \\times m\\) matrix, meaning \\(n\\) nows and \\(m\\) columns. $$A_{3x3} = \\begin{pmatrix} a_{11} & a_{12} & a_{31}\\\\ a_{21} & a_{22}& a_{32}\\\\ a_{31} & a_{32} & a_{33} \\end{pmatrix}$$ Scalar Matrix Multiplication To multiply a scalar by a matrix, simply multiply each element of the matrix by the scalar. $$2 \\times \\begin{bmatrix} 1 & 2\\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 2 & 4\\\\ 6 & 8 \\end{bmatrix}$$ Matrix Multiplication To multiply two matrices, it is simply the dot product of the each of first matrix's rows and the second matrix's columns. $$\\begin{bmatrix} 1 & 2\\\\ 3 & 4\\\\ 5 & 6 \\end{bmatrix}\\times \\begin{bmatrix} 1 & 2& 3\\\\ 4 & 5& 6 \\end{bmatrix} = \\begin{bmatrix} (1\\times1) + (2\\times4)& (1\\times2) + (2\\times5)& (1\\times3) + (2\\times6)\\\\ (3\\times1) + (4\\times4) & (3\\times2) + (4\\times5)& (3\\times3) + (4\\times6)\\\\ (5\\times1) + (6\\times4) & (5\\times2) + (6\\times5) & (5\\times3) + (6\\times6) \\end{bmatrix} $$ Trace Of A Matrix The trace of a matrix is the sum of it's diagonal elements. Identity Matrix A square matrix of \\(1\\) on the diagonal elements and \\(0\\) everywhere else. $$\\begin{bmatrix} 1 & 0\\\\ 0 & 1 \\end{bmatrix}$$ Inverse Of A Matrix The inverse of a matrix, \\(\\boldsymbol{A}\\) , is a second matrix, \\(\\boldsymbol{A}&#94;{-1}\\) , such that \\(\\boldsymbol{A} \\cdot \\boldsymbol{A}&#94;{-1} = \\boldsymbol{I}\\) , where \\(\\boldsymbol{I}\\) is an identity matrix. Matrix Rank The rank of a matrix is the maximum number of linearly independent rows or columns. Eigenvalue and Eigenvector An eigenvalue of a matrix is the solution to \\(A\\boldsymbol{x}=\\lambda\\boldsymbol{x}\\) , where \\(\\boldsymbol{x}\\) is called the eigenvector, and \\(\\lambda\\) is called the eigenvalue. If the eigenvector \\(\\boldsymbol{x}\\) is multiplied by \\(A\\) , then \\(\\boldsymbol{x}\\) ends up as a scalar multiple of itself that always stays pointed in the same direction but just gets longer or shorter. The \\(\\lambda\\) tells us how much longer or shorter it is. In some cases like Markov Chains, researchers think of the eigenvector, \\(\\boldsymbol{x}\\) , as the state of the system, and \\(A\\) as the transition matrix between states. Markov Chains Markov processes are memoryless, meaning there is no dependence between the present state and the past state. A Markov chain is a stochastic process that is memoryless. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Mathematics","url":"http://chrisalbon.com/mathematics/basic_mathematical_terms_in_python.html"},{"title":"Sort A List Of Names By Last Name","loc":"http://chrisalbon.com/python/sort_a_list_by_last_name.html","text":"Create a list of names commander_names = [ \"Alan Brooke\" , \"George Marshall\" , \"Frank Jack Fletcher\" , \"Conrad Helfrich\" , \"Albert Kesselring\" ] Sort Alphabetically By Last Name To complete the sort, we will combine three operations: lambda x: x.split(\" \") , which is a function that takes a string x and breaks it up along each blank space. This outputs a list. [-1] , which takes the last element of a list. sorted() , which sorts a list. # Sort a variable called 'commander_names' by the last elements of each name. sorted ( commander_names , key = lambda x : x . split ( \" \" )[ - 1 ]) ['Alan Brooke', 'Frank Jack Fletcher', 'Conrad Helfrich', 'Albert Kesselring', 'George Marshall']","tags":"Python","url":"http://chrisalbon.com/python/sort_a_list_by_last_name.html"},{"title":"Sort A List Of Strings By Length","loc":"http://chrisalbon.com/python/sort_a_list_of_strings_by_length.html","text":"Create a list of names commander_names = [ \"Alan Brooke\" , \"George Marshall\" , \"Frank Jack Fletcher\" , \"Conrad Helfrich\" , \"Albert Kesselring\" ] Sort Alphabetically By Length To complete the sort, we will combine two operations: lambda x: len(x) , which returns the length of each string. sorted() , which sorts a list. # Sort a variable called 'commander_names' by the length of each string sorted ( commander_names , key = lambda x : len ( x )) ['Alan Brooke', 'George Marshall', 'Conrad Helfrich', 'Albert Kesselring', 'Frank Jack Fletcher']","tags":"Python","url":"http://chrisalbon.com/python/sort_a_list_of_strings_by_length.html"},{"title":"Rename Column Headers In Pandas","loc":"http://chrisalbon.com/python/pandas_rename_column_headers.html","text":"Originally from rgalbo on StackOverflow . Preliminaries # Import required modules import pandas as pd Create example data # Create a values as dictionary of lists raw_data = { '0' : [ 'first_name' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], '1' : [ 'last_name' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], '2' : [ 'age' , 52 , 36 , 24 , 73 ], '3' : [ 'preTestScore' , 24 , 31 , 2 , 3 ]} # Create a dataframe df = pd . DataFrame ( raw_data ) # View a dataframe df 0 1 2 3 0 first_name last_name age preTestScore 1 Molly Jacobson 52 24 2 Tina Ali 36 31 3 Jake Milner 24 2 4 Amy Cooze 73 3 Replace the header value with the first row's values # Create a new variable called 'header' from the first row of the dataset header = df . iloc [ 0 ] 0 first_name 1 last_name 2 age 3 preTestScore Name: 0, dtype: object # Replace the dataframe with a new one which does not contain the first row df = df [ 1 :] # Rename the dataframe's column values with the header variable df . rename ( columns = header ) first_name last_name age preTestScore 1 Molly Jacobson 52 24 2 Tina Ali 36 31 3 Jake Milner 24 2 4 Amy Cooze 73 3","tags":"Python","url":"http://chrisalbon.com/python/pandas_rename_column_headers.html"},{"title":"Two Way Frequency Table","loc":"http://chrisalbon.com/r-stats/2-way-frequency-table.html","text":"Original source # Create some data A <- c ( \"yes\" , \"no\" , \"yes\" , \"no\" , \"yes\" , \"no\" , \"yes\" , \"no\" ) B <- c ( \"male\" , \"female\" , \"female\" , \"male\" , \"male\" , \"male\" , \"male\" , \"male\" ) # A will be rows, B will be columns mytable <- table ( A , B ) # print table mytable B A female male no 1 3 yes 1 3 # A frequencies (summed over B) margin.table ( mytable , 1 ) A no yes 4 4 # B frequencies (summed over A) margin.table ( mytable , 2 ) B female male 2 6 # cell percentages prop.table ( mytable ) B A female male no 0.125 0.375 yes 0.125 0.375 # row percentages prop.table ( mytable , 1 ) B A female male no 0.25 0.75 yes 0.25 0.75 # column percentages prop.table ( mytable , 2 ) B A female male no 0.5 0.5 yes 0.5 0.5","tags":"R Stats","url":"http://chrisalbon.com/r-stats/2-way-frequency-table.html"},{"title":"2D Density Plot","loc":"http://chrisalbon.com/r-stats/2d-density-plot.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () # create the ggplot2 data p <- ggplot ( faithful , aes ( x = eruptions , y = waiting )) + # add a layer with the points geom_point () + # and a layer for the density heatmap with the alpha and the color determined by density (the .. refers to the fact that density is a variable that was created inside the ggplot() function) stat_density2d ( aes ( alpha = .. density.. , fill = .. density.. ), geom = \"tile\" , contour = FALSE ) null device 1 p","tags":"R Stats","url":"http://chrisalbon.com/r-stats/2d-density-plot.html"},{"title":"Adding Time To A Date","loc":"http://chrisalbon.com/r-stats/add-dates-to-a-date.html","text":"# load the lubridate package library ( lubridate ) # create a date variable date.ex <- dmy ( \"1/1/2001\" ) date.ex [1] \"2001-01-01 UTC\" # add 45 days to a date date.ex.2 <- date.ex + days ( 45 ) date.ex.2 [1] \"2001-02-15 UTC\" # add six weeks to a date date.ex.3 <- date.ex + weeks ( 6 ) date.ex.3 [1] \"2001-02-12 UTC\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/add-dates-to-a-date.html"},{"title":"Adding labels to a ggplot2 bar graph","loc":"http://chrisalbon.com/r-stats/add-labels-to-bar-graph.html","text":"original source: r graphics cookbook # load the ggplot2 package library ( ggplot2 ) # load the gcookbook package library ( gcookbook ) Below the top # create a ggplot data ggplot ( cabbage_exp , aes ( x = interaction ( Date , Cultivar ), y = Weight )) + # draw the bar plot geom_bar ( stat = \"identity\" ) + # create the weight text above the bar in white geom_text ( aes ( label = Weight ), vjust = 1.5 , colour = \"white\" ) Above the top # create a ggplot data ggplot ( cabbage_exp , aes ( x = interaction ( Date , Cultivar ), y = Weight )) + # draw the bar plot geom_bar ( stat = \"identity\" ) + # create the weight text below the bar in white geom_text ( aes ( label = Weight ), vjust = -0.2 ) Labels on a grouped bar chart # create the ggplot data for a grouped bar chart ggplot ( cabbage_exp , aes ( x = Date , y = Weight , fill = Cultivar )) + # plot the bars geom_bar ( stat = \"identity\" , position = \"dodge\" ) + # create the label, \"dodged\" to fit the bars geom_text ( aes ( label = Weight ), vjust = 1.5 , colour = \"white\" , position = position_dodge ( .9 ), size = 3 ) ymax not defined: adjusting position using y instead","tags":"R Stats","url":"http://chrisalbon.com/r-stats/add-labels-to-bar-graph.html"},{"title":"Adding Levels To A Factor","loc":"http://chrisalbon.com/r-stats/add-levels-to-factors.html","text":"# create simulated distract name data district <- c ( \"NORTH\" , \"NORTHWEST\" , \"CENTRAL\" , \"SOUTH\" , \"SOUTHWEST\" , \"EAST\" ) # remake district categories with the combination of district categories and a new SOUTH CENTRAL category levels ( district ) <- c ( district , \"SOUTH CENTRAL\" ) levels ( district ) [1] \"NORTH\" \"NORTHWEST\" \"CENTRAL\" \"SOUTH\" [5] \"SOUTHWEST\" \"EAST\" \"SOUTH CENTRAL\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/add-levels-to-factors.html"},{"title":"Aggregate Data By Week Or Month","loc":"http://chrisalbon.com/r-stats/aggregate-by-week-or-month.html","text":"original source: http://stackoverflow.com/questions/19716244/aggregate-data-by-week-month-etc-in-r?lq=1 # load the xts package library ( xts ) Loading required package: zoo Attaching package: ‘zoo' The following objects are masked from ‘package:base': as.Date, as.Date.numeric Create some simulated data # create an element for every year between two dates date <- seq ( as.Date ( \"2006-01-01\" ), as.Date ( \"2007-01-01\" ), by = \"1 day\" ) # create some simulated values score <- runif ( 366 ) # create a zoo time series object of score and ata zoo <- zoo ( score , date ) Create some averages # create a weekly average weekly.avg <- apply.weekly ( zoo , mean ) weekly.avg 2006-01-01 2006-01-08 2006-01-15 2006-01-22 2006-01-29 2006-02-05 2006-02-12 0.6463105 0.3696941 0.4492466 0.5587588 0.3330893 0.7490642 0.3463500 2006-02-19 2006-02-26 2006-03-05 2006-03-12 2006-03-19 2006-03-26 2006-04-02 0.4594144 0.3015816 0.5016827 0.3824588 0.4501046 0.5086366 0.6927037 2006-04-09 2006-04-16 2006-04-23 2006-04-30 2006-05-07 2006-05-14 2006-05-21 0.5238080 0.6618441 0.4366701 0.6187016 0.6110044 0.5724795 0.5267836 2006-05-28 2006-06-04 2006-06-11 2006-06-18 2006-06-25 2006-07-02 2006-07-09 0.4003268 0.3999404 0.6366840 0.4546525 0.5675619 0.4411083 0.5747285 2006-07-16 2006-07-23 2006-07-30 2006-08-06 2006-08-13 2006-08-20 2006-08-27 0.4136250 0.4936679 0.4814989 0.4419165 0.3644543 0.6385395 0.5230308 2006-09-03 2006-09-10 2006-09-17 2006-09-24 2006-10-01 2006-10-08 2006-10-15 0.5259253 0.5474812 0.4658602 0.4771834 0.6106620 0.4471343 0.4576065 2006-10-22 2006-10-29 2006-11-05 2006-11-12 2006-11-19 2006-11-26 2006-12-03 0.6124155 0.5418694 0.3136825 0.4227544 0.2406943 0.3723846 0.6079556 2006-12-10 2006-12-17 2006-12-24 2006-12-31 2007-01-01 0.5289365 0.4426345 0.6362102 0.4849858 0.1102631 # create a monthly average monthly.avg <- apply.monthly ( zoo , mean ) monthly.avg 2006-01-31 2006-02-28 2006-03-31 2006-04-30 2006-05-31 2006-06-30 2006-07-31 0.4636550 0.4345047 0.4883293 0.5791776 0.5045688 0.5301847 0.4698487 2006-08-31 2006-09-30 2006-10-31 2006-11-30 2006-12-31 2007-01-01 0.5033712 0.5218908 0.5171836 0.3661705 0.5341010 0.1102631","tags":"R Stats","url":"http://chrisalbon.com/r-stats/aggregate-by-week-or-month.html"},{"title":"The Probability An Economy Seat Is An Aisle?","loc":"http://chrisalbon.com/articles/aisle_seat_probabilities.html","text":"There are two types of people in the world, aisle seaters and window seaters. I am an aisle seater, nothing is worse than limited bathroom access on a long flight. The first thing I do when I get my ticket is check to see if I have a window seat. If not, I immediately head over to the airline counter and try to get one. Last flight, on Turkish Airlines, I ran into a curious situation. I recieved my boarding pass with my seat number, 18C, but the ticket did not specify if C was an aisle seat or not. Making matters worse, the airline counter was swamped with a few dozen people. So I asked myself: given only the seat letter, C, what is the probability that it is an aisle seat? Later, on the flight, I decided to find out. Preliminaries # Import required modules import pandas as pd import numpy as np # Set plots to display in the iPython notebook % matplotlib inline Setup possible seat configurations I am a pretty frequently flyer on a variety of airlines and aircraft. There are a variety of seating configurations out there, but typically they follow some basic rules: No window cluster of seats has more than three seats. On small slights with three seats, the single seat is on the left side. No flight has more than nine rows. Based on these rules, here are the \"typical\" seating configurations from aircraft with between two and nine seats per row. A '1' codifies that a seat is an aisle seat, a '0' codifies that it is a non-aisle seat (i.e. window or middle), and 'np.nan' denotes that the aircraft has less than nine seats (this is so all the list lengths are the same). # An aircraft with two seats per row rows2 = [ 1 , 1 , np . nan , np . nan , np . nan , np . nan , np . nan , np . nan , np . nan ] # An aircraft with three seats per row rows3 = [ 1 , 1 , 0 , np . nan , np . nan , np . nan , np . nan , np . nan , np . nan ,] # An aircraft with four seats per row rows4 = [ 0 , 1 , 1 , 0 , np . nan , np . nan , np . nan , np . nan , np . nan ] # An aircraft with five seats per row rows5 = [ 0 , 1 , 1 , 0 , 0 , np . nan , np . nan , np . nan , np . nan ] # An aircraft with six seats per row rows6 = [ 0 , 1 , 1 , 0 , 1 , 0 , np . nan , np . nan , np . nan ] # An aircraft with seven seats per row rows7 = [ 0 , 1 , 1 , 0 , 1 , 1 , 0 , np . nan , np . nan ] # An aircraft with eight seats per row rows8 = [ 0 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , np . nan ] # An aircraft with nine seats per row rows9 = [ 0 , 0 , 1 , 1 , 0 , 1 , 1 , 0 , 0 ] For example, in an aircraft with five seats per row, rows5 , the seating arrangement would be: window aisle aisle middle window no seat no seat no seat no seat Next, I'm take advantage of pandas row summation options, but to do this I need to wrangle the data into a pandas dataframe. Essentially I am using the pandas dataframe as a matrix. # Create a list variable of all possible aircraft configurations seating_map = [ rows2 , rows3 , rows4 , rows5 , rows6 , rows7 , rows8 , rows9 ] # Create a dataframe from the seating_map variable df = pd . DataFrame ( seating_map , columns = [ 'A' , 'B' , 'C' , 'D' , 'E' , 'F' , 'G' , 'H' , 'I' ], index = [ 'rows2' , 'rows3' , 'rows4' , 'rows5' , 'rows6' , 'rows7' , 'rows8' , 'rows9' ]) Here is all the data we need to construct our probabilities. The columns represent individual seat letters (A, B, etc.) while the rows represent the number of seats-per-row in the aircraft. # View the dataframe df A B C D E F G H I rows2 1 1 NaN NaN NaN NaN NaN NaN NaN rows3 1 1 0 NaN NaN NaN NaN NaN NaN rows4 0 1 1 0 NaN NaN NaN NaN NaN rows5 0 1 1 0 0 NaN NaN NaN NaN rows6 0 1 1 0 1 0 NaN NaN NaN rows7 0 1 1 0 1 1 0 NaN NaN rows8 0 0 1 1 1 1 0 0 NaN rows9 0 0 1 1 0 1 1 0 0 Calculate aisle probability Because each aircraft seats-per-row configuration (i.e. row) is binary (1 if aisle, 0 if non-aisle), the probability that a seat is an aisle is simply the mean value of each seat letter (i.e. column). # Create a list wherein each element is the mean value of a column aisle_probability = [ df [ 'A' ] . mean (), df [ 'B' ] . mean (), df [ 'C' ] . mean (), df [ 'D' ] . mean (), df [ 'E' ] . mean (), df [ 'F' ] . mean (), df [ 'G' ] . mean (), df [ 'H' ] . mean (), df [ 'I' ] . mean ()] # Display the variable aisle_probability [0.25, 0.75, 0.8571428571428571, 0.33333333333333331, 0.59999999999999998, 0.75, 0.33333333333333331, 0.0, 0.0] So there you have it, the probability that each seat letter is an aisle. However, we can make the presentation a little more intituative. Visualize seat letter probabilities The most obvious visualization to convey the probabilities would be seat letters on the x-axis and probabilities on the y-axis. Panda's plot function makes that easy. # Create a list of strings to use as the x-axis labels seats = [ 'Seat A' , 'Seat B' , 'Seat C' , 'Seat D' , 'Seat E' , 'Seat F' , 'Seat G' , 'Seat H' , 'Seat I' ] # Plot the probabilities, using 'seats' as the index as a bar chart pd . Series ( aisle_probability , index = seats ) . plot ( kind = 'bar' , # set y to range between 0 and 1 ylim = [ 0 , 1 ], # set the figure size figsize = [ 10 , 6 ], # set the figure title title = 'Probabilty of being an Aisle Seat in Economy Class' ) <matplotlib.axes._subplots.AxesSubplot at 0x1078300f0> So there we have it! If given a boarding pass with seat C you have a 86% probability of being in an aisle seat! I hope this was helpful!","tags":"Articles","url":"http://chrisalbon.com/articles/aisle_seat_probabilities.html"},{"title":"All Combinations For A List Of Objects","loc":"http://chrisalbon.com/python/all_combinations_of_a_list_of_objects.html","text":"Preliminary # Import combinations with replacements from itertools from itertools import combinations_with_replacement Create a list of objects # Create a list of objects to combine list_of_objects = [ 'warplanes' , 'armor' , 'infantry' ] Find all combinations (with replacement) for the list # Create an empty list object to hold the results of the loop combinations = [] # Create a loop for every item in the length of list_of_objects, that, for i in list ( range ( len ( list_of_objects ))): # Finds every combination (with replacement) for each object in the list combinations . append ( list ( combinations_with_replacement ( list_of_objects , i + 1 ))) # View the results combinations [[('warplanes',), ('armor',), ('infantry',)], [('warplanes', 'warplanes'), ('warplanes', 'armor'), ('warplanes', 'infantry'), ('armor', 'armor'), ('armor', 'infantry'), ('infantry', 'infantry')], [('warplanes', 'warplanes', 'warplanes'), ('warplanes', 'warplanes', 'armor'), ('warplanes', 'warplanes', 'infantry'), ('warplanes', 'armor', 'armor'), ('warplanes', 'armor', 'infantry'), ('warplanes', 'infantry', 'infantry'), ('armor', 'armor', 'armor'), ('armor', 'armor', 'infantry'), ('armor', 'infantry', 'infantry'), ('infantry', 'infantry', 'infantry')]] # Flatten the list of lists into just a list combinations = [ i for row in combinations for i in row ] # View the results combinations [('warplanes',), ('armor',), ('infantry',), ('warplanes', 'warplanes'), ('warplanes', 'armor'), ('warplanes', 'infantry'), ('armor', 'armor'), ('armor', 'infantry'), ('infantry', 'infantry'), ('warplanes', 'warplanes', 'warplanes'), ('warplanes', 'warplanes', 'armor'), ('warplanes', 'warplanes', 'infantry'), ('warplanes', 'armor', 'armor'), ('warplanes', 'armor', 'infantry'), ('warplanes', 'infantry', 'infantry'), ('armor', 'armor', 'armor'), ('armor', 'armor', 'infantry'), ('armor', 'infantry', 'infantry'), ('infantry', 'infantry', 'infantry')]","tags":"Python","url":"http://chrisalbon.com/python/all_combinations_of_a_list_of_objects.html"},{"title":"Annotating Plots","loc":"http://chrisalbon.com/r-stats/annotating-plots.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () quartz_off_screen 3 # create the ggplot2 data p <- ggplot ( faithful , aes ( x = eruptions , y = waiting )) Add Text # create the ggplot2 plot p + geom_point () + # add text annotate ( \"text\" , x = 3 , y = 48 , label = \"Group 1\" , family = \"serif\" , fontface = \"italic\" , colour = \"darkred\" , size = 6 ) + # add more text annotate ( \"text\" , x = 4.5 , y = 66 , label = \"Group 2\" , family = \"serif\" , fontface = \"italic\" , colour = \"darkred\" , size = 6 ) # Add Mathematical Expressions # create the ggplot2 plot p + geom_point () + # add the formula, parse=TRUE turns the next into a formula annotate ( \"text\" , x = 4.5 , y = 66 , parse = TRUE , label = \"frac(1, sqrt(2 * pi)) * e &#94; {-x&#94;2 / 2}\" ) Add Mathematical Expressions # create the ggplot2 plot p + geom_point () + # add the formula, parse=TRUE turns the next into a formula annotate ( \"text\" , x = 4.5 , y = 66 , parse = TRUE , label = \"frac(1, sqrt(2 * pi)) * e &#94; {-x&#94;2 / 2}\" ) Add Lines # load the grid package to create the flat ends of the line seqment and arrow library ( grid ) # create the ggplot2 plot p + geom_point () + # add a horizontal line at y = 66 geom_hline ( yintercept = 66 ) + # add a vertical line at 3 = 3 geom_vline ( xintercept = 3 ) + # add an angled line geom_abline ( intercept = 37.4 , slope = 9 ) + # add a line segment annotate ( \"segment\" , x = 1 , xend = 2.5 , y = 75 , yend = 75 , arrow = arrow ( ends = \"both\" , angle = 90 , length = unit ( .2 , \"cm\" ))) + # add an arrow annotate ( \"segment\" , x = 4 , xend = 5 , y = 60 , yend = 55 , colour = \"blue\" , size = 2 , arrow = arrow ()) Add A Shaded Rectangle # create the ggplot2 plot p + geom_point () + # add a shaped blue rectangle annotate ( \"rect\" , xmin = 1 , xmax = 3 , ymin = 40 , ymax = 100 , alpha = .1 , fill = \"blue\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/annotating-plots.html"},{"title":"Apply A Function On Every Row Of A Dataframe","loc":"http://chrisalbon.com/r-stats/apply-function-to-every-row.html","text":"original source: http://stackoverflow.com/questions/2074606/doing-a-plyr-operation-on-every-row-of-a-data-frame-in-r # Load packages library ( plyr ) # create a simulated dataframe x <- rnorm ( 10 ) y <- rnorm ( 10 ) df <- data.frame ( x , y ) # array to dataframe apply, on df, for each row, apply transform() to create a variable called \"max\" whose values are the maximum value of x or y (whichever is higher). adply ( df , 1 , transform , max = max ( x , y )) x y max 1 -1.0286311 0.6621974 0.6621974 2 0.5466022 0.2977963 0.5466022 3 -0.6559125 -2.0830247 -0.6559125 4 -1.6942847 -0.2205220 -0.2205220 5 2.2678281 -0.4791234 2.2678281 6 -1.6849528 -0.4873940 -0.4873940 7 1.1627351 0.5137251 1.1627351 8 1.4182618 0.9697840 1.4182618 9 0.2025052 -0.3519337 0.2025052 10 -0.7100003 -0.6827529 -0.6827529","tags":"R Stats","url":"http://chrisalbon.com/r-stats/apply-function-to-every-row.html"},{"title":"Using apply, sapply, and lapply","loc":"http://chrisalbon.com/r-stats/apply-lapply-sapply.html","text":"Original source: http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/ # Create some data data <- matrix ( data = cbind ( rnorm ( 100 , 0 ), rnorm ( 100 , 25 ), rnorm ( 100 , 50 )), nrow = 100 , ncol = 3 ) Apply The apply function performs some action on a block of values. Note, all the values must be of the same data type (e.g. numeric, string, etc.). If they contain both numeric and string, the numeric values will be converted into strings before the function is applied. # Find mean value of each row (denoted by \"1\") apply ( data , 1 , mean ) [1] 24.59901 26.20459 25.24876 24.70247 24.16932 24.81458 24.52746 25.41626 [9] 25.46790 25.10375 24.70040 24.45695 25.34161 25.13964 24.75298 25.19169 [17] 24.23966 25.04549 25.79292 23.79149 25.14577 24.69546 25.24540 24.63793 [25] 25.25698 25.16858 24.98593 25.76139 24.90628 24.99306 24.62098 25.04368 [33] 24.46879 25.25711 25.18968 24.73647 25.36020 24.75081 24.71036 25.78176 [41] 23.90849 24.28943 24.96408 23.59350 25.23699 24.64131 25.20141 24.77597 [49] 25.35063 24.94441 25.32127 26.06588 24.89428 25.04112 24.10786 25.55573 [57] 25.01078 24.43195 25.15557 24.85495 25.07362 24.83866 25.13600 25.20097 [65] 24.86642 23.31778 25.00892 25.03551 25.21849 25.14255 26.10872 25.98042 [73] 25.56874 25.23941 24.29674 24.91502 24.49740 24.61255 24.14655 24.59718 [81] 24.90140 25.91023 24.92573 24.23798 24.52005 25.66280 24.22381 25.19263 [89] 24.54775 24.96474 24.50890 25.30642 25.47648 24.69407 25.57084 24.71331 [97] 25.22384 24.40210 25.25857 24.73543 # Find mean value of each column (denoted by \"2\") apply ( data , 2 , mean ) [1] 0.09619052 24.89715930 49.84308683 # Find mean of reach additional value. That is, apply the mean() functions to each individual observation instead of entire rows or entire columns (denoted by \"1:2\") apply ( data , 1 : 2 , mean ) [,1] [,2] [,3] [1,] -1.568708977 25.19731 50.16843 [2,] 1.150529977 25.82535 51.63790 [3,] 0.977275865 24.50395 50.26505 [4,] -0.263941077 24.75872 49.61265 [5,] -0.757359741 24.43448 48.83085 [6,] 0.632602125 24.57053 49.24061 [7,] -0.244478069 24.63318 49.19367 [8,] -0.614889100 25.05391 51.80975 [9,] -0.592501893 26.41062 50.58558 [10,] 0.052256014 24.92898 50.33003 [11,] -0.507014529 24.75101 49.85721 [12,] 0.473880143 24.26357 48.63340 [13,] 2.615240491 25.42869 47.98090 [14,] -0.350429812 25.74276 50.02659 [15,] 0.661601288 24.33628 49.26105 [16,] 0.797786704 24.39170 50.38559 [17,] -1.015018178 23.70737 50.02661 [18,] 0.331179662 25.62967 49.17563 [19,] 1.255608573 25.30459 50.81856 [20,] -2.162252797 25.38918 48.14754 [21,] 0.525101996 24.14938 50.76283 [22,] 0.247228755 24.87424 48.96490 [23,] 1.196517482 24.49384 50.04583 [24,] 0.588874298 23.34821 49.97671 [25,] -0.902219593 25.60770 51.06546 [26,] -0.383807172 26.20012 49.68944 [27,] -1.427233324 25.75519 50.62983 [28,] 1.050320147 25.28623 50.94763 [29,] -0.599947956 26.38403 48.93474 [30,] -0.302622507 25.55671 49.72508 [31,] 0.019930217 23.56713 50.27587 [32,] 0.883376538 25.30232 48.94533 [33,] 0.840728908 23.36012 49.20553 [34,] 1.366500062 23.14374 51.26110 [35,] 0.176682690 25.19876 50.19360 [36,] -0.811892248 24.28875 50.73254 [37,] -0.274652977 24.22998 52.12526 [38,] -0.157275521 24.89746 49.51226 [39,] 0.132177317 24.87886 49.12005 [40,] 1.136077454 25.41962 50.78960 [41,] -1.300481517 24.50820 48.51774 [42,] -0.383029453 22.84781 50.40350 [43,] 0.099370222 25.41390 49.37896 [44,] -2.050115390 23.07011 49.76051 [45,] 0.501352187 25.41343 49.79617 [46,] 0.009760826 24.30647 49.60769 [47,] -0.064267636 25.58423 50.08428 [48,] 0.317009746 24.11671 49.89420 [49,] -0.229239652 25.26184 51.01928 [50,] 0.458235055 24.48696 49.88805 [51,] 0.076155053 25.44080 50.44686 [52,] 0.734427587 26.65986 50.80336 [53,] -0.084391861 25.56510 49.20214 [54,] -0.400739985 25.20466 50.31946 [55,] 0.620316321 23.12588 48.57738 [56,] -0.369067753 26.46223 50.57403 [57,] 0.583869219 24.85391 49.59456 [58,] -1.012146164 23.52167 50.78632 [59,] -0.176942984 25.33134 50.31231 [60,] 1.031038799 23.63650 49.89732 [61,] 0.291820274 24.04088 50.88817 [62,] -0.286363289 26.31617 48.48616 [63,] 0.295092879 25.12000 49.99292 [64,] 1.738129994 25.47344 48.39135 [65,] -1.444422908 25.03078 51.01289 [66,] -1.992472026 23.99107 47.95475 [67,] -0.126062864 24.88056 50.27226 [68,] 1.511624181 24.90431 48.69059 [69,] 0.407953425 24.83007 50.41746 [70,] 0.729027047 25.18040 49.51824 [71,] 1.614534378 26.13329 50.57833 [72,] 1.528257134 25.48461 50.92839 [73,] 0.185587393 25.58042 50.94020 [74,] 0.035201742 25.84610 49.83694 [75,] -0.220080957 25.49684 47.61347 [76,] 0.751172599 24.12418 49.86970 [77,] -0.706358939 24.17086 50.02771 [78,] -0.748064620 25.79963 48.78608 [79,] -0.763060888 23.32252 49.88018 [80,] 0.813353831 24.79051 48.18769 [81,] 0.327942825 23.91075 50.46549 [82,] 2.386272878 25.54947 49.79496 [83,] 1.099094285 25.26427 48.41382 [84,] 0.487128984 24.79840 47.42842 [85,] -0.921897261 25.64221 48.83984 [86,] 1.619382465 24.87703 50.49199 [87,] -2.430495109 25.74597 49.35596 [88,] 0.136899216 25.98202 49.45896 [89,] -1.055246874 23.77861 50.91988 [90,] -0.691470394 24.56741 51.01827 [91,] 0.019588244 24.59630 48.91080 [92,] -0.070343647 25.54222 50.44737 [93,] 0.432749908 24.74199 51.25470 [94,] 0.678025865 23.27195 50.13224 [95,] 0.405506770 27.02547 49.28155 [96,] 1.652979613 23.30071 49.18624 [97,] 1.006041068 24.88392 49.78156 [98,] -0.425817664 25.72349 47.90862 [99,] -0.437871402 25.73701 50.47658 [100,] -0.750629736 24.24426 50.71267 Sapply # Sapply applies a function over each individual element in an object, returning a vector sapply ( data , mean ) [1] -1.568708977 1.150529977 0.977275865 -0.263941077 -0.757359741 [6] 0.632602125 -0.244478069 -0.614889100 -0.592501893 0.052256014 [11] -0.507014529 0.473880143 2.615240491 -0.350429812 0.661601288 [16] 0.797786704 -1.015018178 0.331179662 1.255608573 -2.162252797 [21] 0.525101996 0.247228755 1.196517482 0.588874298 -0.902219593 [26] -0.383807172 -1.427233324 1.050320147 -0.599947956 -0.302622507 [31] 0.019930217 0.883376538 0.840728908 1.366500062 0.176682690 [36] -0.811892248 -0.274652977 -0.157275521 0.132177317 1.136077454 [41] -1.300481517 -0.383029453 0.099370222 -2.050115390 0.501352187 [46] 0.009760826 -0.064267636 0.317009746 -0.229239652 0.458235055 [51] 0.076155053 0.734427587 -0.084391861 -0.400739985 0.620316321 [56] -0.369067753 0.583869219 -1.012146164 -0.176942984 1.031038799 [61] 0.291820274 -0.286363289 0.295092879 1.738129994 -1.444422908 [66] -1.992472026 -0.126062864 1.511624181 0.407953425 0.729027047 [71] 1.614534378 1.528257134 0.185587393 0.035201742 -0.220080957 [76] 0.751172599 -0.706358939 -0.748064620 -0.763060888 0.813353831 [81] 0.327942825 2.386272878 1.099094285 0.487128984 -0.921897261 [86] 1.619382465 -2.430495109 0.136899216 -1.055246874 -0.691470394 [91] 0.019588244 -0.070343647 0.432749908 0.678025865 0.405506770 [96] 1.652979613 1.006041068 -0.425817664 -0.437871402 -0.750629736 [101] 25.197306545 25.825350781 24.503953832 24.758718674 24.434477602 [106] 24.570526558 24.633180740 25.053909206 26.410619962 24.928981884 [111] 24.751005799 24.263566175 25.428692964 25.742763722 24.336277408 [116] 24.391703303 23.707374457 25.629673993 25.304587483 25.389176630 [121] 24.149383170 24.874239009 24.493836724 23.348211090 25.607700571 [126] 26.200116013 25.755185549 25.286226101 26.384032256 25.556712376 [131] 23.567129337 25.302324415 23.360122623 23.143743252 25.198758203 [136] 24.288745427 24.229984320 24.897463124 24.878858415 25.419617225 [141] 24.508196500 22.847811945 25.413903990 23.070107275 25.413433368 [146] 24.306468586 25.584229464 24.116708141 25.261839601 24.486956445 [151] 25.440800049 26.659857557 25.565101945 25.204656283 23.125881246 [156] 26.462228263 24.853909780 23.521668193 25.331337679 23.636499726 [161] 24.040880917 26.316172578 25.119996193 25.473437131 25.030778969 [166] 23.991070971 24.880564659 24.904307929 24.830074007 25.180398308 [171] 26.133289979 25.484612200 25.580424723 25.846103841 25.496839595 [176] 24.124179656 24.170857114 25.799628759 23.322523521 24.790511111 [181] 23.910751115 25.549466633 25.264265733 24.798400630 25.642209925 [186] 24.877027823 25.745973119 25.982021165 23.778612923 24.567407556 [191] 24.596298326 25.542218418 24.741988971 23.271952314 27.025469544 [196] 23.300710553 24.883916973 25.723485456 25.737009580 24.244258255 [201] 50.168431849 51.637900971 50.265045566 49.612645844 48.830848358 [206] 49.240613619 49.193669941 51.809750441 50.585582341 50.330025798 [211] 49.857207672 48.633404522 47.980896119 50.026593960 49.261052113 [216] 50.385585194 50.026611946 49.175626026 50.818555563 48.147539082 [221] 50.762826545 48.964901402 50.045832988 49.976713639 51.065456311 [226] 49.689440608 50.629825924 50.947626860 48.934744168 49.725079985 [231] 50.275872304 48.945331047 49.205531669 51.261095381 50.193603345 [236] 50.732543196 52.125259556 49.512255163 49.120047270 50.789598761 [241] 48.517741319 50.403500715 49.378962367 49.760507109 49.796171242 [246] 49.607690616 50.084275217 49.894199065 51.019282225 49.888051966 [251] 50.446861713 50.803355703 49.202143279 50.319458013 48.577378982 [256] 50.574032023 49.594563809 50.786319423 50.312312319 49.897318172 [261] 50.888168155 48.486159534 49.992922240 48.391347666 51.012893420 [266] 47.954752646 50.272263973 48.690586502 50.417455451 49.518237586 [271] 50.578329878 50.928392627 50.940201968 49.836937110 47.613474293 [276] 49.869701038 50.027708980 48.786075248 49.880176985 48.187685772 [281] 50.465491452 49.794963712 48.413817125 47.428420395 48.839838668 [286] 50.491987788 49.355956225 49.458958967 50.919880652 51.018268137 [291] 48.910802035 50.447373239 51.254699133 50.132239055 49.281546656 [296] 49.186239840 49.781557088 47.908620364 50.476582295 50.712671097 Lapply Lapply applies a function of each individual element in an object, returning a list # Create a list with two elements l <- list ( a = 1 : 10 , b = 11 : 20 ) # Apply the sum function to each list lapply ( l , sum ) $a [1] 55 $b [1] 155","tags":"R Stats","url":"http://chrisalbon.com/r-stats/apply-lapply-sapply.html"},{"title":"Advanced Applying With Plyr","loc":"http://chrisalbon.com/r-stats/apply-with-plyr.html","text":"The plyr package uses **ply() functions, where the first star in the input and the second star is the output. For example, llplyr takes a list in and spits a list out. # load the plyr package library ( plyr ) #generate some fake list data war.name <- c ( \"WWII\" , \"WWII\" , \"WWI\" , \"WWI\" , \"Franco-Prussian\" , \"Franco-Prussian\" , \"Franco-Prussian\" , \"Boer War\" , \"Boer War\" , \"Boer War\" ) deaths <- c ( 938 , 9480 , 2049 , 1039 , 3928 , 9202 , 10933 , 40293 , 10394 , 20394 ) allies <- c ( 9 , 5 , 4 , 6 , 3 , 2 , 4 , 1 , 2 , 3 ) casualties <- list ( war.name , deaths , allies ) casualties.df <- data.frame ( war.name , deaths , allies ) # split up the list by casualties, find all the unique elements, output them as a list llply ( casualties , unique ) [[1]] [1] \"WWII\" \"WWI\" \"Franco-Prussian\" \"Boer War\" [[2]] [1] 938 9480 2049 1039 3928 9202 10933 40293 10394 20394 [[3]] [1] 9 5 4 6 3 2 1 r*ply replaces replicate, with the * denoting the output # run runif(1) five times, outputting a data frame rdply ( 5 , runif ( 1 )) .n V1 1 1 0.09292281 2 2 0.06861817 3 3 0.04870200 4 4 0.57864348 5 5 0.21716079 ddply replaces tapply, it inputs a data frame and outputs a data frame # take the data frame casualties.df, split it up by war.name (for some reasons it uses the .() function, the find the mean) ddply ( casualties.df , . ( war.name ), colwise ( mean ) ) war.name deaths allies 1 Boer War 23693.67 2 2 Franco-Prussian 8021.00 3 3 WWI 1544.00 5 4 WWII 5209.00 7","tags":"R Stats","url":"http://chrisalbon.com/r-stats/apply-with-plyr.html"},{"title":"Apply Operations Over Items In A List","loc":"http://chrisalbon.com/python/apply_operations_over_items_in_lists.html","text":"Method 1: map() # Create a list of casualties from battles battleDeaths = [ 482 , 93 , 392 , 920 , 813 , 199 , 374 , 237 , 244 ] # Create a function that updates all battle deaths by adding 100 def updated ( x ): return x + 100 # Create a list that applies updated() to all elements of battleDeaths list ( map ( updated , battleDeaths )) [582, 193, 492, 1020, 913, 299, 474, 337, 344] Method 2: for x in y # Create a list of deaths casualties = [ 482 , 93 , 392 , 920 , 813 , 199 , 374 , 237 , 244 ] # Create a variable where we will put the updated casualty numbers casualtiesUpdated = [] # Create a function that for each item in casualties, adds 10 for x in casualties : casualtiesUpdated . append ( x + 100 ) # View casualties variables casualtiesUpdated [582, 193, 492, 1020, 913, 299, 474, 337, 344] Method 3: lambda functions # Map the lambda function x() over casualties list ( map (( lambda x : x + 100 ), casualties )) [582, 193, 492, 1020, 913, 299, 474, 337, 344]","tags":"Python","url":"http://chrisalbon.com/python/apply_operations_over_items_in_lists.html"},{"title":"Applying Functions To List Items","loc":"http://chrisalbon.com/python/applying_functions_to_list_items.html","text":"Create a list of regiment names regimentNames = [ 'Night Riflemen' , 'Jungle Scouts' , 'The Dragoons' , 'Midnight Revengence' , 'Wily Warriors' ] Using A For Loop Create a for loop goes through the list and capitalizes each # create a variable for the for loop results regimentNamesCapitalized_f = [] # for every item in regimentNames for i in regimentNames : # capitalize the item and add it to regimentNamesCapitalized_f regimentNamesCapitalized_f . append ( i . upper ()) # View the outcome regimentNamesCapitalized_f ['NIGHT RIFLEMEN', 'JUNGLE SCOUTS', 'THE DRAGOONS', 'MIDNIGHT REVENGENCE', 'WILY WARRIORS'] Using Map() Create a lambda function that capitalizes x capitalizer = lambda x : x . upper () Map the capitalizer function to regimentNames, convert the map into a list, and view the variable regimentNamesCapitalized_m = list ( map ( capitalizer , regimentNames )); regimentNamesCapitalized_m ['NIGHT RIFLEMEN', 'JUNGLE SCOUTS', 'THE DRAGOONS', 'MIDNIGHT REVENGENCE', 'WILY WARRIORS'] Using List Comprehension Apply the expression x.upper to each item in the list called regiment names. Then view the output regimentNamesCapitalized_l = [ x . upper () for x in regimentNames ]; regimentNamesCapitalized_l ['NIGHT RIFLEMEN', 'JUNGLE SCOUTS', 'THE DRAGOONS', 'MIDNIGHT REVENGENCE', 'WILY WARRIORS']","tags":"Python","url":"http://chrisalbon.com/python/applying_functions_to_list_items.html"},{"title":"Basic Math Functions In R","loc":"http://chrisalbon.com/r-stats/arithmetic.html","text":"Arithmetic operators # 1 plus 1 1 + 1 [1] 2 # 4 minus 3 4 - 3 [1] 1 # 14 divided by 10 14 / 10 [1] 1.4 # 10 multiplied by 5 10 * 5 [1] 50 # 3 squared 3 &#94; 2 [1] 9 # 5 mod 2 5 %% 2 [1] 1 # 4 divided by 2 (integer division) 4 %/% 2 [1] 2 # log to the base e of 2 log ( 2 ) [1] 0.6931472 # antilog of 2 exp ( 2 ) [1] 7.389056 # log to base 2 of 3 log ( 3 , 2 ) [1] 1.584963 # log to base 10 of 2 log10 ( 2 ) [1] 0.30103 # square root of 2 sqrt ( 2 ) [1] 1.414214 # !5 factorial ( 4 ) [1] 24 # largest interger smaller than 2 floor ( 2 ) [1] 2 # smallest integer greater than 6 ceiling ( 6 ) [1] 6 # round 3.14159 to three digits round ( 3.14159 , digits = 3 ) [1] 3.142 # create 10 random digits between zero and 1 (from a uniform distribution) runif ( 10 ) [1] 0.07613962 0.66543266 0.48379168 0.40593920 0.67715428 0.49170373 [7] 0.62351598 0.19275859 0.48018351 0.34890640 # cosine of 3 cos ( 3 ) [1] -0.9899925 # sine of 3 sin ( 3 ) [1] 0.14112 # tangent of 3 tan ( 3 ) [1] -0.1425465 # absolute value of -3 abs ( -3 ) [1] 3","tags":"R Stats","url":"http://chrisalbon.com/r-stats/arithmetic.html"},{"title":"Arithmetic Basics","loc":"http://chrisalbon.com/python/arithmetic_basics.html","text":"Create some simulated variables x = 6 y = 9 x plus y print ( x + y ) 15 x minus y print ( x - y ) -3 x times y print ( x * y ) 54 the remainder of x divided by y print ( x % y ) 6 x divided by y print ( x / y ) 0.6666666666666666 x divided by y (floor) (i.e. the quotient) print ( x // y ) 0 x raised to the y power print ( x ** y ) 10077696 x plus y, then divide by x print (( x + y ) / x ) 2.5 Classics vs. floor division. This varies between 2.x and 3.x. Classic divison of 3 by 5 print ( 3 / 5 ) 0.6 Floor divison of 3 by 5. This means it truncates any remainder down the its \"floor\" print ( 3 // 5 ) 0","tags":"Python","url":"http://chrisalbon.com/python/arithmetic_basics.html"},{"title":"Assignment Operators","loc":"http://chrisalbon.com/python/assignment_operators.html","text":"Create some variables a = 2 b = 1 c = 0 d = 3 Assigns values from right side to left side c = a + b c 3 Add right to the left and assign the result to left (c = a + c) c += a c 5 Subtract right from the left and assign the result to left (c = a - c) c -= a c 3 Multiply right with the left and assign the result to left (c = a * c) c *= a c 6 Divide left with the right and assign the result to left (c = c / a) c /= a c 3.0 Takes modulus using two operands and assign the result to left (a = d % a) d %= a d 1 Exponential (power) calculation on operators and assign value to the left (d = d &#94; a) d **= a d 1 Floor division on operators and assign value to the left (d = d // a) d //= a d 0","tags":"Python","url":"http://chrisalbon.com/python/assignment_operators.html"},{"title":"Making A Back To Back Histogram","loc":"http://chrisalbon.com/r-stats/back-to-back-histogram.html","text":"Original source: http://onertipaday.blogspot.com/2007/06/back-to-back-historgram.html # Load the Hmisc package (install it if necessary) library ( Hmisc ) # Create 1000 observations, with mean 300 data <- rnorm ( 1000 , 300 , 10 ) # Create 1000 sex observations of male and female sex <- sample ( c ( 'female' , 'male' ), 1000 , TRUE ) # Create a back to back histogram of the data divided up by sex out <- histbackback ( split ( data , sex ), probability = TRUE , xlim = c ( -.06 , .06 ), main = 'Back to Back Histogram' ) # Add color to the left histogram barplot ( - out $ left , col = \"red\" , horiz = TRUE , space = 0 , add = TRUE , axes = FALSE ) # Add color to the right histogram barplot ( out $ right , col = \"blue\" , horiz = TRUE , space = 0 , add = TRUE , axes = FALSE )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/back-to-back-histogram.html"},{"title":"Balloon Plot","loc":"http://chrisalbon.com/r-stats/balloon-plot.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # create a subset of countries for year 2009 and only for a vector of countries names cdat <- subset ( countries , Year == 2009 & Name %in% c ( \"Canada\" , \"Ireland\" , \"United Kingdom\" , \"United States\" , \"New Zealand\" , \"Iceland\" , \"Japan\" , \"Luxembourg\" , \"Netherlands\" , \"Switzerland\" )) # create the ggplot data with the size of the balloons determined by GDP p <- ggplot ( cdat , aes ( x = healthexp , y = infmortality , size = GDP )) + # select the select of the points and their colors geom_point ( shape = 21 , colour = \"#3333ff\" , fill = \"#0066CC\" ) + # change the scale of the point size to make them bigger scale_size_area ( max_size = 15 ); p","tags":"R Stats","url":"http://chrisalbon.com/r-stats/balloon-plot.html"},{"title":"Bar Charts","loc":"http://chrisalbon.com/r-stats/bar-charts.html","text":"original source: ggplot2 book The bar geom counts the number of instances of each class so that you don't need to tabulate your values beforehand # load the ggplot2 library library ( ggplot2 ) # set the seed so we can reproduce the results set.seed ( 1410 ) # plot the color qplot ( color , data = diamonds , geom = \"bar\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/bar-charts.html"},{"title":"Bar Graph","loc":"http://chrisalbon.com/r-stats/bar-graph.html","text":"# load the ggplot2 library library ( ggplot2 ) # plot BOD$Time and BOD$demand qplot ( BOD $ Time , BOD $ demand , geom = \"bar\" , stat = \"identity\" ) # Convert the x variable to a factor, so that it is treated as discrete qplot ( factor ( BOD $ Time ), BOD $ demand , geom = \"bar\" , stat = \"identity\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/bar-graph.html"},{"title":"Bar Graph Of Two Sets Of Data","loc":"http://chrisalbon.com/r-stats/barplot-two-sets-of-data.html","text":"# Create a variable of rain fall in Spain spain <- c ( 0.0001 , 0.0059 , 0.0855 , 0.9082 ) # Create a variable of rain fall in Yemen yemen <- c ( 0.54 , 0.813 , 0.379 , 0.35 ) # create a two row matrix with spain and yemen rainfall <- rbind ( spain , yemen ) # Create a bar graph with paired data (using \"beside = TRUE\") and with the names of some months mp <- barplot ( rainfall , beside = TRUE , ylim = c ( 0 , 1 ), names.arg = c ( \"March\" , \"April\" , \"May\" , \"June\" )) # Draw the bar values above the bars text ( mp , rainfall , labels = format ( rainfall , 4 ), pos = 3 , cex = .75 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/barplot-two-sets-of-data.html"},{"title":"Drilling Down With Beautiful Soup","loc":"http://chrisalbon.com/python/beautiful_soup_drill_down.html","text":"# Import required modules import requests from bs4 import BeautifulSoup import pandas as pd Download the HTML and create a Beautiful Soup object # Create a variable with the URL to this tutorial url = 'http://en.wikipedia.org/wiki/List_of_A_Song_of_Ice_and_Fire_characters' # Scrape the HTML at the url r = requests . get ( url ) # Turn the HTML into a Beautiful Soup object soup = BeautifulSoup ( r . text , \"lxml\" ) If we looked at the soup object, we'd see that the names we want are in a heirarchical list. In psuedo-code, it looks like: class=toclevel-1 span=toctext class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES class=toclevel-2 span=toctext CHARACTER NAMES To get the CHARACTER NAMES, we are going to need to drill down to grap into loclevel-2 and grab the toctext Setting up where to put the results # Create a variable to score the scraped data in character_name = [] Drilling down with a forloop # for each item in all the toclevel-2 li items # (except the last three because they are not character names), for item in soup . find_all ( 'li' ,{ 'class' : 'toclevel-2' })[: - 3 ]: # find each span with class=toctext, for post in item . find_all ( 'span' ,{ 'class' : 'toctext' }): # add the stripped string of each to character_name, one by one character_name . append ( post . string . strip ()) The results # View all the character names character_name ['Eddard Stark', 'Catelyn Tully', 'Robb Stark', 'Sansa Stark', 'Arya Stark', 'Bran Stark', 'Rickon Stark', 'Jon Snow', 'Benjen Stark', 'Lyanna Stark', 'Theon Greyjoy', 'Roose Bolton', 'Ramsay Bolton', 'Hodor', 'Osha', 'Jeyne Poole', 'Jojen and Meera Reed', 'Jeyne Westerling', 'Daenerys Targaryen', 'Viserys Targaryen', 'Rhaegar Targaryen', 'Aegon V Targaryen', 'Aerys II Targaryen', 'Aegon VI Targaryen', 'Jon Connington', 'Jorah Mormont', 'Brynden Rivers', 'Missandei', 'Daario Naharis', 'Grey Worm', 'Jon Arryn', 'Lysa Arryn', 'Robert Arryn', 'Yohn Royce', 'Tywin Lannister', 'Cersei Lannister', 'Jaime Lannister', 'Joffrey Baratheon', 'Myrcella Baratheon', 'Tommen Baratheon', 'Tyrion Lannister', 'Kevan Lannister', 'Lancel Lannister', 'Bronn', 'Gregor Clegane', 'Sandor Clegane', 'Podrick Payne', 'Robert Baratheon', 'Stannis Baratheon', 'Selyse Baratheon', 'Shireen Baratheon', 'Melisandre', 'Davos Seaworth', 'Renly Baratheon', 'Brienne of Tarth', 'Beric Dondarrion', 'Gendry', 'Balon Greyjoy', 'Asha Greyjoy', 'Euron Greyjoy', 'Victarion Greyjoy', 'Aeron Greyjoy', 'Doran Martell', 'Arianne Martell', 'Quentyn Martell', 'Trystane Martell', 'Elia Martell', 'Oberyn Martell', 'Ellaria Sand', 'The Sand Snakes', 'Areo Hotah', 'Hoster Tully', 'Edmure Tully', 'Brynden Tully', 'Walder Frey', 'Mace Tyrell', 'Loras Tyrell', 'Margaery Tyrell', 'Olenna Tyrell', 'Randyll Tarly', 'Jeor Mormont', 'Maester Aemon', 'Yoren', 'Samwell Tarly', 'Janos Slynt', 'Alliser Thorne', 'Mance Rayder', 'Ygritte', 'Craster', 'Gilly', 'Val', 'Lord of Bones', 'Bowen Marsh', 'Eddison Tollett', 'Tormund Giantsbane', 'Petyr Baelish', 'Varys', 'Pycelle', 'Barristan Selmy', 'Arys Oakheart', 'Ilyn Payne', 'Qyburn', 'The High Sparrow', 'Khal Drogo', 'Syrio Forel', \"Jaqen H'ghar\", 'Illyrio Mopatis', 'Thoros of Myr', 'Ser Duncan the Tall', 'Hizdahr zo Loraq', 'Yezzan zo Qaggaz', 'Tycho Nestoris', 'The Waif', 'Septa Unella'] Quick analysis: Which house has the most main characters? # Create a list object where to store the for loop results houses = [] # For each element in the character_name list, for name in character_name : # split up the names by a blank space and select the last element # this works because it is the last name if they are a house, # but the first name if they only have one name, # Then append each last name to the houses list houses . append ( name . split ( ' ' )[ - 1 ]) # Convert houses into a pandas series (so we can use value_counts()) houses = pd . Series ( houses ) # Count the number of times each name/house name appears houses . value_counts () Baratheon 8 Stark 8 Targaryen 6 Greyjoy 6 Lannister 6 Martell 6 Tyrell 4 Tully 4 Arryn 3 Clegane 2 Bolton 2 Mormont 2 Payne 2 Tarly 2 Melisandre 1 Giantsbane 1 Ygritte 1 Bronn 1 Westerling 1 Sand 1 Osha 1 Gendry 1 Sparrow 1 Drogo 1 Qyburn 1 Gilly 1 Pycelle 1 Craster 1 H'ghar 1 Oakheart 1 .. Rivers 1 Seaworth 1 Marsh 1 Connington 1 Hodor 1 Val 1 Unella 1 Aemon 1 Myr 1 Slynt 1 Dondarrion 1 Baelish 1 Qaggaz 1 Yoren 1 Mopatis 1 Worm 1 Varys 1 Royce 1 Nestoris 1 Tarth 1 Naharis 1 Snakes 1 Reed 1 Bones 1 Tollett 1 Rayder 1 Tall 1 Selmy 1 Hotah 1 Snow 1 dtype: int64","tags":"Python","url":"http://chrisalbon.com/python/beautiful_soup_drill_down.html"},{"title":"Beautiful Soup Basic HTML Scraping","loc":"http://chrisalbon.com/python/beautiful_soup_html_basics.html","text":"Import the modules # Import required modules import requests from bs4 import BeautifulSoup Scrap the html and turn into a beautiful soup object # Create a variable with the url url = 'http://chrisralbon.com' # Use requests to get the contents r = requests . get ( url ) # Get the text of the contents html_content = r . text # Convert the html content into a beautiful soup object soup = BeautifulSoup ( html_content , 'lxml' ) Select the website's title # View the title tag of the soup object soup . title <title>Chris Albon</title> Website title tag's string # View the string within the title tag soup . title . string 'Chris Albon' First paragraph tag # view the paragraph tag of the soup soup . p <p>I am a <a href=\"./pages/about.html\">data scientist originally trained as a quantitative political scientist</a>. I specialize in the technical and organizational aspects of applying data science to political and social issues. </p> The parent of the title tag soup . title . parent . name 'head' The first link tag soup . a <a class=\"navbar-brand\" href=\".\">Chris Albon</a> Find all the link tags and list the first five soup . find_all ( 'a' )[ 0 : 5 ] [<a class=\"navbar-brand\" href=\".\">Chris Albon</a>, <a aria-expanded=\"false\" aria-haspopup=\"true\" class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"#\" role=\"button\">About<span class=\"caret\"></span></a>, <a href=\"./pages/about.html\">About Chris</a>, <a href=\"https://github.com/chrisalbon\">GitHub</a>, <a href=\"https://twitter.com/chrisalbon\">Twitter</a>] The string inside the first paragraph tag soup . p . string Find all the h2 tags and list the first five soup . find_all ( 'h2' )[ 0 : 5 ] [<h2 class=\"homepage_category_title\">Articles</h2>, <h2 class=\"homepage_category_title\">Projects</h2>, <h2 class=\"homepage_category_title\">Python</h2>, <h2 class=\"homepage_category_title\">R Stats</h2>, <h2 class=\"homepage_category_title\">Regex</h2>] Find all the links on the page and list the first five soup . find_all ( 'a' )[ 0 : 5 ] [<a class=\"navbar-brand\" href=\".\">Chris Albon</a>, <a aria-expanded=\"false\" aria-haspopup=\"true\" class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"#\" role=\"button\">About<span class=\"caret\"></span></a>, <a href=\"./pages/about.html\">About Chris</a>, <a href=\"https://github.com/chrisalbon\">GitHub</a>, <a href=\"https://twitter.com/chrisalbon\">Twitter</a>]","tags":"Python","url":"http://chrisalbon.com/python/beautiful_soup_html_basics.html"},{"title":"Scraping a HTML with Beauitful Soup","loc":"http://chrisalbon.com/python/beautiful_soup_scrape_table.html","text":"# Import required modules import requests from bs4 import BeautifulSoup import pandas as pd Create a dataframe. We will scrape iPython's HTML table output # Create a values as dictionary of lists raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} # Create a dataframe raw_df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) # View a dataframe raw_df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Download the HTML and create a Beautiful Soup object # Create a variable with the URL to this tutorial url = 'http://nbviewer.ipython.org/github/chrisalbon/code_py/blob/master/beautiful_soup_scrape_table.ipynb' # Scrape the HTML at the url r = requests . get ( url ) # Turn the HTML into a Beautiful Soup object soup = BeautifulSoup ( r . text , 'lxml' ) Parse the Beautiful Soup object # Create four variables to score the scraped data in first_name = [] last_name = [] age = [] preTestScore = [] postTestScore = [] # Create an object of the first object that is class=dataframe table = soup . find ( class_ = 'dataframe' ) # Find all the <tr> tag pairs, skip the first one, then for each. for row in table . find_all ( 'tr' )[ 1 :]: # Create a variable of all the <td> tag pairs in each <tr> tag pair, col = row . find_all ( 'td' ) # Create a variable of the string inside 1st <td> tag pair, column_1 = col [ 0 ] . string . strip () # and append it to first_name variable first_name . append ( column_1 ) # Create a variable of the string inside 2nd <td> tag pair, column_2 = col [ 1 ] . string . strip () # and append it to last_name variable last_name . append ( column_2 ) # Create a variable of the string inside 3rd <td> tag pair, column_3 = col [ 2 ] . string . strip () # and append it to age variable age . append ( column_3 ) # Create a variable of the string inside 4th <td> tag pair, column_4 = col [ 3 ] . string . strip () # and append it to preTestScore variable preTestScore . append ( column_4 ) # Create a variable of the string inside 5th <td> tag pair, column_5 = col [ 4 ] . string . strip () # and append it to postTestScore variable postTestScore . append ( column_5 ) # Create a variable of the value of the columns columns = { 'first_name' : first_name , 'last_name' : last_name , 'age' : age , 'preTestScore' : preTestScore , 'postTestScore' : postTestScore } # Create a dataframe from the columns variable df = pd . DataFrame ( columns ) # View the dataframe df age first_name last_name postTestScore preTestScore 0 42 Jason Miller 25 4 1 52 Molly Jacobson 94 24 2 36 Tina Ali 57 31 3 24 Jake Milner 62 2 4 73 Amy Cooze 70 3","tags":"Python","url":"http://chrisalbon.com/python/beautiful_soup_scrape_table.html"},{"title":"Benchmarking R Speed","loc":"http://chrisalbon.com/r-stats/benchmarking.html","text":"# how long does it take R to create a huge sequence of numbers? system.time ( 1 : 10000000 ) user system elapsed 0.037 0.014 0.053 # how long does it take R to create a equally long set of random draws system.time ( runif ( 10000000 )) user system elapsed 0.392 0.028 0.436","tags":"R Stats","url":"http://chrisalbon.com/r-stats/benchmarking.html"},{"title":"Boxplots","loc":"http://chrisalbon.com/r-stats/boxplots.html","text":"Original Source: http://www.statmethods.net/graphs/boxplot.html # Create some fake data about different types of casualties in a war deaths <- c ( 234 , 234 , 643 , 74 , 323 , 67 , 34 , 78 , 434 ) population <- factor ( c ( \"civilian\" , \"soldier\" , \"rebel\" )) war <- cbind ( deaths , population ) # Create a boxplot of the number of deaths, seperated by populaton type, with teach boxplot labelled with the factor name, and colored different shades of red boxplot ( deaths ~ population , data = war , main = \"Deaths By Type\" , xlab = \"Type of Victims\" , ylab = \"Number Of Deaths\" , names = population , col = c ( \"red3\" , \"indianred4\" , \"orangered1\" ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/boxplots.html"},{"title":"Breaking Up String Variables","loc":"http://chrisalbon.com/python/breaking_up_string_variables.html","text":"Basic name assignment variableName = 'This is a string.' List assignment One , Two , Three = [ 1 , 2 , 3 ] Break up a string into variables firstLetter , secondLetter , thirdLetter , fourthLetter = 'Bark' firstLetter 'B' secondLetter 'a' thirdLetter 'r' fourthLetter 'k' Breaking up a number into seperate variables firstNumber , secondNumber , thirdNumber , fourthNumber = '9485' firstLetter 'B' secondLetter 'a' thirdLetter 'r' fourthLetter 'k' Assign the first letter of 'spam' into varible a, assign all the remaining letters to variable b a , * b = 'spam' a 's' b ['p', 'a', 'm']","tags":"Python","url":"http://chrisalbon.com/python/breaking_up_string_variables.html"},{"title":"Brute Force D20 Roll Simulator","loc":"http://chrisalbon.com/python/brute_force_d20_simulator.html","text":"This snippet is a completely inefficient simulator of a 20 sided dice. To create a \"succussful roll\" the snippet has to generate dozens of random numbers. Import random module import random Create a variable with a TRUE value rolling = True Create a while loop that rolls until the first digit is 2 or less and the second digit is 10 or less # while rolling is true while rolling : # create x, a random number between 0 and 99 x = random . randint ( 0 , 99 ) # create y, a random number between 0 and 99 y = random . randint ( 0 , 99 ) # if x is less than 2 and y is between 0 and 10 if x < 2 and 0 < y < 10 : # Print the outcome print ( 'You rolled a {0}{1}.' . format ( x , y )) # And set roll of False rolling = False # Otherwise else : # Try again print ( 'Trying again.' ) Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. Trying again. You rolled a 13.","tags":"Python","url":"http://chrisalbon.com/python/brute_force_d20_simulator.html"},{"title":"Calculate A Moving Average","loc":"http://chrisalbon.com/r-stats/calculate-moving-average.html","text":"original source: http://stackoverflow.com/questions/743812/calculating-moving-average-in-r # create some simulated data x <- 1 : 10 library ( forecast ) Warning message: : package ‘forecast' was built under R version 3.1.3Loading required package: zoo Attaching package: ‘zoo' The following objects are masked from ‘package:base': as.Date, as.Date.numeric Loading required package: timeDate This is forecast 5.9 ma ( x , order = 5 , centre = TRUE ) [1] NA NA 3 4 5 6 7 8 NA NA","tags":"R Stats","url":"http://chrisalbon.com/r-stats/calculate-moving-average.html"},{"title":"Cartesian Product","loc":"http://chrisalbon.com/python/cartesian_product.html","text":"# import pandas as pd import pandas as pd # Create two lists i = [ 1 , 2 , 3 , 4 , 5 ] j = [ 1 , 2 , 3 , 4 , 5 ] # List every single x in i with every single y (i.e. Cartesian product) [( x , y ) for x in i for y in j ] [(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (4, 1), (4, 2), (4, 3), (4, 4), (4, 5), (5, 1), (5, 2), (5, 3), (5, 4), (5, 5)] # An alternative way to do the cartesian product # import itertools import itertools # for two sets, find the the cartisan product for i in itertools . product ([ 1 , 2 , 3 , 4 , 5 ], [ 1 , 2 , 3 , 4 , 5 ]): # and print it print ( i ) (1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (5, 1) (5, 2) (5, 3) (5, 4) (5, 5)","tags":"Python","url":"http://chrisalbon.com/python/cartesian_product.html"},{"title":"Converting Categorical Variables To Continuous","loc":"http://chrisalbon.com/r-stats/categorical-to-continous.html","text":"Original source: Learning R # Create some dirty data that because of the mispelling is imported as a character string dirty <- data.frame ( x <- c ( \"1.23\" , \"4..56\" , \"7.89\" )) # Convert the elements to numeric factor_to_numeric <- function ( f ) { as.numeric ( levels ( f ))[ as.integer ( f )] } # The data is converted, but the 4..56 is treated as an NA factor_to_numeric ( dirty $ x ) Warning message: In factor_to_numeric(dirty$x): NAs introduced by coercion [1] 1.23 NA 7.89","tags":"R Stats","url":"http://chrisalbon.com/r-stats/categorical-to-continous.html"},{"title":"Change The Barplot Bar Width","loc":"http://chrisalbon.com/r-stats/change-barplot-bar-width.html","text":"# load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # plot a default barplot ggplot ( pg_mean , aes ( x = group , y = weight )) + geom_bar ( stat = \"identity\" ) # plot a barplot with skinny bars ggplot ( pg_mean , aes ( x = group , y = weight )) + geom_bar ( stat = \"identity\" , width = 0.5 ) # plot a barplot with thick bars ggplot ( pg_mean , aes ( x = group , y = weight )) + geom_bar ( stat = \"identity\" , width = 1 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/change-barplot-bar-width.html"},{"title":"Change The Class Of An Object","loc":"http://chrisalbon.com/r-stats/change-object-class.html","text":"Changing the class of an object is called \"casting\". # Create a character string object number.of.casualties <- \"929040\" # Convert it into a numeric object number.of.casualties <- as.numeric ( number.of.casualties )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/change-object-class.html"},{"title":"Character Dates","loc":"http://chrisalbon.com/r-stats/character-dates.html","text":"# create a dates2 variable with simulated data dates2 <- as.data.frame ( cbind ( c ( 1 : 5 ), c ( \"8/31/70\" , \"NA\" , \"10/12/60\" , \"1/1/66\" , \"12/31/80\" ), c ( \"8/31/56\" , \"12-31-1977\" , \"12Aug55\" , \"July 31 1965\" , \"30jan1952\" ))) # add column names colnames ( dates2 ) <- c ( \"ID\" , \"date_factor\" , \"date_horrible\" ) # view the dataframe dates2 ID date_factor date_horrible 1 1 8/31/70 8/31/56 2 2 NA 12-31-1977 3 3 10/12/60 12Aug55 4 4 1/1/66 July 31 1965 5 5 12/31/80 30jan1952 # load the chron packaage library ( chron ) # convert date_factor into characters and the translate it into date format using format = dates2 $ date.fmt <- chron ( as.character ( dates2 $ date_factor ), format = \"m/d/y\" ) # same as above but change the output format dates2 $ date.fmt <- chron ( as.character ( dates2 $ date_factor ), format = \"m/d/y\" , out.format = \"month day year\" ) # create a new variable that is their age in years (thus divide by 360), the floor() function converts the number into integers dates2 $ age <- as.numeric ( floor ( difftime ( chron ( \"03/01/2013\" ), dates2 $ date.fmt , unit = \"days\" ) / 360 )) # Add a day to each element of a vector of ages dates2 $ date.fmt + 1 # Ask which dates in a vector came before a set date dates2 $ date.fmt < chron ( \"04/02/62\" ) Warning message: In unpaste(dates., sep = fmt$sep, fnames = fmt$periods, nfields = 3): wrong number of fields in entry(ies) 2Warning message: In unpaste(dates., sep = fmt$sep, fnames = fmt$periods, nfields = 3): wrong number of fields in entry(ies) 2 [1] September 01 1970 <NA> October 13 1960 January 02 1966 [5] January 01 1981 [1] FALSE NA TRUE FALSE FALSE # load the date package library ( date ) dates2 $ date_autofmt <- as.date ( as.character ( dates2 $ date_horrible )) dates2 [, c ( 1 , 3 , 6 )] ID date_horrible date_autofmt 1 1 8/31/56 -1218 2 2 12-31-1977 6574 3 3 12Aug55 -1603 4 4 July 31 1965 2038 5 5 30jan1952 -2893 # take the horribly formatted dates, convert them into characters, auto convert the messy dates into clean dates (using as.date) then convert it into R's date class (as.Date). Notice the capitalization of as.date and as.Date. dates2 $ date_amazing <- as.Date ( as.date ( as.character ( dates2 $ date_horrible ))) dates2 [, c ( 1 , 3 , 7 )] ID date_horrible date_amazing 1 1 8/31/56 1956-08-31 2 2 12-31-1977 1977-12-31 3 3 12Aug55 1955-08-12 4 4 July 31 1965 1965-07-31 5 5 30jan1952 1952-01-30","tags":"R Stats","url":"http://chrisalbon.com/r-stats/character-dates.html"},{"title":"Check To See If A Variable Exists","loc":"http://chrisalbon.com/r-stats/check-if-variable-exists.html","text":"original source: http://www.r-bloggers.com/check-if-a-variable-exists-in-r/ # create a dataframe with simulated values x <- runif ( 1000 ) y <- runif ( 1000 ) z <- runif ( 1000 ) a <- runif ( 1000 ) data <- data.frame ( x , y , z , a ) rm ( x , y , z , a ) # does a variable called \"x\" exists in the object \"data\"? \"x\" %in% names ( data ) [1] TRUE # does a column called \"x\" exists in the object \"data\"? \"x\" %in% colnames ( data ) [1] TRUE","tags":"R Stats","url":"http://chrisalbon.com/r-stats/check-if-variable-exists.html"},{"title":"Circle Plot","loc":"http://chrisalbon.com/r-stats/circle-plot.html","text":"original source: http://stackoverflow.com/questions/11641356/circular-plot-in-ggplot2-with-line-segments-connected-in-r #load ggplot library ( ggplot2 ) # create simulated data. start denotes where out of 10 each segment starts, end where they end, and label what they are called filld <- data.frame ( start = c ( 1 , 4 , 6 , 7.5 , 8 , 9 ), end = c ( 4 , 6 , 7.5 , 8 , 9 , 10 ), label = c ( \"A\" , \"B\" , \"C\" , \"A\" , \"C\" , \"D\" )) # create p, which is the middle of each row (to tell R where to place the label text) filld $ p <- rowMeans ( subset ( filld , select = c ( start , end ))) # create the ggplot data ggplot ( filld , aes ( xmin = start , xmax = end , ymin = 4 , ymax = 5 , fill = label )) + # draw the bar plot geom_rect () + # add the label lext geom_text ( aes ( x = p , y = 4.5 , label = label ), colour = \"white\" , size = 10 ) + # wrap the bar plot into a circle coord_polar () + # scale y between 0 and 5, so it creates an empty circle in the middle scale_y_continuous ( limits = c ( 0 , 5 ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/circle-plot.html"},{"title":"Classes","loc":"http://chrisalbon.com/r-stats/classes.html","text":"All variables have a class. Variables also have modes and storage types, but those are legacy and don't worry about them. # Find a variable's class class ( TRUE ) [1] \"logical\" R has three classes of numbers # Numeric class ( sqrt ( 3 )) [1] \"numeric\" # Complex class ( 3i ) [1] \"complex\" # Integer (add L to make a number an integer) class ( 3L ) [1] \"integer\" # Integer class ( 3 : 33 ) [1] \"integer\" R Also Has Other Classes # Characters (Strings, like text) class ( c ( \"Arizona\" , \"Maryland\" )) [1] \"character\" # Factors (like unordered categories) class ( factor ( c ( \"male\" , \"female\" ))) [1] \"factor\" # Factors have both values (i.e. a label) and a level (i.e. a numeric ID number) gender <- factor ( c ( \"male\" , \"female\" )) # View the values levels ( gender ) [1] \"female\" \"male\" # View the number of levels nlevels ( gender ) [1] 2 # View the levels of each element of a factor as.integer ( gender ) [1] 2 1 # View the values of a factor as character strings as.character ( gender ) [1] \"male\" \"female\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/classes.html"},{"title":"Cleaning Up Gender","loc":"http://chrisalbon.com/r-stats/cleaning-gender.html","text":"Original source: Learning R # create some messy fake gender data gender <- c ( \"MALE\" , \"Male\" , \"male\" , \"M\" , \"FEMALE\" , \"Female\" , \"female\" , \"f\" , NA ) # find strings that start with m and optionally ending in ale clean_gender <- str_replace ( gender , ignore.case ( \"&#94;m(ale)?$\" ), \"Male\" ) # find strings the start with f and optionally end in emale clean_gender <- str_replace ( clean_gender , ignore.case ( \"&#94;f(emale)?$\" ), \"Female\" ) clean_gender [1] \"Male\" \"Male\" \"Male\" \"Male\" \"Female\" \"Female\" \"Female\" \"Female\" [9] NA","tags":"R Stats","url":"http://chrisalbon.com/r-stats/cleaning-gender.html"},{"title":"Cleaning Strings","loc":"http://chrisalbon.com/r-stats/cleaning-strings.html","text":"Original source: Learning R R includes grep, grepl, regexpr, sub, and gsub to handle strings. However they can be clunky, so the stringr package provides a \"UI\" for these functions to making working with them easier. # Load LearningR package library ( learningr ) # Load english_monarchs data from the LearningR package data ( english_monarchs , package = \"learningr\" ) # Load Stringr Library library ( stringr ) # detect commas in the domain variable, meaning that during that time a monarch had multiple territories (domains) multiple_kingdoms <- str_detect ( english_monarchs $ domain , fixed ( \",\" )); multiple_kingdoms [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [13] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE [25] TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE [85] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [193] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE [253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE # index domains where multiple_kingdoms is true. Show name and domain columns for those rows where it is true. english_monarchs [ multiple_kingdoms , c ( \"name\" , \"domain\" )] name domain 17 Offa East Anglia, Mercia 18 Offa East Anglia, Kent, Mercia 19 Offa and Ecgfrith East Anglia, Kent, Mercia 20 Ecgfrith East Anglia, Kent, Mercia 22 C\\u009cnwulf East Anglia, Kent, Mercia 23 C\\u009cnwulf and Cynehelm East Anglia, Kent, Mercia 24 C\\u009cnwulf East Anglia, Kent, Mercia 25 Ceolwulf East Anglia, Kent, Mercia 26 Beornwulf East Anglia, Mercia 82 Ecgbehrt and Æthelwulf Kent, Wessex 83 Ecgbehrt and Æthelwulf Kent, Mercia, Wessex 84 Ecgbehrt and Æthelwulf Kent, Wessex 85 Æthelwulf and Æðelstan I Kent, Wessex 86 Æthelwulf Kent, Wessex 87 Æthelwulf and Æðelberht III Kent, Wessex 88 Æðelberht III Kent, Wessex 89 Æthelred I Kent, Wessex 95 Oswiu Mercia, Northumbria # detect either a comma or an \"and\" in the ruler variable, meaning that a domain had multiple rulers multiple_rulers <- str_detect ( english_monarchs $ name , \",|and\" ) # index domains where multiple rulers was true and that data isn't missing english_monarchs $ name [ multiple_rulers & ! is.na ( multiple_rulers )] # since individual rulers are split up by a comma or an and, we can split them up. The output is a list. individual_rulers <- str_split ( english_monarchs $ name , \", | and \" ) [1] Sigeberht and Ecgric [2] Hun, Beonna and Alberht [3] Offa and Ecgfrith [4] C\\u009cnwulf and Cynehelm [5] Sighere and Sebbi [6] Sigeheard and Swaefred [7] Eorcenberht and Eormenred [8] Oswine, Swæfbehrt, Swæfheard [9] Swæfbehrt, Swæfheard, Wihtred [10] Æðelberht II, Ælfric and Eadberht I [11] Æðelberht II and Eardwulf [12] Eadberht II, Eanmund and Sigered [13] Heaberht and Ecgbehrt II [14] Ecgbehrt and Æthelwulf [15] Ecgbehrt and Æthelwulf [16] Ecgbehrt and Æthelwulf [17] Æthelwulf and Æðelstan I [18] Æthelwulf and Æðelberht III [19] Penda and Eowa [20] Penda and Peada [21] Æthelred, Lord of the Mercians [22] Æthelflæd, Lady of the Mercians [23] Ælfwynn, Second Lady of the Mercians [24] Hálfdan and Eowils [25] Noðhelm and Watt [26] Noðhelm and Bryni [27] Noðhelm and Osric [28] Noðhelm and Æðelstan [29] Ælfwald, Oslac and Osmund [30] Ælfwald, Ealdwulf, Oslac and Osmund [31] Ælfwald, Ealdwulf, Oslac, Osmund and Oswald [32] Cenwalh and Seaxburh 211 Levels: Adda Æðelbehrt Æðelberht I ... Wulfhere # take a look at the data head ( individual_rulers [ sapply ( individual_rulers , length ) > 1 ]) [[1]] [1] \"Sigeberht\" \"Ecgric\" [[2]] [1] \"Hun\" \"Beonna\" \"Alberht\" [[3]] [1] \"Offa\" \"Ecgfrith\" [[4]] [1] \"C\\u009cnwulf\" \"Cynehelm\" [[5]] [1] \"Sighere\" \"Sebbi\" [[6]] [1] \"Sigeheard\" \"Swaefred\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/cleaning-strings.html"},{"title":"Cleaning Text","loc":"http://chrisalbon.com/python/cleaning_text.html","text":"Create some raw text # Create a list of three strings. incoming_reports = [ \"We are attacking on their left flank but are losing many men.\" , \"We cannot see the enemy army. Nothing else to report.\" , \"We are ready to attack but are waiting for your orders.\" ] Seperate by word # import word tokenizer from nltk.tokenize import word_tokenize # Apply word_tokenize to each element of the list called incoming_reports tokenized_reports = [ word_tokenize ( report ) for report in incoming_reports ] # View tokenized_reports tokenized_reports [['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men', '.'], ['We', 'can', 'not', 'see', 'the', 'enemy', 'army', '.', 'Nothing', 'else', 'to', 'report', '.'], ['We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders', '.']] # Import regex import re # Import string import string regex = re . compile ( '[ %s ]' % re . escape ( string . punctuation )) #see documentation here: http://docs.python.org/2/library/string.html tokenized_reports_no_punctuation = [] for review in tokenized_reports : new_review = [] for token in review : new_token = regex . sub ( u'' , token ) if not new_token == u'' : new_review . append ( new_token ) tokenized_reports_no_punctuation . append ( new_review ) tokenized_reports_no_punctuation [['We', 'are', 'attacking', 'on', 'their', 'left', 'flank', 'but', 'are', 'losing', 'many', 'men'], ['We', 'can', 'not', 'see', 'the', 'enemy', 'army', 'Nothing', 'else', 'to', 'report'], ['We', 'are', 'ready', 'to', 'attack', 'but', 'are', 'waiting', 'for', 'your', 'orders']] Remove filler words from nltk.corpus import stopwords tokenized_reports_no_stopwords = [] for report in tokenized_reports_no_punctuation : new_term_vector = [] for word in report : if not word in stopwords . words ( 'english' ): new_term_vector . append ( word ) tokenized_reports_no_stopwords . append ( new_term_vector ) tokenized_reports_no_stopwords [['We', 'attacking', 'left', 'flank', 'losing', 'many', 'men'], ['We', 'see', 'enemy', 'army', 'Nothing', 'else', 'report'], ['We', 'ready', 'attack', 'waiting', 'orders']]","tags":"Python","url":"http://chrisalbon.com/python/cleaning_text.html"},{"title":"Cleveland Plot","loc":"http://chrisalbon.com/r-stats/cleveland-plot.html","text":"original source: r graphics cookbook # load the ggplot2 library library ( ggplot2 ) # load the gcookbook library library ( gcookbook ) # create an object that is the top 25 hitters of 2001 tophit <- tophitters2001 [ 1 : 25 , ] # get the names, sorted first by lg, then by avg nameorder <- tophit $ name [ order ( tophit $ lg , tophit $ avg )] #order the factor variable in the order of nameorder tophit $ name <- factor ( tophit $ name , levels = nameorder ) # create the ggplot data ggplot ( tophit , aes ( x = avg , y = reorder ( name , avg ))) + # use a larger dot geom_point ( size = 3 ) + # use the simple theme theme ( panel.grid.major.x = element_blank (), panel.grid.minor.x = element_blank (), panel.grid.major.y = element_line ( colour = \"grey60\" , linetype = \"dashed\" )) # create the ggplot2 data ggplot ( tophit , aes ( x = reorder ( name , avg ), y = avg )) + geom_point ( size = 3 ) + # use the black and white theme theme_bw () + # angle axis text theme ( axis.text.x = element_text ( angle = 60 , hjust = 1 ), # no major y grid panel.grid.major.y = element_blank (), # no minor y grid panel.grid.minor.y = element_blank (), # dashed major x grids panel.grid.major.x = element_line ( colour = \"grey60\" , linetype = \"dashed\" )) # create the basic ggplot data ggplot ( tophit , aes ( x = avg , y = name )) + # create a segment plot, with names at the end of the y axis and a grey line geom_segment ( aes ( yend = name ), xend = 0 , colour = \"grey50\" ) + # plot the n points and color them geom_point ( size = 3 , aes ( colour = lg )) + # change the plot color to red and blue scale_colour_brewer ( palette = \"Set1\" , limits = c ( \"NL\" , \"AL\" ), guide = FALSE ) + # make the grid simple, black, and white theme_bw () + theme ( panel.grid.major.y = element_blank ()) + # sort the plots by lg facet_grid ( lg ~ . , scales = \"free_y\" , space = \"free_y\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/cleveland-plot.html"},{"title":"Coloring Boxplot Outliers","loc":"http://chrisalbon.com/r-stats/color-boxplot-outliers.html","text":"original source: http://stackoverflow.com/questions/8499378/ggplot2-boxplot-how-do-i-match-the-outliers-color-to-fill-aesthetics # load the ggplot2 package library ( ggplot2 ) # create the ggplot2 data with color determined by Animation m <- ggplot ( movies , aes ( y = votes , x = factor ( round ( rating )), colour = factor ( Animation ))) + # draw a boxplot without standard boxplot colors geom_boxplot ( outlier.colour = NULL ) + # draw a large y-scale scale_y_log10 () m","tags":"R Stats","url":"http://chrisalbon.com/r-stats/color-boxplot-outliers.html"},{"title":"Combining Factors","loc":"http://chrisalbon.com/r-stats/combining-factors.html","text":"Combining factors acts like interacting two variables. In other words, like interacting two binary variables to create all four possible combinations. # Create a binary variable for treatment or control treatment <- gl ( 2 , 1 , labels = c ( \"treatment\" , \"control\" )) # Create a binary variable for female or male gender <- gl ( 2 , 1 , labels = c ( \"female\" , \"male\" )) # Interact the factors by combining them interaction ( treatment , gender ) [1] treatment.female control.male Levels: treatment.female control.female treatment.male control.male","tags":"R Stats","url":"http://chrisalbon.com/r-stats/combining-factors.html"},{"title":"Comma Seperate A String","loc":"http://chrisalbon.com/r-stats/comma-seperate-a-string.html","text":"Original code: http://onertipaday.blogspot.com/2007/06/string-manipulation-insert-delim.html # Create a text string cities <- c ( \"sonomanapahealdsburgoakland\" ) # A vector with the locations of where we want each comma comma.locations <- c ( 6 , 4 , 10 , 7 ) # Place the commas into the string according to the locations specifics in comma.locations paste ( read.fwf ( textConnection ( cities ), comma.locations , as.is = TRUE ), collapse = \",\" ) [1] \"sonoma,napa,healdsburg,oakland\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/comma-seperate-a-string.html"},{"title":"Conditional execution","loc":"http://chrisalbon.com/r-stats/conditional.html","text":"# Make some data my.age <- 20 our.ages <- cbind ( 20 , 30 , 40 ) # Sometimes we might only want R to do one thing under certain conditions and another thing under other conditions. Conditional execution let's us do that. if ( my.age < 30 ) { # if my.age is greater than 30 under.30 <- TRUE # then set under.30 as TRUE } else { # if my.age is not greater than 30 under.30 <- FALSE # then set under.30 as FALSE } # close this conditional execution under.30 # display under.30 [1] TRUE # If we want to run a conditional execution on all values in a vector, we can use the ifelse function old.or.young <- ifelse ( our.ages < mean ( our.ages ), \"OLDER\" , \"YOUNGER\" ) # create an object old.or.young and add to label a value as older if the age of someone in our.age is older than the average (mean) age of the vector and \"younger\" if they are younger than the average age. old.or.young # display old.or.young [,1] [,2] [,3] [1,] \"OLDER\" \"YOUNGER\" \"YOUNGER\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/conditional.html"},{"title":"Converting Continous Variables To Categorical Variables","loc":"http://chrisalbon.com/r-stats/continous-to-categorical.html","text":"Original source: Learning R # Generate some age data of 10000 soldiers between 16 and 66 age <- 16 + 50 * rbeta ( 10000 , 2 , 3 ) # Use cut() to chunk up the observations into bins of 10 year block, the outcome is an ordered factor grouped.ages <- ordered ( cut ( age , seq.int ( 16 , 66 , 10 ))) # view a table of the results table ( grouped.ages ) grouped.ages (16,26] (26,36] (36,46] (46,56] (56,66] 1817 3473 2898 1526 286 # plot the results plot ( grouped.ages )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/continous-to-categorical.html"},{"title":"Continue And Break Loops","loc":"http://chrisalbon.com/python/continue_and_break_loops.html","text":"Import the random module import random Create a while loop # set running to true running = True # while running is true while running : # Create a random integer between 0 and 5 s = random . randint ( 0 , 5 ) # If the integer is less than 3 if s < 3 : # Print this print ( 'It is too small, starting over.' ) # Reset the next interation of the loop # (i.e skip everything below and restart from the top) continue # If the integer is 4 if s == 4 : running = False # Print this print ( 'It is 4! Changing running to false' ) # If the integer is 5, if s == 5 : # Print this print ( 'It is 5! Breaking Loop!' ) # then stop the loop break It is too small, starting over. It is too small, starting over. It is too small, starting over. It is 5! Breaking Loop!","tags":"Python","url":"http://chrisalbon.com/python/continue_and_break_loops.html"},{"title":"Converting Between Wide And Long Form Data","loc":"http://chrisalbon.com/r-stats/convert-between-wide-and-long.html","text":"Original Source: Learning R # Load the reshape library library ( reshape2 ) # Generate five subjects, each with five measurements name <- c ( \"Jack\" , \"Jill\" , \"Steve\" , \"Jake\" , \"Bill\" ) obs1 <- runif ( 5 ) obs2 <- runif ( 5 ) obs3 <- runif ( 5 ) obs4 <- runif ( 5 ) obs5 <- runif ( 5 ) # create a wide form data frame. That is, one row per person, one column per measurement survey.wide <- data.frame ( name , obs1 , obs2 , obs3 , obs4 , obs5 ) # convert from wide to long. id.vars denotes the variable to make the transition by and measure.vars denotes the variables to flip survey.long <- melt ( survey.wide , id.vars = \"name\" , measure.vars = c ( \"obs1\" , \"obs2\" , \"obs3\" , \"obs4\" , \"obs5\" )) survey.long name variable value 1 Jack obs1 0.74041288 2 Jill obs1 0.52529347 3 Steve obs1 0.97317970 4 Jake obs1 0.85910345 5 Bill obs1 0.55121888 6 Jack obs2 0.03121784 7 Jill obs2 0.30319273 8 Steve obs2 0.05282432 9 Jake obs2 0.31912928 10 Bill obs2 0.15419403 11 Jack obs3 0.11751059 12 Jill obs3 0.15045148 13 Steve obs3 0.45201750 14 Jake obs3 0.99880524 15 Bill obs3 0.73128530 16 Jack obs4 0.09517355 17 Jill obs4 0.97003014 18 Steve obs4 0.57478533 19 Jake obs4 0.68951869 20 Bill obs4 0.17464191 21 Jack obs5 0.85086481 22 Jill obs5 0.45689664 23 Steve obs5 0.80029436 24 Jake obs5 0.14644067 25 Bill obs5 0.36649096 # convert long to wide survey.wide.2 <- dcast ( survey.long , name ~ variable ) survey.wide.2 name obs1 obs2 obs3 obs4 obs5 1 Bill 0.5512189 0.15419403 0.7312853 0.17464191 0.3664910 2 Jack 0.7404129 0.03121784 0.1175106 0.09517355 0.8508648 3 Jake 0.8591035 0.31912928 0.9988052 0.68951869 0.1464407 4 Jill 0.5252935 0.30319273 0.1504515 0.97003014 0.4568966 5 Steve 0.9731797 0.05282432 0.4520175 0.57478533 0.8002944","tags":"R Stats","url":"http://chrisalbon.com/r-stats/convert-between-wide-and-long.html"},{"title":"Converting List Of Dataframes Into A Single Dataframe","loc":"http://chrisalbon.com/r-stats/convert-list-of-dfs-into-one-df.html","text":"original source: http://stackoverflow.com/questions/2851327/converting-a-list-of-data-frames-into-one-data-frame-in-r Imagine we have two dataframes of numeric data contained within a single list, and we want to combine that data by stacking them on top of each other as a single dataframe # create some simulated data high <- data.frame ( a = 1001 : 1005 , b = 1006 : 1010 ) low <- data.frame ( a = 1 : 5 , b = 6 : 10 ) # combine them into a list df.list <- list ( low , high ) # rbind across every item of df.list and view it df <- do.call ( \"rbind\" , df.list ); df a b 1 1 6 2 2 7 3 3 8 4 4 9 5 5 10 6 1001 1006 7 1002 1007 8 1003 1008 9 1004 1009 10 1005 1010","tags":"R Stats","url":"http://chrisalbon.com/r-stats/convert-list-of-dfs-into-one-df.html"},{"title":"Convert Strings To Objects","loc":"http://chrisalbon.com/r-stats/convert-strings-to-objects.html","text":"# Create a vector of strings name <- c ( \"John\" , \"Jason\" , \"Sarah\" ) # Create a variable from the first element of the \"name\" variable, called that element's string and give that variable from values. assign ( name [ 1 ], c ( 23 , 24 , 33 , 46 , 65 , 86 )) # Display the variable \"John\" John [1] 23 24 33 46 65 86","tags":"R Stats","url":"http://chrisalbon.com/r-stats/convert-strings-to-objects.html"},{"title":"Convert A String Categorical Variable To A Numeric Variable","loc":"http://chrisalbon.com/python/convert_categorical_to_numeric.html","text":"Originally from: Data Origami . import modules import pandas as pd Create dataframe raw_data = { 'patient' : [ 1 , 1 , 1 , 2 , 2 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 'strong' , 'weak' , 'normal' , 'weak' , 'strong' ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) df patient obs treatment score 0 1 1 0 strong 1 1 2 1 weak 2 1 3 0 normal 3 2 1 1 weak 4 2 2 0 strong Create a function that converts all values of df['score'] into numbers def score_to_numeric ( x ): if x == 'strong' : return 3 if x == 'normal' : return 2 if x == 'weak' : return 1 Apply the function to the score variable df [ 'score_num' ] = df [ 'score' ] . apply ( score_to_numeric ) df patient obs treatment score score_num 0 1 1 0 strong 3 1 1 2 1 weak 1 2 1 3 0 normal 2 3 2 1 1 weak 1 4 2 2 0 strong 3","tags":"Python","url":"http://chrisalbon.com/python/convert_categorical_to_numeric.html"},{"title":"Count The Number Of Times X Appears In A Vector","loc":"http://chrisalbon.com/r-stats/count-number-elements.html","text":"# create a vector of simulated data numbers <- c ( 4 , 23 , 4 , 23 , 5 , 43 , 54 , 56 , 657 , 67 , 67 , 435 , 453 , 435 , 324 , 34 , 456 , 56 , 567 , 65 , 34 , 435 ) # create an object that is our x x <- 67 # the length of the subset of the variable numbers where numbers equals x length ( subset ( numbers , numbers == x )) [1] 2","tags":"R Stats","url":"http://chrisalbon.com/r-stats/count-number-elements.html"},{"title":"Convert All Jupyter Notebooks In A Folder To Basic HTML Using Bash","loc":"http://chrisalbon.com/command-line/covert_all_ipynb_to_html.html","text":"Note: This code has been commented out. To run the code, remove the comments. # %%bash # #!/bin/bash # sets the working directory to the current directory # acd \"$(dirname \"$0\")\" # converts all Jupyter Notebook files to basic html # for f in *.ipynb; do jupyter nbconvert --to html --template basic $f; done [NbConvertApp] Converting notebook covert_all_ipynb_to_html.ipynb to html [NbConvertApp] Writing 2632 bytes to covert_all_ipynb_to_html.html [NbConvertApp] Converting notebook list_all_files_and_folders_in_a_directory.ipynb to html [NbConvertApp] Writing 2153 bytes to list_all_files_and_folders_in_a_directory.html [NbConvertApp] Converting notebook open_ipython_nb_in_nondefault_browser.ipynb to html [NbConvertApp] Writing 1552 bytes to open_ipython_nb_in_nondefault_browser.html","tags":"Command Line","url":"http://chrisalbon.com/command-line/covert_all_ipynb_to_html.html"},{"title":"Creating Lists From Dictionary Keys And Values","loc":"http://chrisalbon.com/python/create_list_from_dictionary_keys_and_values.html","text":"Create a dictionary dict = { 'county' : [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'fireReports' : [ 4 , 24 , 31 , 2 , 3 ]} Create a list from the dictionary keys # Create a list to place the dictionary keys in dictionaryKeys = [] # For each key in the dictionary's keys, for key in dict . keys (): # add the key to dictionaryKeys dictionaryKeys . append ( key ) # View the dictionaryKeys list dictionaryKeys ['year', 'county', 'fireReports'] Create a list from the dictionary values # Create a list to place the dictionary values in dictionaryValues = [] # For each key in the dictionary's Values, for x in dict . values (): # add the key to dictionaryValues dictionaryValues . append ( x ) # View the dictionaryValues list dictionaryValues [[2012, 2012, 2013, 2014, 2014], ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'], [4, 24, 31, 2, 3]]","tags":"Python","url":"http://chrisalbon.com/python/create_list_from_dictionary_keys_and_values.html"},{"title":"Creating Numpy Arrays","loc":"http://chrisalbon.com/python/creating_numpy_arrays.html","text":"# Import Modules import numpy as np # Create a list regimentSize = [ 534 , 5468 , 6546 , 542 , 9856 , 4125 ] # Create a ndarray from the regimentSize list regimentSizeArray = np . array ( regimentSize ); regimentSizeArray array([ 534, 5468, 6546, 542, 9856, 4125]) # What are the number of dimensions of the array? regimentSizeArray . ndim 1 # What is the shape of the array? regimentSizeArray . shape (6,) Nested Lists To Multidimensional Arrays # Create two lists regimentSizePreWar = [ 534 , 5468 , 6546 , 542 , 9856 , 4125 ] regimentSizePostWar = [ 234 , 255 , 267 , 732 , 235 , 723 ] # Create a ndarray from a nested list regimentSizePrePostArray = np . array ([ regimentSizePreWar , regimentSizePostWar ]); regimentSizePrePostArray array([[ 534, 5468, 6546, 542, 9856, 4125], [ 234, 255, 267, 732, 235, 723]]) # What are the number of dimensions of the array? regimentSizePrePostArray . ndim 2 # What is the shape of the array? regimentSizePrePostArray . shape (2, 6)","tags":"Python","url":"http://chrisalbon.com/python/creating_numpy_arrays.html"},{"title":"Crosstabs","loc":"http://chrisalbon.com/r-stats/crosstabs.html","text":"# create some simulated disaster data event <- c ( \"flood\" , \"fire\" , \"flood\" , \"fire\" , \"riot\" , \"flood\" , \"riot\" , \"riot\" , \"flood\" ); location <- c ( \"africa\" , \"asia\" , \"europe\" , \"africa\" , \"asia\" , \"europe\" , \"africa\" , \"asia\" , \"europe\" ) disaster <- data.frame ( event , location ) rm ( event , location ) # create a variable that is the frequency counts of different types of disaster events event.counts.df <- as.data.frame ( table ( disaster $ event )); event.counts.df Var1 Freq 1 fire 2 2 flood 4 3 riot 3 # create a crosstab of event types by location disaster.crosstab <- table ( disaster $ event , disaster $ location ); disaster.crosstab africa asia europe fire 1 1 0 flood 1 0 3 riot 1 2 0","tags":"R Stats","url":"http://chrisalbon.com/r-stats/crosstabs.html"},{"title":"Convert A CSV Into Python Code To Recreate It","loc":"http://chrisalbon.com/python/csv_to_python_code.html","text":"This might seem like a strange bit of code, but it serves a very valuable (though niche) function. I prefer to the code in my tutorials to not rely on outside data to run. That is, dieally the data is created within the code itself, rather than requiring loading an data from a csv file. Obviously this is not reasonable for real analyses, but for tutorials it can make everything simpler and easier. However, this preference to embed the generation of data in the snippets themselves becomes a problem when I want to use data found in existing datasets. So, I created this script to complete one simple task: To take a dataset and generate the python code required to recreate it. Preliminaries # Import the pandas package import pandas as pd Load the external dataset # Load the csv file as a pandas dataframe df_original = pd . read_csv ( 'http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv' ) df = pd . read_csv ( 'http://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv' ) Print the code required to create that dataset # Print the code to create the dataframe print ( '==============================' ) print ( 'RUN THE CODE BELOW THIS LINE' ) print ( '==============================' ) print ( 'raw_data =' , df . to_dict ( orient = 'list' )) print ( 'df = pd.DataFrame(raw_data, columns = ' + str ( list ( df_original )) + ')' ) ============================== RUN THE CODE BELOW THIS LINE ============================== raw_data = {'Sepal.Length': [5.0999999999999996, 4.9000000000000004, 4.7000000000000002, 4.5999999999999996, 5.0, 5.4000000000000004, 4.5999999999999996, 5.0, 4.4000000000000004, 4.9000000000000004, 5.4000000000000004, 4.7999999999999998, 4.7999999999999998, 4.2999999999999998, 5.7999999999999998, 5.7000000000000002, 5.4000000000000004, 5.0999999999999996, 5.7000000000000002, 5.0999999999999996, 5.4000000000000004, 5.0999999999999996, 4.5999999999999996, 5.0999999999999996, 4.7999999999999998, 5.0, 5.0, 5.2000000000000002, 5.2000000000000002, 4.7000000000000002, 4.7999999999999998, 5.4000000000000004, 5.2000000000000002, 5.5, 4.9000000000000004, 5.0, 5.5, 4.9000000000000004, 4.4000000000000004, 5.0999999999999996, 5.0, 4.5, 4.4000000000000004, 5.0, 5.0999999999999996, 4.7999999999999998, 5.0999999999999996, 4.5999999999999996, 5.2999999999999998, 5.0, 7.0, 6.4000000000000004, 6.9000000000000004, 5.5, 6.5, 5.7000000000000002, 6.2999999999999998, 4.9000000000000004, 6.5999999999999996, 5.2000000000000002, 5.0, 5.9000000000000004, 6.0, 6.0999999999999996, 5.5999999999999996, 6.7000000000000002, 5.5999999999999996, 5.7999999999999998, 6.2000000000000002, 5.5999999999999996, 5.9000000000000004, 6.0999999999999996, 6.2999999999999998, 6.0999999999999996, 6.4000000000000004, 6.5999999999999996, 6.7999999999999998, 6.7000000000000002, 6.0, 5.7000000000000002, 5.5, 5.5, 5.7999999999999998, 6.0, 5.4000000000000004, 6.0, 6.7000000000000002, 6.2999999999999998, 5.5999999999999996, 5.5, 5.5, 6.0999999999999996, 5.7999999999999998, 5.0, 5.5999999999999996, 5.7000000000000002, 5.7000000000000002, 6.2000000000000002, 5.0999999999999996, 5.7000000000000002, 6.2999999999999998, 5.7999999999999998, 7.0999999999999996, 6.2999999999999998, 6.5, 7.5999999999999996, 4.9000000000000004, 7.2999999999999998, 6.7000000000000002, 7.2000000000000002, 6.5, 6.4000000000000004, 6.7999999999999998, 5.7000000000000002, 5.7999999999999998, 6.4000000000000004, 6.5, 7.7000000000000002, 7.7000000000000002, 6.0, 6.9000000000000004, 5.5999999999999996, 7.7000000000000002, 6.2999999999999998, 6.7000000000000002, 7.2000000000000002, 6.2000000000000002, 6.0999999999999996, 6.4000000000000004, 7.2000000000000002, 7.4000000000000004, 7.9000000000000004, 6.4000000000000004, 6.2999999999999998, 6.0999999999999996, 7.7000000000000002, 6.2999999999999998, 6.4000000000000004, 6.0, 6.9000000000000004, 6.7000000000000002, 6.9000000000000004, 5.7999999999999998, 6.7999999999999998, 6.7000000000000002, 6.7000000000000002, 6.2999999999999998, 6.5, 6.2000000000000002, 5.9000000000000004], 'Petal.Width': [0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.40000000000000002, 0.29999999999999999, 0.20000000000000001, 0.20000000000000001, 0.10000000000000001, 0.20000000000000001, 0.20000000000000001, 0.10000000000000001, 0.10000000000000001, 0.20000000000000001, 0.40000000000000002, 0.40000000000000002, 0.29999999999999999, 0.29999999999999999, 0.29999999999999999, 0.20000000000000001, 0.40000000000000002, 0.20000000000000001, 0.5, 0.20000000000000001, 0.20000000000000001, 0.40000000000000002, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.40000000000000002, 0.10000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.10000000000000001, 0.20000000000000001, 0.20000000000000001, 0.29999999999999999, 0.29999999999999999, 0.20000000000000001, 0.59999999999999998, 0.40000000000000002, 0.29999999999999999, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 1.3999999999999999, 1.5, 1.5, 1.3, 1.5, 1.3, 1.6000000000000001, 1.0, 1.3, 1.3999999999999999, 1.0, 1.5, 1.0, 1.3999999999999999, 1.3, 1.3999999999999999, 1.5, 1.0, 1.5, 1.1000000000000001, 1.8, 1.3, 1.5, 1.2, 1.3, 1.3999999999999999, 1.3999999999999999, 1.7, 1.5, 1.0, 1.1000000000000001, 1.0, 1.2, 1.6000000000000001, 1.5, 1.6000000000000001, 1.5, 1.3, 1.3, 1.3, 1.2, 1.3999999999999999, 1.2, 1.0, 1.3, 1.2, 1.3, 1.3, 1.1000000000000001, 1.3, 2.5, 1.8999999999999999, 2.1000000000000001, 1.8, 2.2000000000000002, 2.1000000000000001, 1.7, 1.8, 1.8, 2.5, 2.0, 1.8999999999999999, 2.1000000000000001, 2.0, 2.3999999999999999, 2.2999999999999998, 1.8, 2.2000000000000002, 2.2999999999999998, 1.5, 2.2999999999999998, 2.0, 2.0, 1.8, 2.1000000000000001, 1.8, 1.8, 1.8, 2.1000000000000001, 1.6000000000000001, 1.8999999999999999, 2.0, 2.2000000000000002, 1.5, 1.3999999999999999, 2.2999999999999998, 2.3999999999999999, 1.8, 1.8, 2.1000000000000001, 2.3999999999999999, 2.2999999999999998, 1.8999999999999999, 2.2999999999999998, 2.5, 2.2999999999999998, 1.8999999999999999, 2.0, 2.2999999999999998, 1.8], 'Petal.Length': [1.3999999999999999, 1.3999999999999999, 1.3, 1.5, 1.3999999999999999, 1.7, 1.3999999999999999, 1.5, 1.3999999999999999, 1.5, 1.5, 1.6000000000000001, 1.3999999999999999, 1.1000000000000001, 1.2, 1.5, 1.3, 1.3999999999999999, 1.7, 1.5, 1.7, 1.5, 1.0, 1.7, 1.8999999999999999, 1.6000000000000001, 1.6000000000000001, 1.5, 1.3999999999999999, 1.6000000000000001, 1.6000000000000001, 1.5, 1.5, 1.3999999999999999, 1.5, 1.2, 1.3, 1.3999999999999999, 1.3, 1.5, 1.3, 1.3, 1.3, 1.6000000000000001, 1.8999999999999999, 1.3999999999999999, 1.6000000000000001, 1.3999999999999999, 1.5, 1.3999999999999999, 4.7000000000000002, 4.5, 4.9000000000000004, 4.0, 4.5999999999999996, 4.5, 4.7000000000000002, 3.2999999999999998, 4.5999999999999996, 3.8999999999999999, 3.5, 4.2000000000000002, 4.0, 4.7000000000000002, 3.6000000000000001, 4.4000000000000004, 4.5, 4.0999999999999996, 4.5, 3.8999999999999999, 4.7999999999999998, 4.0, 4.9000000000000004, 4.7000000000000002, 4.2999999999999998, 4.4000000000000004, 4.7999999999999998, 5.0, 4.5, 3.5, 3.7999999999999998, 3.7000000000000002, 3.8999999999999999, 5.0999999999999996, 4.5, 4.5, 4.7000000000000002, 4.4000000000000004, 4.0999999999999996, 4.0, 4.4000000000000004, 4.5999999999999996, 4.0, 3.2999999999999998, 4.2000000000000002, 4.2000000000000002, 4.2000000000000002, 4.2999999999999998, 3.0, 4.0999999999999996, 6.0, 5.0999999999999996, 5.9000000000000004, 5.5999999999999996, 5.7999999999999998, 6.5999999999999996, 4.5, 6.2999999999999998, 5.7999999999999998, 6.0999999999999996, 5.0999999999999996, 5.2999999999999998, 5.5, 5.0, 5.0999999999999996, 5.2999999999999998, 5.5, 6.7000000000000002, 6.9000000000000004, 5.0, 5.7000000000000002, 4.9000000000000004, 6.7000000000000002, 4.9000000000000004, 5.7000000000000002, 6.0, 4.7999999999999998, 4.9000000000000004, 5.5999999999999996, 5.7999999999999998, 6.0999999999999996, 6.4000000000000004, 5.5999999999999996, 5.0999999999999996, 5.5999999999999996, 6.0999999999999996, 5.5999999999999996, 5.5, 4.7999999999999998, 5.4000000000000004, 5.5999999999999996, 5.0999999999999996, 5.0999999999999996, 5.9000000000000004, 5.7000000000000002, 5.2000000000000002, 5.0, 5.2000000000000002, 5.4000000000000004, 5.0999999999999996], 'Species': ['setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'setosa', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'versicolor', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica', 'virginica'], 'Sepal.Width': [3.5, 3.0, 3.2000000000000002, 3.1000000000000001, 3.6000000000000001, 3.8999999999999999, 3.3999999999999999, 3.3999999999999999, 2.8999999999999999, 3.1000000000000001, 3.7000000000000002, 3.3999999999999999, 3.0, 3.0, 4.0, 4.4000000000000004, 3.8999999999999999, 3.5, 3.7999999999999998, 3.7999999999999998, 3.3999999999999999, 3.7000000000000002, 3.6000000000000001, 3.2999999999999998, 3.3999999999999999, 3.0, 3.3999999999999999, 3.5, 3.3999999999999999, 3.2000000000000002, 3.1000000000000001, 3.3999999999999999, 4.0999999999999996, 4.2000000000000002, 3.1000000000000001, 3.2000000000000002, 3.5, 3.6000000000000001, 3.0, 3.3999999999999999, 3.5, 2.2999999999999998, 3.2000000000000002, 3.5, 3.7999999999999998, 3.0, 3.7999999999999998, 3.2000000000000002, 3.7000000000000002, 3.2999999999999998, 3.2000000000000002, 3.2000000000000002, 3.1000000000000001, 2.2999999999999998, 2.7999999999999998, 2.7999999999999998, 3.2999999999999998, 2.3999999999999999, 2.8999999999999999, 2.7000000000000002, 2.0, 3.0, 2.2000000000000002, 2.8999999999999999, 2.8999999999999999, 3.1000000000000001, 3.0, 2.7000000000000002, 2.2000000000000002, 2.5, 3.2000000000000002, 2.7999999999999998, 2.5, 2.7999999999999998, 2.8999999999999999, 3.0, 2.7999999999999998, 3.0, 2.8999999999999999, 2.6000000000000001, 2.3999999999999999, 2.3999999999999999, 2.7000000000000002, 2.7000000000000002, 3.0, 3.3999999999999999, 3.1000000000000001, 2.2999999999999998, 3.0, 2.5, 2.6000000000000001, 3.0, 2.6000000000000001, 2.2999999999999998, 2.7000000000000002, 3.0, 2.8999999999999999, 2.8999999999999999, 2.5, 2.7999999999999998, 3.2999999999999998, 2.7000000000000002, 3.0, 2.8999999999999999, 3.0, 3.0, 2.5, 2.8999999999999999, 2.5, 3.6000000000000001, 3.2000000000000002, 2.7000000000000002, 3.0, 2.5, 2.7999999999999998, 3.2000000000000002, 3.0, 3.7999999999999998, 2.6000000000000001, 2.2000000000000002, 3.2000000000000002, 2.7999999999999998, 2.7999999999999998, 2.7000000000000002, 3.2999999999999998, 3.2000000000000002, 2.7999999999999998, 3.0, 2.7999999999999998, 3.0, 2.7999999999999998, 3.7999999999999998, 2.7999999999999998, 2.7999999999999998, 2.6000000000000001, 3.0, 3.3999999999999999, 3.1000000000000001, 3.0, 3.1000000000000001, 3.1000000000000001, 3.1000000000000001, 2.7000000000000002, 3.2000000000000002, 3.2999999999999998, 3.0, 2.5, 3.0, 3.3999999999999999, 3.0], 'Unnamed: 0': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]} df = pd.DataFrame(raw_data, columns = ['Unnamed: 0', 'Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width', 'Species']) If you want to check the results... 1. Enter the code produced from the cell above in this cell raw_data = { 'Petal.Width' : [ 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.29999999999999999 , 0.20000000000000001 , 0.20000000000000001 , 0.10000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.10000000000000001 , 0.10000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.40000000000000002 , 0.29999999999999999 , 0.29999999999999999 , 0.29999999999999999 , 0.20000000000000001 , 0.40000000000000002 , 0.20000000000000001 , 0.5 , 0.20000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.40000000000000002 , 0.10000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.10000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.29999999999999999 , 0.29999999999999999 , 0.20000000000000001 , 0.59999999999999998 , 0.40000000000000002 , 0.29999999999999999 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 0.20000000000000001 , 1.3999999999999999 , 1.5 , 1.5 , 1.3 , 1.5 , 1.3 , 1.6000000000000001 , 1.0 , 1.3 , 1.3999999999999999 , 1.0 , 1.5 , 1.0 , 1.3999999999999999 , 1.3 , 1.3999999999999999 , 1.5 , 1.0 , 1.5 , 1.1000000000000001 , 1.8 , 1.3 , 1.5 , 1.2 , 1.3 , 1.3999999999999999 , 1.3999999999999999 , 1.7 , 1.5 , 1.0 , 1.1000000000000001 , 1.0 , 1.2 , 1.6000000000000001 , 1.5 , 1.6000000000000001 , 1.5 , 1.3 , 1.3 , 1.3 , 1.2 , 1.3999999999999999 , 1.2 , 1.0 , 1.3 , 1.2 , 1.3 , 1.3 , 1.1000000000000001 , 1.3 , 2.5 , 1.8999999999999999 , 2.1000000000000001 , 1.8 , 2.2000000000000002 , 2.1000000000000001 , 1.7 , 1.8 , 1.8 , 2.5 , 2.0 , 1.8999999999999999 , 2.1000000000000001 , 2.0 , 2.3999999999999999 , 2.2999999999999998 , 1.8 , 2.2000000000000002 , 2.2999999999999998 , 1.5 , 2.2999999999999998 , 2.0 , 2.0 , 1.8 , 2.1000000000000001 , 1.8 , 1.8 , 1.8 , 2.1000000000000001 , 1.6000000000000001 , 1.8999999999999999 , 2.0 , 2.2000000000000002 , 1.5 , 1.3999999999999999 , 2.2999999999999998 , 2.3999999999999999 , 1.8 , 1.8 , 2.1000000000000001 , 2.3999999999999999 , 2.2999999999999998 , 1.8999999999999999 , 2.2999999999999998 , 2.5 , 2.2999999999999998 , 1.8999999999999999 , 2.0 , 2.2999999999999998 , 1.8 ], 'Sepal.Width' : [ 3.5 , 3.0 , 3.2000000000000002 , 3.1000000000000001 , 3.6000000000000001 , 3.8999999999999999 , 3.3999999999999999 , 3.3999999999999999 , 2.8999999999999999 , 3.1000000000000001 , 3.7000000000000002 , 3.3999999999999999 , 3.0 , 3.0 , 4.0 , 4.4000000000000004 , 3.8999999999999999 , 3.5 , 3.7999999999999998 , 3.7999999999999998 , 3.3999999999999999 , 3.7000000000000002 , 3.6000000000000001 , 3.2999999999999998 , 3.3999999999999999 , 3.0 , 3.3999999999999999 , 3.5 , 3.3999999999999999 , 3.2000000000000002 , 3.1000000000000001 , 3.3999999999999999 , 4.0999999999999996 , 4.2000000000000002 , 3.1000000000000001 , 3.2000000000000002 , 3.5 , 3.6000000000000001 , 3.0 , 3.3999999999999999 , 3.5 , 2.2999999999999998 , 3.2000000000000002 , 3.5 , 3.7999999999999998 , 3.0 , 3.7999999999999998 , 3.2000000000000002 , 3.7000000000000002 , 3.2999999999999998 , 3.2000000000000002 , 3.2000000000000002 , 3.1000000000000001 , 2.2999999999999998 , 2.7999999999999998 , 2.7999999999999998 , 3.2999999999999998 , 2.3999999999999999 , 2.8999999999999999 , 2.7000000000000002 , 2.0 , 3.0 , 2.2000000000000002 , 2.8999999999999999 , 2.8999999999999999 , 3.1000000000000001 , 3.0 , 2.7000000000000002 , 2.2000000000000002 , 2.5 , 3.2000000000000002 , 2.7999999999999998 , 2.5 , 2.7999999999999998 , 2.8999999999999999 , 3.0 , 2.7999999999999998 , 3.0 , 2.8999999999999999 , 2.6000000000000001 , 2.3999999999999999 , 2.3999999999999999 , 2.7000000000000002 , 2.7000000000000002 , 3.0 , 3.3999999999999999 , 3.1000000000000001 , 2.2999999999999998 , 3.0 , 2.5 , 2.6000000000000001 , 3.0 , 2.6000000000000001 , 2.2999999999999998 , 2.7000000000000002 , 3.0 , 2.8999999999999999 , 2.8999999999999999 , 2.5 , 2.7999999999999998 , 3.2999999999999998 , 2.7000000000000002 , 3.0 , 2.8999999999999999 , 3.0 , 3.0 , 2.5 , 2.8999999999999999 , 2.5 , 3.6000000000000001 , 3.2000000000000002 , 2.7000000000000002 , 3.0 , 2.5 , 2.7999999999999998 , 3.2000000000000002 , 3.0 , 3.7999999999999998 , 2.6000000000000001 , 2.2000000000000002 , 3.2000000000000002 , 2.7999999999999998 , 2.7999999999999998 , 2.7000000000000002 , 3.2999999999999998 , 3.2000000000000002 , 2.7999999999999998 , 3.0 , 2.7999999999999998 , 3.0 , 2.7999999999999998 , 3.7999999999999998 , 2.7999999999999998 , 2.7999999999999998 , 2.6000000000000001 , 3.0 , 3.3999999999999999 , 3.1000000000000001 , 3.0 , 3.1000000000000001 , 3.1000000000000001 , 3.1000000000000001 , 2.7000000000000002 , 3.2000000000000002 , 3.2999999999999998 , 3.0 , 2.5 , 3.0 , 3.3999999999999999 , 3.0 ], 'Species' : [ 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'setosa' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'versicolor' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' , 'virginica' ], 'Unnamed: 0' : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , 34 , 35 , 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 , 47 , 48 , 49 , 50 , 51 , 52 , 53 , 54 , 55 , 56 , 57 , 58 , 59 , 60 , 61 , 62 , 63 , 64 , 65 , 66 , 67 , 68 , 69 , 70 , 71 , 72 , 73 , 74 , 75 , 76 , 77 , 78 , 79 , 80 , 81 , 82 , 83 , 84 , 85 , 86 , 87 , 88 , 89 , 90 , 91 , 92 , 93 , 94 , 95 , 96 , 97 , 98 , 99 , 100 , 101 , 102 , 103 , 104 , 105 , 106 , 107 , 108 , 109 , 110 , 111 , 112 , 113 , 114 , 115 , 116 , 117 , 118 , 119 , 120 , 121 , 122 , 123 , 124 , 125 , 126 , 127 , 128 , 129 , 130 , 131 , 132 , 133 , 134 , 135 , 136 , 137 , 138 , 139 , 140 , 141 , 142 , 143 , 144 , 145 , 146 , 147 , 148 , 149 , 150 ], 'Sepal.Length' : [ 5.0999999999999996 , 4.9000000000000004 , 4.7000000000000002 , 4.5999999999999996 , 5.0 , 5.4000000000000004 , 4.5999999999999996 , 5.0 , 4.4000000000000004 , 4.9000000000000004 , 5.4000000000000004 , 4.7999999999999998 , 4.7999999999999998 , 4.2999999999999998 , 5.7999999999999998 , 5.7000000000000002 , 5.4000000000000004 , 5.0999999999999996 , 5.7000000000000002 , 5.0999999999999996 , 5.4000000000000004 , 5.0999999999999996 , 4.5999999999999996 , 5.0999999999999996 , 4.7999999999999998 , 5.0 , 5.0 , 5.2000000000000002 , 5.2000000000000002 , 4.7000000000000002 , 4.7999999999999998 , 5.4000000000000004 , 5.2000000000000002 , 5.5 , 4.9000000000000004 , 5.0 , 5.5 , 4.9000000000000004 , 4.4000000000000004 , 5.0999999999999996 , 5.0 , 4.5 , 4.4000000000000004 , 5.0 , 5.0999999999999996 , 4.7999999999999998 , 5.0999999999999996 , 4.5999999999999996 , 5.2999999999999998 , 5.0 , 7.0 , 6.4000000000000004 , 6.9000000000000004 , 5.5 , 6.5 , 5.7000000000000002 , 6.2999999999999998 , 4.9000000000000004 , 6.5999999999999996 , 5.2000000000000002 , 5.0 , 5.9000000000000004 , 6.0 , 6.0999999999999996 , 5.5999999999999996 , 6.7000000000000002 , 5.5999999999999996 , 5.7999999999999998 , 6.2000000000000002 , 5.5999999999999996 , 5.9000000000000004 , 6.0999999999999996 , 6.2999999999999998 , 6.0999999999999996 , 6.4000000000000004 , 6.5999999999999996 , 6.7999999999999998 , 6.7000000000000002 , 6.0 , 5.7000000000000002 , 5.5 , 5.5 , 5.7999999999999998 , 6.0 , 5.4000000000000004 , 6.0 , 6.7000000000000002 , 6.2999999999999998 , 5.5999999999999996 , 5.5 , 5.5 , 6.0999999999999996 , 5.7999999999999998 , 5.0 , 5.5999999999999996 , 5.7000000000000002 , 5.7000000000000002 , 6.2000000000000002 , 5.0999999999999996 , 5.7000000000000002 , 6.2999999999999998 , 5.7999999999999998 , 7.0999999999999996 , 6.2999999999999998 , 6.5 , 7.5999999999999996 , 4.9000000000000004 , 7.2999999999999998 , 6.7000000000000002 , 7.2000000000000002 , 6.5 , 6.4000000000000004 , 6.7999999999999998 , 5.7000000000000002 , 5.7999999999999998 , 6.4000000000000004 , 6.5 , 7.7000000000000002 , 7.7000000000000002 , 6.0 , 6.9000000000000004 , 5.5999999999999996 , 7.7000000000000002 , 6.2999999999999998 , 6.7000000000000002 , 7.2000000000000002 , 6.2000000000000002 , 6.0999999999999996 , 6.4000000000000004 , 7.2000000000000002 , 7.4000000000000004 , 7.9000000000000004 , 6.4000000000000004 , 6.2999999999999998 , 6.0999999999999996 , 7.7000000000000002 , 6.2999999999999998 , 6.4000000000000004 , 6.0 , 6.9000000000000004 , 6.7000000000000002 , 6.9000000000000004 , 5.7999999999999998 , 6.7999999999999998 , 6.7000000000000002 , 6.7000000000000002 , 6.2999999999999998 , 6.5 , 6.2000000000000002 , 5.9000000000000004 ], 'Petal.Length' : [ 1.3999999999999999 , 1.3999999999999999 , 1.3 , 1.5 , 1.3999999999999999 , 1.7 , 1.3999999999999999 , 1.5 , 1.3999999999999999 , 1.5 , 1.5 , 1.6000000000000001 , 1.3999999999999999 , 1.1000000000000001 , 1.2 , 1.5 , 1.3 , 1.3999999999999999 , 1.7 , 1.5 , 1.7 , 1.5 , 1.0 , 1.7 , 1.8999999999999999 , 1.6000000000000001 , 1.6000000000000001 , 1.5 , 1.3999999999999999 , 1.6000000000000001 , 1.6000000000000001 , 1.5 , 1.5 , 1.3999999999999999 , 1.5 , 1.2 , 1.3 , 1.3999999999999999 , 1.3 , 1.5 , 1.3 , 1.3 , 1.3 , 1.6000000000000001 , 1.8999999999999999 , 1.3999999999999999 , 1.6000000000000001 , 1.3999999999999999 , 1.5 , 1.3999999999999999 , 4.7000000000000002 , 4.5 , 4.9000000000000004 , 4.0 , 4.5999999999999996 , 4.5 , 4.7000000000000002 , 3.2999999999999998 , 4.5999999999999996 , 3.8999999999999999 , 3.5 , 4.2000000000000002 , 4.0 , 4.7000000000000002 , 3.6000000000000001 , 4.4000000000000004 , 4.5 , 4.0999999999999996 , 4.5 , 3.8999999999999999 , 4.7999999999999998 , 4.0 , 4.9000000000000004 , 4.7000000000000002 , 4.2999999999999998 , 4.4000000000000004 , 4.7999999999999998 , 5.0 , 4.5 , 3.5 , 3.7999999999999998 , 3.7000000000000002 , 3.8999999999999999 , 5.0999999999999996 , 4.5 , 4.5 , 4.7000000000000002 , 4.4000000000000004 , 4.0999999999999996 , 4.0 , 4.4000000000000004 , 4.5999999999999996 , 4.0 , 3.2999999999999998 , 4.2000000000000002 , 4.2000000000000002 , 4.2000000000000002 , 4.2999999999999998 , 3.0 , 4.0999999999999996 , 6.0 , 5.0999999999999996 , 5.9000000000000004 , 5.5999999999999996 , 5.7999999999999998 , 6.5999999999999996 , 4.5 , 6.2999999999999998 , 5.7999999999999998 , 6.0999999999999996 , 5.0999999999999996 , 5.2999999999999998 , 5.5 , 5.0 , 5.0999999999999996 , 5.2999999999999998 , 5.5 , 6.7000000000000002 , 6.9000000000000004 , 5.0 , 5.7000000000000002 , 4.9000000000000004 , 6.7000000000000002 , 4.9000000000000004 , 5.7000000000000002 , 6.0 , 4.7999999999999998 , 4.9000000000000004 , 5.5999999999999996 , 5.7999999999999998 , 6.0999999999999996 , 6.4000000000000004 , 5.5999999999999996 , 5.0999999999999996 , 5.5999999999999996 , 6.0999999999999996 , 5.5999999999999996 , 5.5 , 4.7999999999999998 , 5.4000000000000004 , 5.5999999999999996 , 5.0999999999999996 , 5.0999999999999996 , 5.9000000000000004 , 5.7000000000000002 , 5.2000000000000002 , 5.0 , 5.2000000000000002 , 5.4000000000000004 , 5.0999999999999996 ]} df = pd . DataFrame ( raw_data , columns = [ 'Unnamed: 0' , 'Sepal.Length' , 'Sepal.Width' , 'Petal.Length' , 'Petal.Width' , 'Species' ]) 2. Compare the original and recreated dataframes # Look at the top few rows of the original dataframe df . head () Unnamed: 0 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 1 5.1 3.5 1.4 0.2 setosa 1 2 4.9 3.0 1.4 0.2 setosa 2 3 4.7 3.2 1.3 0.2 setosa 3 4 4.6 3.1 1.5 0.2 setosa 4 5 5.0 3.6 1.4 0.2 setosa # Look at the top few rows of the dataframe created with our code df_original . head () Unnamed: 0 Sepal.Length Sepal.Width Petal.Length Petal.Width Species 0 1 5.1 3.5 1.4 0.2 setosa 1 2 4.9 3.0 1.4 0.2 setosa 2 3 4.7 3.2 1.3 0.2 setosa 3 4 4.6 3.1 1.5 0.2 setosa 4 5 5.0 3.6 1.4 0.2 setosa","tags":"Python","url":"http://chrisalbon.com/python/csv_to_python_code.html"},{"title":"Cutting Up Data","loc":"http://chrisalbon.com/r-stats/cutting-up-data.html","text":"Original source: http://rforpublichealth.blogspot.com # create some simulated data ID <- 1 : 10 Age <- c ( 26 , 65 , 15 , 7 , 88 , 43 , 28 , 66 , 45 , 12 ) Sex <- c ( 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ) Weight <- c ( 132 , 122 , 184 , 145 , 118 , NA , 128 , 154 , 166 , 164 ) Height <- c ( 60 , 63 , 57 , 59 , 64 , NA , 67 , 65 , NA , 60 ) Married <- c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ) # create a dataframe of the simulated data mydata <- data.frame ( ID , Age , Sex , Weight , Height , Married ) # cut up Age by 0-5, 6-10, 11-15, 16-20 and label each category 1-6. mydata $ Agecat1 <- cut ( mydata $ Age , c ( 0 , 5 , 10 , 15 , 20 , 25 , 30 ), labels = c ( 1 : 6 )) # it is a factor class ( mydata $ Agecat1 ) [1] \"factor\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/cutting-up-data.html"},{"title":"Data Types","loc":"http://chrisalbon.com/r-stats/data-types.html","text":"Vectors Vectors could best be described as rows of information. ### Create a row containing five values x <- c ( 1 , 2 , 3 , 4 , 5 ) Factors Factors are best thought about as categories, and each individual category is represented by a name and a numerical identifying value. ### Create a factor that represents a variable with three categories y <- factor ( c ( \"yes\" , \"no\" , \"maybe\" )) Matrix A matrix is a great of numbers, but no text strings ### Create a matrix of some data k <- matrix ( data = 1 : 15 , nrow = 5 , ncol = 5 , byrow = T ) Arrays An array is like a matrix, but in 3D (like a time series) ### Create an array z <- array ( 1 : 27 , dim = c ( 3 , 3 , 3 )) Lists A list is a list of R objects, any objects ### Create variables of three different data types f <- c ( 1 : 4 ) j <- FALSE u <- matrix ( c ( 1 : 4 ), nrow = 2 , ncol = 2 ) ### Combine them into a list m <- list ( f , j , u ) Dataframes Data frames are like excel spreadsheets, all types of data in a big grid s <- 1 : 5 d <- c ( T , T , F , F , T ) df <- data.frame ( s , d )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/data-types.html"},{"title":"My Data Science Reading List","loc":"http://chrisalbon.com/articles/data_science_reading_list.html","text":"Everyone has hobbies. My hobby is in a big red bookcase. Currently my big red bookcase contains around ~200 dead-tree books on statistics, data science, research, and mathematics. I rarely travel anywhere without at least one book from this bookcase and on a lazy Sunday afternoon you will likely see to working my way through one of it's members. Mathematics Basics Basic Math for Social Scientists: Concepts (Quantitative Applications in the Social Sciences) Mathematics for the Nonmathematician (Dover Books on Mathematics) A Mathematics Course for Political and Social Research Introduction to Mathematical Thinking Basic Math for Social Scientists: Problems and Solutions (Quantitative Applications in the Social Sciences) The Language of Mathematics: Making the Invisible Visible The Joy of x: A Guided Tour of Math, from One to Infinity Here's Looking at Euclid: A Surprising Excursion Through the Astonishing World of Math Good Math: A Geek's Guide to the Beauty of Numbers, Logic, and Computation (Pragmatic Programmers) Mathematics 1001: Absolutely Everything That Matters in Mathematics. Essential Mathematics for Political and Social Research (Analytical Methods for Social Research) Doing Math with Python: Use Programming to Explore Algebra, Statistics, Calculus, and More! Mathematics and Python Programming Algebra Linear Algebra (Dover Books on Mathematics) Basic Algebra I: Second Edition (Dover Books on Mathematics) Basic Algebra II: Second Edition (Dover Books on Mathematics) Introduction to Linear Algebra and Differential Equations (Dover Books on Mathematics) Coding the Matrix: Linear Algebra through Applications to Computer Science Calculus Quick Calculus: A Self-Teaching Guide, 2nd Edition Calculus: An Intuitive and Physical Approach (Second Edition) (Dover Books on Mathematics) Essential Calculus with Applications (Dover Books on Mathematics) Introduction to Partial Differential Equations with Applications (Dover Books on Mathematics) Partial Differential Equations for Scientists and Engineers (Dover Books on Mathematics) Ordinary Differential Equations (Dover Books on Mathematics) Probability Introduction to Probability (Chapman & Hall/CRC Texts in Statistical Science) Probability Theory: A Concise Course (Dover Books on Mathematics) Other Geometry: A Comprehensive Course (Dover Books on Mathematics) Introduction to Stochastic Processes (Dover Books on Mathematics) Traditional Statistics Basic Statistics Statistics Discovering Statistics Using R Statistics: The Art and Science of Learning from Data (3rd Edition) Principles of Statistics (Dover Books on Mathematics) All of Statistics: A Concise Course in Statistical Inference (Springer Texts in Statistics) Mathematical Methods in Statistics a Workbook Fundamental Statistics for the Behavioral Sciences Statistics Made Simple Using Basic Statistics in the Behavioral Sciences Statistical Rules of Thumb Analysis of Longitudinal Data (Oxford Statistical Science) Practical Longitudinal Data Analysis (Chapman & Hall/CRC Texts in Statistical Science) The SAGE Handbook of Regression Analysis and Causal Inference Statistical Modeling and Inference for Social Science (Analytical Methods for Social Research) Statistical Models and Causal Inference: A Dialogue with the Social Sciences Statistics Done Wrong: The Woefully Complete Guide Even You Can Learn Statistics: A Guide for Everyone Who Has Ever Been Afraid of Statistics Practical Statistics Simply Explained (Dover Books on Mathematics) Exploratory Data Analysis Exploratory Data Analysis (Quantitative Applications in the Social Sciences) Fundamentals of Exploratory Analysis of Variance Practical Longitudinal Data Analysis (Chapman & Hall/CRC Texts in Statistical Science) Exploratory Data Analysis Fundamentals of Exploratory Analysis of Variance Understanding Robust and Exploratory Data Analysis Regression Multiple Regression in Practice (Quantitative Applications in the Social Sciences) Understanding Regression Assumptions (Quantitative Applications in the Social Sciences) Understanding Regression Analysis: An Introductory Guide (Quantitative Applications in the Social Sciences) Applied Regression: An Introduction (Quantitative Applications in the Social Sciences) Multiple Regression in Practice (Quantitative Applications in the Social Sciences) Tests of Significance (Quantitative Applications in the Social Sciences) Introduction to Multivariate Analysis (Chapman & Hall/CRC Texts in Statistical Science) Multivariate Statistical Analysis: A Conceptual Introduction, 2nd Edition What is a p-value anyway? 34 Stories to Help You Actually Understand Statistics Understanding Regression Assumptions (Quantitative Applications in the Social Sciences) Regression Analysis by Example Applied Logistic Regression (Wiley Series in Probability and Statistics) Statistics for the Social Sciences Introduction to Linear Regression Analysis Basic statistics: Tales of distributions Analysis of Covariance (Quantitative Applications in the Social Sciences) Interpreting and Using Regression (Quantitative Applications in the Social Sciences, No. 29) Data Analysis and Regression: A Second Course in Statistics Understanding Significance Testing (Quantitative Applications in the Social Sciences) Loglinear Models with Latent Variables (Quantitative Applications in the Social Sciences) Statistical Models: Theory and Practice Permutation, Parametric, and Bootstrap Tests of Hypotheses (Springer Series in Statistics) Advanced Regression Interpreting Probability Models: Logit, Probit, and Other Generalized Linear Models (Quantitative Applications in the Social Sciences) Regression Models for Categorical and Limited Dependent Variables (Advanced Quantitative Techniques in the Social Sciences) Analysis of Ordinal Data (Quantitative Applications in the Social Sciences) Analysis of Nominal Data (Quantitative Applications in the Social Sciences) Ordinal Log-Linear Models (Quantitative Applications in the Social Sciences) Analysis of Ordinal Categorical Data (Wiley Series in Probability and Statistics) An Introduction to Categorical Data Analysis (Wiley Series in Probability and Statistics) Regression with Dummy Variables (Quantitative Applications in the Social Sciences) An Introduction to Generalized Linear Models, Third Edition (Chapman & Hall/CRC Texts in Statistical Science) Modeling Count Data A Primer on Linear Models (Chapman & Hall/CRC Texts in Statistical Science) Data Analysis Using Regression and Multilevel/Hierarchical Models Statistical Methods for Categorical Data Analysis, 2nd Edition Data Analysis Using Regression and Multilevel/Hierarchical Models Non-Parametric Nonparametric Statistics: An Introduction (Quantitative Applications in the Social Sciences) Time Series The Analysis of Time Series: An Introduction, Sixth Edition (Chapman & Hall/CRC Texts in Statistical Science) Time Series Analysis Interrupted Time Series Analysis (Quantitative Applications in the Social Sciences) Time Series Analysis for the Social Sciences (Analytical Methods for Social Research) Event History Event History Modeling: A Guide for Social Scientists (Analytical Methods for Social Research) Survival Analysis Modelling Survival Data in Medical Research, Second Edition Other The Life and Times of the Central Limit Theorem (History of Mathematics) Introduction to Factor Analysis: What It Is and How To Do It (Quantitative Applications in the Social Sciences) Statistics on the Table: The History of Statistical Concepts and Methods Understanding The New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis (Multivariate Applications Series) Central Tendency and Variability (Quantitative Applications in the Social Sciences) Probability, Statistics and Truth (Dover Books on Mathematics) Applied Missing Data Analysis (Methodology in the Social Sciences) Thinking Statistically Heteroskedasticity in Regression: Detection and Correction (Quantitative Applications in the Social Sciences) Statistical Analysis with Missing Data The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives (Economics, Cognition, and Society) Statistics As Principled Argument The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics) Probability, Statistics and Truth (Dover Books on Mathematics) Statistics, Data Mining, and Machine Learning in Astronomy: A Practical Python Guide for the Analysis of Survey Data Statistical Distributions Mathematical Methods of Statistics Bayesian Statistics Bayes' Rule: A Tutorial Introduction to Bayesian Analysis Data Analysis: A Bayesian Tutorial Introduction to Bayesian Statistics, 2nd Edition Bayesian Data Analysis, Second Edition (Chapman & Hall/CRC Texts in Statistical Science) Bayesian Statistics for the Social Sciences (Methodology in the Social Sciences) Bayesian Programming (Chapman & Hall/CRC Machine Learning & Pattern Recognition) Bayesian Reasoning and Machine Learning Bayesian Methods for Hackers: Probabilistic Programming and Bayesian Methods (Addison-Wesley Data & Analytics Series) Bayesian Methods in Health Economics (Chapman & Hall/CRC Biostatistics Series) Bayesian and Frequentist Regression Methods (Springer Series in Statistics) Machine Learning Machine Learning in Action Learning From Data Machine Learning: An Algorithmic Perspective, Second Edition (Chapman & Hall/Crc Machine Learning & Pattern Recognition) Pattern Recognition and Machine Learning 1st Edition Python Machine Learning An Introduction to Machine Learning Boosting: Foundations and Algorithms (Adaptive Computation and Machine Learning series) Machine Learning in Python: Essential Techniques for Predictive Analysis Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series) Introduction to Machine Learning (Adaptive Computation and Machine Learning series) Algorithims Introduction to Algorithms, 3rd Edition Data Structures and Algorithms with Python (Undergraduate Topics in Computer Science) Python Algorithms: Mastering Basic Algorithms in the Python Language Annotated Algorithms in Python: with Applications in Physics, Biology, and Finance Data Structure and Algorithmic Thinking with Python: Data Structure and Algorithmic Puzzles Other Data Analysis for Politics and Policy Missing Data (Quantitative Applications in the Social Sciences) A Handbook of Small Data Sets (Chapman & Hall Statistics Texts) The Statistical Analysis of Experimental Data (Dover Books on Mathematics) The Chicago Guide to Writing about Multivariate Analysis (Chicago Guides to Writing, Editing, and Publishing) Visual Explanations: Images and Quantities, Evidence and Narrative Visual Storytelling with D3: An Introduction to Data Visualization in JavaScript (Addison-Wesley Data & Analytics Series) Cluster Analysis (Quantitative Applications in the Social Sciences) Clustering Best Practices in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data The Visual Display of Quantitative Information Matched Sampling for Causal Effects Beautiful Data: The Stories Behind Elegant Data Solutions Sampling Techniques, 3rd Edition Counterfactuals and Causal Inference: Methods and Principles for Social Research (Analytical Methods for Social Research) Introduction to Analysis (Dover Books on Mathematics) An Introduction to Information Theory: Symbols, Signals and Noise (Dover Books on Mathematics) Monte Carlo Simulation and Resampling Methods for Social Science Neural Networks for Pattern Recognition (Advanced Texts in Econometrics) Causality: Models, Reasoning and Inference Natural Experiments in the Social Sciences: A Design-Based Approach (Strategies for Social Inquiry) Game Theory: Concepts and Applications (Quantitative Applications in the Social Sciences) Web Scraping with Python: A Comprehensive Guide to Data Collection Solutions Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications Running Randomized Evaluations: A Practical Guide Python for Data Science For Dummies Counterfactuals and Causal Inference: Methods and Principles for Social Research (Analytical Methods for Social Research) A Practical Introduction to Index Numbers Sampling Data Mining for the Social Sciences: An Introduction Beautiful Data: A History of Vision and Reason since 1945 (Experimental Futures) Designing Social Inquiry: Scientific Inference in Qualitative Research Using Propensity Scores in Quasi-Experimental Designs Propensity Score Analysis: Statistical Methods and Applications (Advanced Quantitative Techniques in the Social Sciences)","tags":"Articles","url":"http://chrisalbon.com/articles/data_science_reading_list.html"},{"title":"Data Structure Basics","loc":"http://chrisalbon.com/python/data_structure_basics.html","text":"There are four: lists, Tuples, dictionaries, and sets Lists \"A list is a data structure that holds an ordered collection of items i.e. you can store a sequence of items in a list.\" - A Byte Of Python Lists are mutable. # Create a list of countries, then print the results allies = [ 'USA' , 'UK' , 'France' , 'New Zealand' , 'Australia' , 'Canada' , 'Poland' ]; allies ['USA', 'UK', 'France', 'New Zealand', 'Australia', 'Canada', 'Poland'] # Print the length of the list len ( allies ) 7 # Add an item to the list, then print the results allies . append ( 'China' ); allies ['USA', 'UK', 'France', 'New Zealand', 'Australia', 'Canada', 'Poland', 'China'] # Sort list, then print the results allies . sort (); allies ['Australia', 'Canada', 'China', 'France', 'New Zealand', 'Poland', 'UK', 'USA'] # Reverse sort list, then print the results allies . sort (); allies ['Australia', 'Canada', 'China', 'France', 'New Zealand', 'Poland', 'UK', 'USA'] # View the first item of the list allies [ 0 ] 'Australia' # View the last item of the list allies [ - 1 ] 'USA' # Delete the item in the list del allies [ 0 ]; allies ['Canada', 'China', 'France', 'New Zealand', 'Poland', 'UK', 'USA'] # Add a numeric value to a list of strings allies . append ( 3442 ); allies ['Canada', 'China', 'France', 'New Zealand', 'Poland', 'UK', 'USA', 3442] Tuples \"Though tuples may seem similar to lists, they are often used in different situations and for different purposes. Tuples are immutable, and usually contain an heterogeneous sequence of elements that are accessed via unpacking (or indexing (or even by attribute in the case of namedtuples). Lists are mutable, and their elements are usually homogeneous and are accessed by iterating over the list.\" - Python Documentation \"Tuples are heterogeneous data structures (i.e., their entries have different meanings), while lists are homogeneous sequences.\" - StackOverflow Parentheses are optional, but useful. # Create a tuple of state names usa = ( 'Texas' , 'California' , 'Maryland' ); usa ('Texas', 'California', 'Maryland') # Create a tuple of countries # (notice the USA has a state names in the nested tuple) countries = ( 'canada' , 'mexico' , usa ); countries ('canada', 'mexico', ('Texas', 'California', 'Maryland')) # View the third item of the top tuple countries [ 2 ] ('Texas', 'California', 'Maryland') # View the third item of the third tuple countries [ 2 ][ 2 ] 'Maryland' Dictionaries \"A dictionary is like an address-book where you can find the address or contact details of a person by knowing only his/her name i.e. we associate keys (name) with values (details). Note that the key must be unique just like you cannot find out the correct information if you have two persons with the exact same name.\" - A Byte Of Python # Create a dictionary with key:value combos staff = { 'Chris' : 'chris@stater.org' , 'Jake' : 'jake@stater.org' , 'Ashley' : 'ashley@stater.org' , 'Shelly' : 'shelly@stater.org' } # Print the value using the key staff [ 'Chris' ] 'chris@stater.org' # Delete a dictionary entry based on the key del staff [ 'Chris' ]; staff {'Jake': 'jake@stater.org', 'Ashley': 'ashley@stater.org', 'Shelly': 'shelly@stater.org'} # Add an item to the dictionary staff [ 'Guido' ] = 'guido@python.org' ; staff {'Jake': 'jake@stater.org', 'Ashley': 'ashley@stater.org', 'Guido': 'guido@python.org', 'Shelly': 'shelly@stater.org'} Sets Sets are unordered collections of simple objects. # Create a set of BRI countries BRI = set ([ 'brazil' , 'russia' , 'india' ]) # Is India in the set BRI? 'india' in BRI True # Is the US in the set BRI? 'usa' in BRI False # Create a copy of BRI called BRIC BRIC = BRI . copy () # Add China to BRIC BRIC . add ( 'china' ) # Is BRIC a super-set of BRI? BRIC . issuperset ( BRI ) True # Remove Russia from BRI BRI . remove ( 'russia' ) # What items are the union of BRI and BRIC? BRI & BRIC {'brazil', 'india'}","tags":"Python","url":"http://chrisalbon.com/python/data_structure_basics.html"},{"title":"Data Frames","loc":"http://chrisalbon.com/r-stats/dataframes.html","text":"# Create two variables of 50 observations, note that we only use 10 month names, because to be combined into a dataset all variables must have the same number of lengths OR be a multiple of the longest length. percent.sms <- runif ( 50 ) state <- state.name month <- month.name [ 1 : 10 ] # Create a dataframe of those two variables usa <- data.frame ( state , percent.sms , month ) # Find the number of columns in the data frame length ( usa ) [1] 3 # Select the second and third rows of the first two columns usa [ 2 : 3 , -3 ] state percent.sms 2 Alaska 0.5466144 3 Arizona 0.3224463 # Select the second and third rows of the first column usa [[ 1 ]][ 2 : 3 ] [1] Alaska Arizona 50 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming # Select the second and third rows of the first column usa $ state [ 2 : 3 ] [1] Alaska Arizona 50 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Wyoming # Transpose the data frame usa.t <- t ( usa ) usa.t [,1] [,2] [,3] [,4] state \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" percent.sms \"0.289356397\" \"0.546614370\" \"0.322446264\" \"0.667867042\" month \"January\" \"February\" \"March\" \"April\" [,5] [,6] [,7] [,8] state \"California\" \"Colorado\" \"Connecticut\" \"Delaware\" percent.sms \"0.030940904\" \"0.515846089\" \"0.993535078\" \"0.054146395\" month \"May\" \"June\" \"July\" \"August\" [,9] [,10] [,11] [,12] state \"Florida\" \"Georgia\" \"Hawaii\" \"Idaho\" percent.sms \"0.713894582\" \"0.006578350\" \"0.005815321\" \"0.422469396\" month \"September\" \"October\" \"January\" \"February\" [,13] [,14] [,15] [,16] state \"Illinois\" \"Indiana\" \"Iowa\" \"Kansas\" percent.sms \"0.613361941\" \"0.584833625\" \"0.574096752\" \"0.561261341\" month \"March\" \"April\" \"May\" \"June\" [,17] [,18] [,19] [,20] state \"Kentucky\" \"Louisiana\" \"Maine\" \"Maryland\" percent.sms \"0.915215752\" \"0.110033265\" \"0.250408646\" \"0.508217647\" month \"July\" \"August\" \"September\" \"October\" [,21] [,22] [,23] [,24] state \"Massachusetts\" \"Michigan\" \"Minnesota\" \"Mississippi\" percent.sms \"0.274783572\" \"0.572157144\" \"0.839305733\" \"0.980407253\" month \"January\" \"February\" \"March\" \"April\" [,25] [,26] [,27] [,28] state \"Missouri\" \"Montana\" \"Nebraska\" \"Nevada\" percent.sms \"0.683278756\" \"0.211364157\" \"0.820996565\" \"0.664138581\" month \"May\" \"June\" \"July\" \"August\" [,29] [,30] [,31] [,32] state \"New Hampshire\" \"New Jersey\" \"New Mexico\" \"New York\" percent.sms \"0.958563818\" \"0.479107255\" \"0.619247351\" \"0.561255713\" month \"September\" \"October\" \"January\" \"February\" [,33] [,34] [,35] [,36] state \"North Carolina\" \"North Dakota\" \"Ohio\" \"Oklahoma\" percent.sms \"0.368069716\" \"0.963833767\" \"0.945773752\" \"0.864754913\" month \"March\" \"April\" \"May\" \"June\" [,37] [,38] [,39] [,40] state \"Oregon\" \"Pennsylvania\" \"Rhode Island\" \"South Carolina\" percent.sms \"0.059036551\" \"0.424518585\" \"0.131782993\" \"0.362164821\" month \"July\" \"August\" \"September\" \"October\" [,41] [,42] [,43] [,44] state \"South Dakota\" \"Tennessee\" \"Texas\" \"Utah\" percent.sms \"0.082731801\" \"0.314073189\" \"0.042029392\" \"0.466627718\" month \"January\" \"February\" \"March\" \"April\" [,45] [,46] [,47] [,48] state \"Vermont\" \"Virginia\" \"Washington\" \"West Virginia\" percent.sms \"0.560203050\" \"0.715840876\" \"0.077936989\" \"0.814920678\" month \"May\" \"June\" \"July\" \"August\" [,49] [,50] state \"Wisconsin\" \"Wyoming\" percent.sms \"0.287602357\" \"0.953510491\" month \"September\" \"October\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/dataframes.html"},{"title":"Datasets","loc":"http://chrisalbon.com/r-stats/datasets.html","text":"# view built-in datasets data () # view all installed datasets from all installed packages data ( package = . packages ( TRUE )) Warning message: In data(package = .packages(TRUE)): datasets have been moved from package 'base' to package 'datasets'Warning message: In data(package = .packages(TRUE)): datasets have been moved from package 'stats' to package 'datasets' # load a dataset \"votes.repub\" from the installed package \"cluster\" data ( \"votes.repub\" , package = \"cluster\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/datasets.html"},{"title":"Difference Between Two Date-Times","loc":"http://chrisalbon.com/r-stats/date-sequences.html","text":"# Create two dates start.time <- as.Date ( \"1970-01-01\" ) end.time <- as.Date ( \"2012-12-21\" ) # create an element for every year between two dates seq ( start.time , end.time , by = \"1 year\" ) [1] \"1970-01-01\" \"1971-01-01\" \"1972-01-01\" \"1973-01-01\" \"1974-01-01\" [6] \"1975-01-01\" \"1976-01-01\" \"1977-01-01\" \"1978-01-01\" \"1979-01-01\" [11] \"1980-01-01\" \"1981-01-01\" \"1982-01-01\" \"1983-01-01\" \"1984-01-01\" [16] \"1985-01-01\" \"1986-01-01\" \"1987-01-01\" \"1988-01-01\" \"1989-01-01\" [21] \"1990-01-01\" \"1991-01-01\" \"1992-01-01\" \"1993-01-01\" \"1994-01-01\" [26] \"1995-01-01\" \"1996-01-01\" \"1997-01-01\" \"1998-01-01\" \"1999-01-01\" [31] \"2000-01-01\" \"2001-01-01\" \"2002-01-01\" \"2003-01-01\" \"2004-01-01\" [36] \"2005-01-01\" \"2006-01-01\" \"2007-01-01\" \"2008-01-01\" \"2009-01-01\" [41] \"2010-01-01\" \"2011-01-01\" \"2012-01-01\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/date-sequences.html"},{"title":"Date And Time Basics","loc":"http://chrisalbon.com/python/date_and_time_basics.html","text":"# Import modules from datetime import datetime from datetime import timedelta # Create a variable with the current time now = datetime . now () now datetime.datetime(2014, 5, 11, 20, 5, 11, 688051) # The current year now . year 2014 # The current month now . month 5 # The current day now . day 11 # The current hour now . hour 20 # The current minute now . minute 5 # The difference between two dates delta = datetime ( 2011 , 1 , 7 ) - datetime ( 2011 , 1 , 6 ) delta datetime.timedelta(1) # The difference days delta . days 1 # The difference seconds delta . seconds 0 # Create a time start = datetime ( 2011 , 1 , 7 ) # Add twelve days to the time start + timedelta ( 12 ) datetime.datetime(2011, 1, 19, 0, 0)","tags":"Python","url":"http://chrisalbon.com/python/date_and_time_basics.html"},{"title":"Demonstrate The Central Limit Theorem","loc":"http://chrisalbon.com/frequentist-statistics/demonstrate_the_central_limit_theorem.html","text":"Preliminaries # Import packages import pandas as pd % matplotlib inline import numpy as np Create Population Data From Non-Normal Distribution # Create an empty dataframe population = pd . DataFrame () # Create an column that is 10000 random numbers drawn from a uniform distribution population [ 'numbers' ] = np . random . uniform ( 0 , 10000 , size = 10000 ) # Plot a histogram of the score data. # This confirms the data is not a normal distribution. population [ 'numbers' ] . hist ( bins = 100 ) <matplotlib.axes._subplots.AxesSubplot at 0x112c72710> View the True Mean Of Population # View the mean of the numbers population [ 'numbers' ] . mean () 4983.824612472138 Take A Sample Mean, Repeat 1000 Times # Create a list sampled_means = [] # For 1000 times, for i in range ( 0 , 1000 ): # Take a random sample of 100 rows from the population, take the mean of those rows, append to sampled_means sampled_means . append ( population . sample ( n = 100 ) . mean () . values [ 0 ]) Plot The Sample Means Of All 100 Samples # Plot a histogram of sampled_means. # It is clearly normally distributed and centered around 5000 pd . Series ( sampled_means ) . hist ( bins = 100 ) <matplotlib.axes._subplots.AxesSubplot at 0x11516e668> This is the critical chart, remember that the population distribution was uniform, however, this distribution is approaching normality. This is the key point to the central limit theory, and the reason we can assume sample means are not bias. View The Mean Sample Mean # View the mean of the sampled_means pd . Series ( sampled_means ) . mean () 4981.465310909289 Compare To True Mean # Subtract Mean Sample Mean From True Population Mean error = population [ 'numbers' ] . mean () - pd . Series ( sampled_means ) . mean () # Print print ( 'The Mean Sample Mean is only %f different the True Population mean!' % error ) The Mean Sample Mean is only 2.359302 different the True Population mean!","tags":"Frequentist Statistics","url":"http://chrisalbon.com/frequentist-statistics/demonstrate_the_central_limit_theorem.html"},{"title":"Density Curve Plot","loc":"http://chrisalbon.com/r-stats/density-plot.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # create the ggplot2 data for the faithful$waiting variable ggplot ( faithful , aes ( x = waiting )) + # add a sensity line geom_line ( stat = \"density\" ) + # zoom out the plot a little bit expand_limits ( y = 0 ) # create the ggplot2 data for three lines of different levels of smoothing ggplot ( faithful , aes ( x = waiting )) + # \"adjust\" determines the level of smoothing, larger the number, the more smoothing geom_line ( stat = \"density\" , adjust = .25 , colour = \"red\" ) + geom_line ( stat = \"density\" ) + geom_line ( stat = \"density\" , adjust = 2 , colour = \"blue\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/density-plot.html"},{"title":"Dictionary Basics","loc":"http://chrisalbon.com/python/dictionary_basics.html","text":"Basics Not sequences, but mappings. That is, stored by key, not relative position. Dictionaries are mutable. Build a dictionary via brackets unef_org = { 'name' : 'UNEF' , 'staff' : 32 , 'url' : 'http://unef.org' } View the variable unef_org {'name': 'UNEF', 'staff': 32, 'url': 'http://unef.org'} Build a dict via keys who_org = {} who_org [ 'name' ] = 'WHO' who_org [ 'staff' ] = '10' who_org [ 'url' ] = 'http://who.org' View the variable who_org {'name': 'WHO', 'staff': '10', 'url': 'http://who.org'} Nesting in dictionaries Build a dictionary via brackets unitas_org = { 'name' : 'UNITAS' , 'staff' : 32 , 'url' : [ 'http://unitas.org' , 'http://unitas.int' ]} View the variable unitas_org {'name': 'UNITAS', 'staff': 32, 'url': ['http://unitas.org', 'http://unitas.int']} Index the nested list Index the second item of the list nested in the url key. unitas_org [ 'url' ][ 1 ] 'http://unitas.int'","tags":"Python","url":"http://chrisalbon.com/python/dictionary_basics.html"},{"title":"Difference Between Two Date-Times","loc":"http://chrisalbon.com/r-stats/difference-between-two-dates.html","text":"# Create two dates the_start_of_time <- as.Date ( \"1970-01-01\" ) the_end_of_time <- as.Date ( \"2012-12-21\" ) # calculate the difference in the two dates difftime ( the_end_of_time , the_start_of_time , units = \"auto\" ) Time difference of 15695 days # calculate the difference in the two dates difftime ( the_end_of_time , the_start_of_time , units = \"days\" ) Time difference of 15695 days # calculate the difference in the two dates difftime ( the_end_of_time , the_start_of_time , units = \"weeks\" ) Time difference of 2242.143 weeks","tags":"R Stats","url":"http://chrisalbon.com/r-stats/difference-between-two-dates.html"},{"title":"Dot Plot (Wilkinson Plot)","loc":"http://chrisalbon.com/r-stats/dot-plot.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # create a subset of countries countries2009 <- subset ( countries , Year == 2009 & healthexp > 2000 ) # create the ggplot2 data p <- ggplot ( countries2009 , aes ( x = infmortality )) # create the dotplot layer p + geom_dotplot ( binwidth = .25 ) + # rescale the y axis and remove tic marks scale_y_continuous ( breaks = NULL ) + # remove the axis labels theme ( axis.title.y = element_blank ())","tags":"R Stats","url":"http://chrisalbon.com/r-stats/dot-plot.html"},{"title":"Drop Columns From Dataframe","loc":"http://chrisalbon.com/r-stats/drop-columns-from-dataframe.html","text":"Original Source: http://stackoverflow.com/questions/4605206/drop-columns-r-data-frame # create a dataframe with three columns df <- data.frame ( x = runif ( 100 ), y = runif ( 100 ), z = runif ( 100 )) # drop columns y and z df <- subset ( df , select = - c ( y , z ) ) # only keep column x (thus dropping y and z) df <- subset ( df , select = c ( x ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/drop-columns-from-dataframe.html"},{"title":"Drop Residual Factors","loc":"http://chrisalbon.com/r-stats/drop-residual-factors.html","text":"create the problem # create a dataframe with a factor variable (state names) df <- data.frame ( states = state.name [ 1 : 5 ], score = seq ( 1 : 5 )) # drop two of the observations subdf <- subset ( df , score <= 3 ) # however, all the factors remain levels ( subdf $ states ) [1] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" \"California\" the solution # run factor() to remake the factors correctly subdf $ states <- factor ( subdf $ states ) # check to see it is correct levels ( subdf $ states ) [1] \"Alabama\" \"Alaska\" \"Arizona\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/drop-residual-factors.html"},{"title":"Element Names","loc":"http://chrisalbon.com/r-stats/element-names.html","text":"Each element in a vector can have a name assigned to it # Create a vector with names for values percent.sms <- c ( high = 94 , low = 23 , 50 ) # List names of a vector names ( percent.sms ) [1] \"high\" \"low\" \"\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/element-names.html"},{"title":"Enumerate A List","loc":"http://chrisalbon.com/python/enumerate_a_list.html","text":"# Create a list of strings data = [ 'One' , 'Two' , 'Three' , 'Four' , 'Five' ] # For each item in the enumerated variable, data for item in enumerate ( data ): # Print the whole enumerated element print ( item ) # Print only the value (not the index number) print ( item [ 1 ]) (0, 'One') One (1, 'Two') Two (2, 'Three') Three (3, 'Four') Four (4, 'Five') Five","tags":"Python","url":"http://chrisalbon.com/python/enumerate_a_list.html"},{"title":"Expand A Table Of Counts Into A Dataframe","loc":"http://chrisalbon.com/r-stats/expand-counts-into-dataframe.html","text":"Original source: The R Book # create a dataframe of simulated values count <- c ( 2 , 3 , 4 , 1 ) sex <- c ( \"male\" , \"female\" , \"male\" , \"female\" ) nationality <- c ( \"USA\" , \"USA\" , \"UK\" , \"UK\" ) data.df <- data.frame ( count , sex , nationality ) rm ( count , sex , nationality ) # apply a function that repeats a row the number of times it appears in data.df$count data.expand <- lapply ( data.df , function ( x ) rep ( x , data.df $ count )) # convert it to a data frame data.expand.df <- as.data.frame ( data.expand ) # remove the no-longer-needed count column data.expand.df <- data.expand.df [, -1 ]; data.expand.df sex nationality 1 male USA 2 male USA 3 female USA 4 female USA 5 female USA 6 male UK 7 male UK 8 male UK 9 male UK 10 female UK","tags":"R Stats","url":"http://chrisalbon.com/r-stats/expand-counts-into-dataframe.html"},{"title":"Exponents","loc":"http://chrisalbon.com/r-stats/exponents.html","text":"R can display numbers with exponents with ease, so don't be confused when you see \"e\" in a number. 3.2e3 # 3200 [1] 3200 3.2e-3 # 0.0032 [1] 0.0032","tags":"R Stats","url":"http://chrisalbon.com/r-stats/exponents.html"},{"title":"Faceting in ggplot2","loc":"http://chrisalbon.com/r-stats/faceting.html","text":"Original source: ggplot2 book Faceting is when we show many little charts, one for each category of a factor # load the ggplot2 library library ( ggplot2 ) # set the seed so we can reproduce the results set.seed ( 1410 ) # create a plot of x = carat and y = count, with a single chart for each category of diamond color qplot ( carat , data = diamonds , facets = color ~ . , geom = \"histogram\" , binwidth = 0.1 , xlim = c ( 0 , 3 ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/faceting.html"},{"title":"Factors","loc":"http://chrisalbon.com/r-stats/factors.html","text":"Factors are a variable type used for categorial data. Whenever a data frame is created with text strings, R treats it as a factor. # Create three variables of 50 observations length. turnout <- runif ( 50 ) state <- state.name outcome <- c ( \"win\" , \"loss\" ) # Create a dataframe of those two variables usa <- data.frame ( state , turnout , outcome ) # Is \"outcome\" a factor? Yes. class ( usa $ outcome ) [1] \"factor\" # View the levels (i.e. category names) of the factor levels ( usa $ outcome ) [1] \"loss\" \"win\" # views the number of these levels (i.e. category IDs) of the factor nlevels ( usa $ outcome ) [1] 2 # change levels of a factor levels ( usa $ outcome ) <- c ( \"victory\" , \"defeat\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/factors.html"},{"title":"File Paths","loc":"http://chrisalbon.com/r-stats/file-paths.html","text":"Paths can be relative to the current working directory # View the working directory getwd () [1] \"/Users/chrisralbon/cra/cra_projects/peripheral_brain/notebooks/rstats\" # Set the working directory to the user's home directory setwd ( \"~\" ) In relative paths, . denotes the currect directory In relative paths, .. denotes the parent directory In relative paths, ~ denotes your home directory # Path.expand converts relative paths to absolutely paths path.expand ( \"~\" ) [1] \"/Users/chrisralbon\" # basename() returns the file name without the path basename ( \"~\" ) [1] \"chrisralbon\" # dirname returns the path without the file name dirname ( \"~\" ) [1] \"/Users\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/file-paths.html"},{"title":"Selecting Items In A List With Filters","loc":"http://chrisalbon.com/python/filter_items_in_list_with_filter.html","text":"# Create an list of items denoting the number of soldiers in each regiment, view the list regimentSize = ( 5345 , 6436 , 3453 , 2352 , 5212 , 6232 , 2124 , 3425 , 1200 , 1000 , 1211 ); regimentSize (5345, 6436, 3453, 2352, 5212, 6232, 2124, 3425, 1200, 1000, 1211) One-line Method This line of code does the same thing as the multiline method below, it is just more compact (but also more complicated to understand. # Create a list called smallRegiments that filters regimentSize to # find all items that fulfill the lambda function (which looks for all items under 2500). smallRegiments = list ( filter (( lambda x : x < 2500 ), regimentSize )); smallRegiments [2352, 2124, 1200, 1000, 1211] Multi-line Method The ease with interpreting what is happening, I've broken down the one-line filter method into multiple steps, one per line of code. This appears below. # Create a lambda function that looks for things under 2500 lessThan2500Filter = lambda x : x < 2500 # Filter regimentSize by the lambda function filter filteredRegiments = filter ( lessThan2500Filter , regimentSize ) # Convert the filter results into a list smallRegiments = list ( filteredRegiments ) [2352, 2124, 1200, 1000, 1211] For Loop Equivalent This for loop does the same as both methods above, except it uses a for loop. Create a for loop that go through each item of a list and finds items under 2500 # Create a variable for the results of the loop to be placed smallRegiments_2 = [] # for each item in regimentSize, for x in regimentSize : # look if the item's value is less than 2500 if x < 2500 : # if true, add that item to smallRegiments_2 smallRegiments_2 . append ( x ) # View the smallRegiment_2 variable smallRegiments_2 [2352, 2124, 1200, 1000, 1211]","tags":"Python","url":"http://chrisalbon.com/python/filter_items_in_list_with_filter.html"},{"title":"Classes","loc":"http://chrisalbon.com/r-stats/find-a-variables-class.html","text":"Classes tell you what type a paticular variable is, whether numeric or logistic. Use the class() function to find out a variable's class # Create a variable called \"crime\", containing numeric data crime <- c ( 1 , 2 , 4 , 5 ) # find the class of the variable \"crime\" class ( crime ) [1] \"numeric\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/find-a-variables-class.html"},{"title":"Find And Replace","loc":"http://chrisalbon.com/r-stats/find-and-replace.html","text":"Original source: http://christophergandrud.blogspot.com/2013/12/three-quick-and-simple-data-cleaning.html # load the DataCombine package library ( DataCombine ) # create a dataframe of simulated values data.df <- data.frame ( cities = c ( \"London, UK\" , \"Oxford, UK\" , \"Berlin, DE\" , \"Hamburg, DE\" , \"Oslo, NO\" ), score = c ( 8 , 0.1 , 3 , 2 , 1 )) # create a dataframe of two vectors, one with the characters to be replaced and the other with what to replace it with replace.values <- data.frame ( short = c ( \"UK\" , \"DE\" ), long = c ( \"England\" , \"Germany\" )) # find and replace the character strings data.longnames.df <- FindReplace ( data = data.df , Var = \"cities\" , replaceData = replace.values , from = \"short\" , to = \"long\" ); data.longnames.df Only exact matches will be replaced. cities score 1 London, UK 8.0 2 Oxford, UK 0.1 3 Berlin, DE 3.0 4 Hamburg, DE 2.0 5 Oslo, NO 1.0","tags":"R Stats","url":"http://chrisalbon.com/r-stats/find-and-replace.html"},{"title":"Finding the location (the index) of the first instance of values in a vector","loc":"http://chrisalbon.com/r-stats/find-index-of-first-values.html","text":"Original source: http://www.unc.edu/~jasperlm/Rsnippets.html # Create a list of wars wars <- c ( \"Spanish Civil War\" , \"Spanish Civil War\" , \"Spanish Civil War\" , \"Spanish Civil War\" , \"WWII\" , \"WWII\" ) # Create a function that finds the index of the first unique value find.firsts <- function ( x ) { match ( unique ( x ), x ) } # Run the function on the list of wars find.firsts ( wars ) [1] 1 5","tags":"R Stats","url":"http://chrisalbon.com/r-stats/find-index-of-first-values.html"},{"title":"Finding the most common element","loc":"http://chrisalbon.com/r-stats/find-most-common-element.html","text":"Original source: http://www.unc.edu/~jasperlm/Rsnippets.html # Create a list of wars wars <- c ( \"Spanish Civil War\" , \"Spanish Civil War\" , \"Spanish Civil War\" , \"Spanish Civil War\" , \"WWII\" , \"WWII\" ) # Create a function that finds the most common element in an object most.common <- function ( x ) { count <- sapply ( unique ( x ), function ( i ) sum ( x == i , na.rm = TRUE )) unique ( x )[ which ( count == max ( count ))] } # Run the function on the list of wars most.common ( wars ) [1] \"Spanish Civil War\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/find-most-common-element.html"},{"title":"apropos functions finds objects and functions","loc":"http://chrisalbon.com/r-stats/find-objects-variables-functions.html","text":"# Create a vector with various kitten names kitten.names <- c ( \"Stacy\" , \"McKiddleton\" , \"Cat Number Two\" ) # Find any object and function with \"kitten\" in the name apropos ( \"kitten\" ) character(0)","tags":"R Stats","url":"http://chrisalbon.com/r-stats/find-objects-variables-functions.html"},{"title":"Find Unique Elements In Objects","loc":"http://chrisalbon.com/r-stats/find-unique-elements.html","text":"# generate fake data x <- c ( 4 , 5 , 6 , 2 , 3 , 6 , 7 , 8 , 4 , 5 , 7 , 8 , 3 , 3 , 6 , 7 , 8 , 3 , 2 , 5 , 7 , 8 , 4 , 2 , 6 , 7 , 3 , 2 , 4 , 6 , 7 ) # find unique elements unique ( x ) [1] 4 5 6 2 3 7 8","tags":"R Stats","url":"http://chrisalbon.com/r-stats/find-unique-elements.html"},{"title":"Flatten Lists Of Lists","loc":"http://chrisalbon.com/python/flatten_list_of_lists.html","text":"# Create a list containing three lists of names list_of_lists = [[ 'Amy' , 'Betty' , 'Cathryn' , 'Dana' ], [ 'Elizabeth' , 'Fay' , 'Gora' ], [ 'Heidi' , 'Jane' , 'Kayley' ]] # For each element in list_of_lists, take each element in the list flattened_list = [ i for row in list_of_lists for i in row ] # View the flattened list flattened_list ['Amy', 'Betty', 'Cathryn', 'Dana', 'Elizabeth', 'Fay', 'Gora', 'Heidi', 'Jane', 'Kayley']","tags":"Python","url":"http://chrisalbon.com/python/flatten_list_of_lists.html"},{"title":"Flow Control","loc":"http://chrisalbon.com/r-stats/flow-control.html","text":"This is the same as conditional statements. If If always needs a logical statement in its brackets. # If TRUE, say so. if ( TRUE ) message ( \"This is true.\" ) This is true. # If FALSE, say so. if ( FALSE ) message ( \"Wrong.\" ) # If the random number generated is over .5, execute what is inside the top brackets, else execute what is inside the bottom brackets if ( runif ( 1 ) > 0.5 ) { message ( \"The number is over .5\" ) } else { message ( \"The number is under .5\" ) } The number is under .5 ELSE IF # Generate a random number between 0 and 1 x <- runif ( 1 ) # Execute different message commands depending on the value of the number. if ( x > 0.25 ) { message ( \"x is greater than 0.25\" ) } else if ( x > 0.5 ) { message ( \"x is greater than 0.50\" ) } else if ( x > 0.75 ) { message ( \"x is greater than 0.75\" ) } else if ( x > 0.95 ) { message ( \"x is greater than 0.95\" ) } x is greater than 0.25 IFELSE On Every Element In A Vector IFELSE is for the flow control of entire vectors # Generate 10 random elements between 0 and 1 y <- runif ( 10 ) # If an element is above .5, mark as \"high\", else, mark as \"low\" ifelse ( y > 0.5 , \"High\" , \"Low\" ) [1] \"Low\" \"Low\" \"High\" \"Low\" \"Low\" \"High\" \"Low\" \"Low\" \"Low\" \"High\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/flow-control.html"},{"title":"For Loops","loc":"http://chrisalbon.com/r-stats/for-loop.html","text":"Original source: the r book # create a dataframe with simulated values x <- runif ( 1000 ) y <- runif ( 1000 ) z <- runif ( 1000 ) a <- runif ( 1000 ) data <- data.frame ( x , y , z , a ) rm ( x , y , z , a ) # create a variable to place the results of the for loop in data.altered <- NULL # for each element in data, square x and put the value in data.altered for ( i in data ) { data.altered <- data $ x &#94; 2 }","tags":"R Stats","url":"http://chrisalbon.com/r-stats/for-loop.html"},{"title":"For Loop","loc":"http://chrisalbon.com/python/for_loops.html","text":"The for loop iterates over each item in a sequences. # One at a time, assign each value of the sequence to i and, for i in [ 432 , 342 , 928 , 920 ]: # multiply i by 10 and store the product in a new variable, x create a new variable, x, x = i * 10 # print the value of x print ( x ) # after the entire sequence has been processes, else : # print this print ( 'All done!' ) 4320 3420 9280 9200 All done!","tags":"Python","url":"http://chrisalbon.com/python/for_loops.html"},{"title":"Frequency Polygon Plot","loc":"http://chrisalbon.com/r-stats/frequency-polygon-plot.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # divide the faithful$waiting data into 10 bins binsize <- diff ( range ( faithful $ waiting )) / 10 # create the ggplot2 data for the faithful$waiting variable ggplot ( faithful , aes ( x = waiting )) + # add a sensity line geom_freqpoly ( binwidth = binsize ) + # zoom out the plot a little bit expand_limits ( y = 0 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/frequency-polygon-plot.html"},{"title":"Function Basics","loc":"http://chrisalbon.com/python/function_basics.html","text":"Based on Byte of Python Create a function called printMax with the paramaters x and y. def printMax ( x , y ): # if a is larger than b if x > y : # then print this print ( x , 'is maximum' ) # if a is equal to b elif x == y : # print this print ( x , 'is equal to' , y ) # otherwise else : # print this print ( y , 'is maximum' ) 4 is maximum Run the function with two arguments printMax ( 3 , 4 ) 4 is maximum Note: By default, variables created within functions are local to the function. But you can create a global function that IS defined outside the function. Create a variable called x x = 50 Create a function called func() def func (): # Create a global variable called x global x # Print this print ( 'x is' , x ) # Set x to 2. x = 2 # Print this print ( 'Changed global x to' , x ) Run the func() function func () x is 2 Changed global x to 2 Print x x 2 Default Argument Values Create a function called say() that displays x with the default value of 1 def say ( x , times = 1 , times2 = 3 ): print ( x * times , x * times2 ) # Run the function say() with the default values say ( '!' ) # Run the function say() with the non-default values of 5 and 10 say ( '!' , 5 , 10 ) ! !!! !!!!! !!!!!!!!!! VarArgs Parameters (i.e. unlimited number of parameters) * denotes that all positonal arguments from that point to next arg are used ** dnotes that all keyword arguments from that point to the next arg are used Create a function called total() with three parameters def total ( initial = 5 , * numbers , ** keywords ): # Create a variable called count that takes it's value from initial count = initial # for each item in numbers for number in numbers : # add count to that number count += number # for each item in keywords for key in keywords : # add count to keyword's value count += keywords [ key ] # return counts return count Run total() 10 is for initial. 1,2,3 are for *numbers. vegetables and fruit is for **keywords. total ( 10 , 1 , 2 , 3 , vegetables = 50 , fruits = 100 ) 166 DocStrings (outputs documentation about a function) Create a function called printMax with the paramaters x and y def printMax ( x , y ): # Create the docstring '''Prints out the maximum of two values''' # if a is larger than b if x > y : # then print this print ( x , 'is maximum' ) # if a is equal to b elif x == y : # print this print ( x , 'is equal to' , y ) # otherwise else : # print this print ( y , 'is maximum' ) Run the function with two arguments printMax ( 3 , 4 ) 4 is maximum View the docstring print ( printMax . __doc__ ) Prints out the maximum of two values","tags":"Python","url":"http://chrisalbon.com/python/function_basics.html"},{"title":"Gantt Charts","loc":"http://chrisalbon.com/r-stats/gantt-chart.html","text":"Original source: http://stackoverflow.com/questions/3550341/gantt-charts-with-r # load the reshape package library ( reshape ) # load the ggplot2 package library ( ggplot2 ) # create some labels tasks <- c ( \"Review literature\" , \"Mung data\" , \"Stats analysis\" , \"Write Report\" ) # create a data frame with some simulated data dfr <- data.frame ( name = factor ( tasks , levels = tasks ), start.date = c ( \"24/08/2010\" , \"01/10/2010\" , \"01/11/2010\" , \"14/02/2011\" ), end.date = c ( \"31/10/2010\" , \"14/12/2010\" , \"28/02/2011\" , \"30/04/2011\" ), is.critical = c ( TRUE , FALSE , FALSE , TRUE ) ) # melt the data so every date, whether start or end, has it's own row mdfr <- melt ( dfr , measure.vars = c ( \"start.date\" , \"end.date\" )) # create a ggplot of mdfr with x value being dates in a certain date format, y value being names, and the color being determined by critical ggplot ( mdfr , aes ( as.Date ( value , \"%d/%m/%Y\" ), name , colour = is.critical )) + # draw the lines geom_line ( size = 6 ) + # add x and y axis xlab ( \"Date\" ) + ylab ( \"Activity\" ) + # make the theme minimal theme_minimal ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/gantt-chart.html"},{"title":"Generating Random Numbers With Numpy","loc":"http://chrisalbon.com/python/generating_random_numbers_with_numpy.html","text":"Import Numpy import numpy as np Generate A Random Number From The Normal Distribution np . random . normal () 0.5661104974399703 Generate Four Random Numbers From The Normal Distribution np . random . normal ( size = 4 ) array([-1.03175853, 1.2867365 , -0.23560103, -1.05225393]) Generate Four Random Numbers From The Uniform Distribution np . random . uniform ( size = 4 ) array([ 0.00193123, 0.51932356, 0.87656884, 0.33684494]) Generate Four Random Integers Between 1 and 100 np . random . randint ( low = 1 , high = 100 , size = 4 ) array([96, 25, 94, 77])","tags":"Python","url":"http://chrisalbon.com/python/generating_random_numbers_with_numpy.html"},{"title":"Geocoding And Reverse Geocoding","loc":"http://chrisalbon.com/python/geocoding_and_reverse_geocoding.html","text":"Geocoding (converting a phyiscal address or location into latitude/longitude) and reverse geocoding (converting a lat/long to a phyiscal address or location) are common tasks when working with geo-data. Python offers a number of packages to make the task incredibly easy. In the tutorial below, I use pygeocoder, a wrapper for Google's geo-API, to both geocode and reverse geocode. Preliminaries First we want to load the packages we will want to use in the script. Specifically, I am loading pygeocoder for its geo-functionality, pandas for it's dataframe structures, and numpy for it's missing value (np.nan) functionality. # Load packages from pygeocoder import Geocoder import pandas as pd import numpy as np Create some simulated geo data Geo-data comes in a wide variety of forms, in this case we have a Python dictionary of five latitude and longitude strings, with each coordinate in a coordinate pair seperated by a comma. # Create a dictionary of raw data data = { 'Site 1' : '31.336968, -109.560959' , 'Site 2' : '31.347745, -108.229963' , 'Site 3' : '32.277621, -107.734724' , 'Site 4' : '31.655494, -106.420484' , 'Site 5' : '30.295053, -104.014528' } While technically unneccesary, because I originally come from R, I am a big fan of dataframes, so let us turn the dictionary of simulated data into a dataframe. # Convert the dictionary into a pandas dataframe df = pd . DataFrame . from_dict ( data , orient = 'index' ) # View the dataframe df 0 Site 5 30.295053, -104.014528 Site 1 31.336968, -109.560959 Site 3 32.277621, -107.734724 Site 4 31.655494, -106.420484 Site 2 31.347745, -108.229963 You can see now that we have a a dataframe with five rows, with each now containing a string of latitude and longitude. Before we can work with the data, we'll need to 1) seperate the strings into latitude and longitude and 2) convert them into floats. The function below does just that. # Create two lists for the loop results to be placed lat = [] lon = [] # For each row in a varible, for row in df [ 0 ]: # Try to, try : # Split the row by comma, convert to float, and append # everything before the comma to lat lat . append ( float ( row . split ( ',' )[ 0 ])) # Split the row by comma, convert to float, and append # everything after the comma to lon lon . append ( float ( row . split ( ',' )[ 1 ])) # But if you get an error except : # append a missing value to lat lat . append ( np . NaN ) # append a missing value to lon lon . append ( np . NaN ) # Create two new columns from lat and lon df [ 'latitude' ] = lat df [ 'longitude' ] = lon Let's take a took a what we have now. # View the dataframe df 0 latitude longitude Site 5 30.295053, -104.014528 30.295053 -104.014528 Site 1 31.336968, -109.560959 31.336968 -109.560959 Site 3 32.277621, -107.734724 32.277621 -107.734724 Site 4 31.655494, -106.420484 31.655494 -106.420484 Site 2 31.347745, -108.229963 31.347745 -108.229963 Awesome. This is exactly what we want to see, one column of floats for latitude and one column of floats for longitude. Reverse Geocoding To reverse geocode, we feed a specific latitude and longitude pair, in this case the first row (indexed as '0') into pygeocoder's reverse_geocoder function. # Convert longitude and latitude to a location results = Geocoder . reverse_geocode ( df [ 'latitude' ][ 0 ], df [ 'longitude' ][ 0 ]) Now we can take can start pulling out the data that we want. # Print the lat/long results . coordinates (30.30077769999999, -104.0129162) # Print the city results . city 'Marfa' # Print the country results . country 'United States' # Print the street address (if applicable) results . street_address # Print the admin1 level results . administrative_area_level_1 'Texas' Geocoding For geocoding, we need to submit a string containing an address or location (such as a city) into the geocode function. However, not all strings are formatted in a way that Google's geo-API can make sense of them. We can text if an input is valid by using the .geocode().valid_address function. # Verify that an address is valid (i.e. in Google's system) Geocoder . geocode ( \"7250 South Tucson Boulevard, Tucson, AZ 85756\" ) . valid_address True Because the output was True, we now know that this is a valid address and thus can print the latitude and longitude coordinates. # Print the lat/long results . coordinates (30.30077769999999, -104.0129162) But even more interesting, once the address is processed by the Google geo API, we can parse it and easily seperate street numbers, street names, etc. # Find the lat/long of a certain address result = Geocoder . geocode ( \"7250 South Tucson Boulevard, Tucson, AZ 85756\" ) # Print the street number result . street_number '7250' # Print the street name result . route 'South Tucson Boulevard' And there you have it. Python makes this entire process easy and inserting it into an analysis only takes a few minutes. Good luck!","tags":"Python","url":"http://chrisalbon.com/python/geocoding_and_reverse_geocoding.html"},{"title":"Basic ggplot2 examples","loc":"http://chrisalbon.com/r-stats/ggplot2-basic-examples.html","text":"Original source: https://github.com/patilv/3graphs/blob/master/ggplot2Page/index.md # make sure the graphic displays in RStudio by turning off external devices dev.cur () dev.off () pdf 2 null device 1 Create some simulated data ## create three factor variables of length 50. FacVar1 = as.factor ( rep ( c ( \"level1\" , \"level2\" ), 25 )) FacVar2 = as.factor ( rep ( c ( \"levelA\" , \"levelB\" , \"levelC\" ), 17 )[ -51 ]) FacVar3 = as.factor ( rep ( c ( \"levelI\" , \"levelII\" , \"levelIII\" , \"levelIV\" ), 13 )[ - c ( 51 : 52 )]) Create four numeric variables frpm different distributions ## normal distribution set.seed ( 123 ) NumVar1 = round ( rnorm ( n = 50 , mean = 1000 , sd = 50 ), digits = 2 ) set.seed ( 123 ) ## uniform distribution NumVar2 = round ( runif ( n = 50 , min = 500 , max = 1500 ), digits = 2 ) ## exponential distribution set.seed ( 123 ) NumVar3 = round ( rexp ( n = 50 , rate = 0.001 )) ## sequence NumVar4 = 2001 : 2050 ## merge all the variables in a dataframe simData = data.frame ( FacVar1 , FacVar2 , FacVar3 , NumVar1 , NumVar2 , NumVar3 , NumVar4 ) ## load required packages library ( ggplot2 ) library ( reshape2 ) One variable plots # create a plot of NumVar1's values and their row number ggplot ( simData , aes ( y = NumVar1 , x = 1 : nrow ( simData ), group = \"NumVar1\" )) + geom_point () + geom_line () + xlab ( \"\" ) Error in nrow(simData): object 'simData' not found # create a histogram of NumVar1 ggplot ( simData , aes ( x = NumVar1 )) + geom_histogram () stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this. # create a density plot of NumVar1 ggplot ( simData , aes ( x = NumVar1 )) + geom_density () # create a boxplot of NumVar1 ggplot ( simData , aes ( x = factor ( \"\" ), y = NumVar1 )) + geom_boxplot () + xlab ( \"\" ) # create a barplot of FacVar3 ggplot ( simData , aes ( x = FacVar3 )) + geom_bar () # create a scatterplot of NumVar1 and NumVar 2 ggplot ( simData , aes ( x = NumVar1 , y = NumVar2 )) + geom_point () # create a crosstab of FacVar2 and FacVar3 to get the Freq data bartabledat = as.data.frame ( table ( simData $ FacVar2 , simData $ FacVar3 )) # create a barplot of Var2's frequency, colored by Var1 ggplot ( bartabledat , aes ( x = Var2 , y = Freq , fill = Var1 )) + geom_bar ( position = \"dodge\" ) # create a stacked barplot of var2's frequency, colored by Var1 ggplot ( bartabledat , aes ( x = Var2 , y = Freq , fill = Var1 )) + geom_bar () # create a proportion table of each combo's propotion of the total bartableprop = as.data.frame ( prop.table ( table ( simData $ FacVar2 , simData $ FacVar3 ), 2 ) * 100 ) # create a bar plot of each combination's proportion of the total ggplot ( bartableprop , aes ( x = Var2 , y = Freq , fill = Var1 )) + geom_bar () # create a scatterplot, with each dot colored by a third variable ggplot ( simData , aes ( x = NumVar1 , y = NumVar2 , color = FacVar1 )) + geom_point () # create a scatterplot with each dot sized by a third variable ggplot ( simData , aes ( x = NumVar1 , y = NumVar2 , size = NumVar3 )) + geom_point () # Turn off external devices dev.off () null device 1","tags":"R Stats","url":"http://chrisalbon.com/r-stats/ggplot2-basic-examples.html"},{"title":"Heatmap For Factors","loc":"http://chrisalbon.com/r-stats/heatmap-for-factors.html","text":"# download the data nba <- read.csv ( \"http://datasets.flowingdata.com/ppg2008.csv\" , sep = \",\" ) # order the data by the PTS variable nba <- nba [ order ( nba $ PTS ),] # name each now by the Name variable row.names ( nba ) <- nba $ Name # drop the first column nba <- nba [, 2 : 20 ] # convert the data to a matrix for the heatmap nba_matrix <- data.matrix ( nba ) # create a heatmap of nba_matrix with the columns colored by the heat.colors nba_heatmap <- heatmap ( nba_matrix , Rowv = NA , Colv = NA , col = heat.colors ( 256 ), scale = \"column\" , margins = c ( 5 , 10 ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/heatmap-for-factors.html"},{"title":"Create High-Res Plots In R","loc":"http://chrisalbon.com/r-stats/high-res-plots.html","text":"## Generate 100,000 normally distributed observations x <- rnorm ( 100000 ) # Create a 1600x1600 png at 300px/in png ( \"100kPoints300dpi.png\" , units = \"px\" , width = 1600 , height = 1600 , res = 300 ) # Plot the 100 points on the png plot ( x , main = \"100,000 points\" , col = adjustcolor ( \"black\" , alpha = 0.2 )) dev.off () quartz_off_screen 3","tags":"R Stats","url":"http://chrisalbon.com/r-stats/high-res-plots.html"},{"title":"Hills And Valley Bar Chart","loc":"http://chrisalbon.com/r-stats/hills-and-valleys-chart.html","text":"# load the ggplot2 package library ( ggplot2 ) # load the gcookbook package library ( gcookbook ) # create a subset of data from the Berkeley data and year after 1900 csub <- subset ( climate , Source == \"Berkeley\" & Year >= 1900 ) # create a logical variable that says if the value is above or below 0 csub $ pos <- csub $ Anomaly10y >= 0 # create a ggplot of Year and Temperature, visualuzed with a bar those filled by csub$pos ggplot ( csub , aes ( x = Year , y = Anomaly10y , fill = pos )) + geom_bar ( stat = \"identity\" , position = \"identity\" ) # same chart above but with more features defeind ggplot ( csub , aes ( x = Year , y = Anomaly10y , fill = pos )) + geom_bar ( stat = \"identity\" , position = \"identity\" , colour = \"black\" , size = 0.25 ) + scale_fill_manual ( values = c ( \"#CCEEFF\" , \"#FFDDDD\" ), guide = FALSE )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/hills-and-valleys-chart.html"},{"title":"Histogram","loc":"http://chrisalbon.com/r-stats/histogram.html","text":"# load the ggplot2 package library ( ggplot2 ) # hisotogram of the mpg variable from the mtcars dataset ggplot ( mtcars , aes ( x = mpg )) + geom_histogram ( binwidth = 4 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/histogram.html"},{"title":"Histograms and Density Plots","loc":"http://chrisalbon.com/r-stats/histograms-and-density-plots.html","text":"Original source: ggplot2 book Histograms and density plots show the distribution of a single variable across it's values # load the ggplot2 library library ( ggplot2 ) # set the seed so we can reproduce the results set.seed ( 1410 ) # plot a histogram qplot ( carat , data = diamonds , geom = \"histogram\" ) stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this. # plot a histogram with more smoothness (by increasing bin size) qplot ( carat , data = diamonds , geom = \"histogram\" , binwidth = 1 ) # plot a density plot qplot ( carat , data = diamonds , geom = \"density\" ) # plot a density plot with a smoother line qplot ( carat , data = diamonds , adjust = 4 , geom = \"density\" ) # plot a density plot with the color of the lines determined by the diamond's color (the \"color\" variable) qplot ( carat , data = diamonds , geom = \"density\" , colour = color ) # plot a histogram plot with the color of the lines determined by the diamond's color (the \"color\" variable) qplot ( carat , data = diamonds , geom = \"histogram\" , fill = color ) stat_bin: binwidth defaulted to range/30. Use 'binwidth = x' to adjust this.","tags":"R Stats","url":"http://chrisalbon.com/r-stats/histograms-and-density-plots.html"},{"title":"if and if else","loc":"http://chrisalbon.com/python/if_and_if_else_statements.html","text":"Create a variable with the status of the conflict. 1 if the conflict is active 0 if the conflict is not active unknown if the status of the conflict is unknwon conflict_active = 1 If the conflict is active print a statement if conflict_active == 1 : print ( 'The conflict is active.' ) The conflict is active. If the conflict is active print a statement, if not, print a different statement if conflict_active == 1 : print ( 'The conflict is active.' ) else : print ( 'The conflict is not active.' ) The conflict is active. If the conflict is active print a statement, if not, print a different statement, if unknown, state a third statement. if conflict_active == 1 : print ( 'The conflict is active.' ) elif conflict_active == 'unknown' : print ( 'The status of the conflict is unknown' ) else : print ( 'The conflict is not active.' ) The conflict is active.","tags":"Python","url":"http://chrisalbon.com/python/if_and_if_else_statements.html"},{"title":"Using ifelse","loc":"http://chrisalbon.com/r-stats/ifelse.html","text":"# create some simulated data ID <- 1 : 10 Age <- c ( 26 , 65 , 15 , 7 , 88 , 43 , 28 , 66 , 45 , 12 ) Sex <- c ( 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ) Weight <- c ( 132 , 122 , 184 , 145 , 118 , NA , 128 , 154 , 166 , 164 ) Height <- c ( 60 , 63 , 57 , 59 , 64 , NA , 67 , 65 , NA , 60 ) Married <- c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ) # create a dataframe of the simulated data mydata <- data.frame ( ID , Age , Sex , Weight , Height , Married ) Simple example # create a new variable called Old that, if Age is greater than 40, is 1, else the value is 0 mydata $ Old <- ifelse ( mydata $ Age > 40 , 1 , 0 ) Nested ifelse() allows us to have more than 1 ruleset # create a new variable called Agegroup2. If Age is greater than 10 and less than 20, Agegroup 2 will have a value of 1. If not, it executes a new ifelse that if Age is over 20, codes the value as 2, else 0. mydata $ Agegroup2 <- ifelse ( mydata $ Age > 10 & mydata $ Age < 20 , 1 , ifelse ( mydata $ Age > 20 , 2 , 0 )) # create a new variable called Height.recode. if Height is missing, then 9999, else, keep Height's original value mydata $ Height.recode <- ifelse ( is.na ( mydata $ Height ), 9999 , mydata $ Height )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/ifelse.html"},{"title":"If Else On Any Or All Elements","loc":"http://chrisalbon.com/python/ifelse_on_any_or_all_elements.html","text":"Preliminaries # import pandas as pd import pandas as pd Create a simulated dataset # Create an example dataframe data = { 'score' : [ 1 , 2 , 3 , 4 , 5 ]} df = pd . DataFrame ( data ) df score 0 1 1 2 2 3 3 4 4 5 Does any cell equal 3? # If any element in df.score equals three, if ( df . score == 3 ) . any (): # Print this print ( 'Does any cells equal 3? Yes!' ) # Otherwise, else : # Print this print ( 'Does any cells equal 3? No!' ) Does any cells equal 3? Yes! Do all cells equal 3? # If all elements in df.score equal three, if ( df . score == 3 ) . all (): # Print this print ( 'Do all cells equal 3? Yes!' ) # Otherwise else : # Print this print ( 'Do all cells equal 3? No!' ) Do all cells equal 3? No!","tags":"Python","url":"http://chrisalbon.com/python/ifelse_on_any_or_all_elements.html"},{"title":"Importing Baltimore's 2010 Monthly Crime Data","loc":"http://chrisalbon.com/r-stats/import-data.html","text":"# create an object called bmore.crime from the csv file called Crime_2010_Count_By_District.csv # bmore.crime <- read.csv(\"~/cra/cra_projects/code_peoples-data/The People's Data/data_files/Crime_2010_Count_By_District.csv\")","tags":"R Stats","url":"http://chrisalbon.com/r-stats/import-data.html"},{"title":"Importing a JSON file into R","loc":"http://chrisalbon.com/r-stats/import-json.html","text":"Source: http://stackoverflow.com/questions/13004813/downloading-json-data-into-r?lq=1 # Load libraries library ( RJSONIO ) library ( RCurl ) # grab the data raw_data <- getURL ( \"http://swapi.co/api/people/1/?format=json\" ) # Then covert from JSON into a list in R data <- fromJSON ( raw_data ) length ( data ) # We can coerce this to a data.frame final_data <- do.call ( rbind , data ) # Then write it to a flat csv file # write.csv(final_data, \"final_data.csv\") [1] 16 final_data [,1] name \"Luke Skywalker\" height \"172\" mass \"77\" hair_color \"blond\" skin_color \"fair\" eye_color \"blue\" birth_year \"19BBY\" gender \"male\" homeworld \"http://swapi.co/api/planets/1/\" films \"http://swapi.co/api/films/1/\" species \"http://swapi.co/api/species/1/\" vehicles \"http://swapi.co/api/vehicles/14/\" starships \"http://swapi.co/api/starships/12/\" created \"2014-12-09T13:50:51.644000Z\" edited \"2014-12-20T21:17:56.891000Z\" url \"http://swapi.co/api/people/1/\" [,2] name \"Luke Skywalker\" height \"172\" mass \"77\" hair_color \"blond\" skin_color \"fair\" eye_color \"blue\" birth_year \"19BBY\" gender \"male\" homeworld \"http://swapi.co/api/planets/1/\" films \"http://swapi.co/api/films/2/\" species \"http://swapi.co/api/species/1/\" vehicles \"http://swapi.co/api/vehicles/30/\" starships \"http://swapi.co/api/starships/22/\" created \"2014-12-09T13:50:51.644000Z\" edited \"2014-12-20T21:17:56.891000Z\" url \"http://swapi.co/api/people/1/\" [,3] name \"Luke Skywalker\" height \"172\" mass \"77\" hair_color \"blond\" skin_color \"fair\" eye_color \"blue\" birth_year \"19BBY\" gender \"male\" homeworld \"http://swapi.co/api/planets/1/\" films \"http://swapi.co/api/films/3/\" species \"http://swapi.co/api/species/1/\" vehicles \"http://swapi.co/api/vehicles/14/\" starships \"http://swapi.co/api/starships/12/\" created \"2014-12-09T13:50:51.644000Z\" edited \"2014-12-20T21:17:56.891000Z\" url \"http://swapi.co/api/people/1/\" [,4] name \"Luke Skywalker\" height \"172\" mass \"77\" hair_color \"blond\" skin_color \"fair\" eye_color \"blue\" birth_year \"19BBY\" gender \"male\" homeworld \"http://swapi.co/api/planets/1/\" films \"http://swapi.co/api/films/6/\" species \"http://swapi.co/api/species/1/\" vehicles \"http://swapi.co/api/vehicles/30/\" starships \"http://swapi.co/api/starships/22/\" created \"2014-12-09T13:50:51.644000Z\" edited \"2014-12-20T21:17:56.891000Z\" url \"http://swapi.co/api/people/1/\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/import-json.html"},{"title":"Import XML","loc":"http://chrisalbon.com/r-stats/import-xml.html","text":"Original source: http://giventhedata.blogspot.com/2012/06/r-and-web-for-beginners-part-ii-xml-in.html # load XML library library ( XML ) # save the URL as a string element xml.url <- \"http://www.w3schools.com/xml/plant_catalog.xml\" # parse the xml file from the web xmlfile <- xmlTreeParse ( xml.url ); xmlfile $doc $file [1] \"http://www.w3schools.com/xml/plant_catalog.xml\" $version [1] \"1.0\" $children $children$CATALOG <CATALOG> <PLANT> <COMMON>Bloodroot</COMMON> <BOTANICAL>Sanguinaria canadensis</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$2.44</PRICE> <AVAILABILITY>031599</AVAILABILITY> </PLANT> <PLANT> <COMMON>Columbine</COMMON> <BOTANICAL>Aquilegia canadensis</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.37</PRICE> <AVAILABILITY>030699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Marsh Marigold</COMMON> <BOTANICAL>Caltha palustris</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Sunny</LIGHT> <PRICE>$6.81</PRICE> <AVAILABILITY>051799</AVAILABILITY> </PLANT> <PLANT> <COMMON>Cowslip</COMMON> <BOTANICAL>Caltha palustris</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.90</PRICE> <AVAILABILITY>030699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Dutchman&apos;s-Breeches</COMMON> <BOTANICAL>Dicentra cucullaria</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$6.44</PRICE> <AVAILABILITY>012099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Ginger, Wild</COMMON> <BOTANICAL>Asarum canadense</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.03</PRICE> <AVAILABILITY>041899</AVAILABILITY> </PLANT> <PLANT> <COMMON>Hepatica</COMMON> <BOTANICAL>Hepatica americana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$4.45</PRICE> <AVAILABILITY>012699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Liverleaf</COMMON> <BOTANICAL>Hepatica americana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$3.99</PRICE> <AVAILABILITY>010299</AVAILABILITY> </PLANT> <PLANT> <COMMON>Jack-In-The-Pulpit</COMMON> <BOTANICAL>Arisaema triphyllum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$3.23</PRICE> <AVAILABILITY>020199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Mayapple</COMMON> <BOTANICAL>Podophyllum peltatum</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$2.98</PRICE> <AVAILABILITY>060599</AVAILABILITY> </PLANT> <PLANT> <COMMON>Phlox, Woodland</COMMON> <BOTANICAL>Phlox divaricata</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$2.80</PRICE> <AVAILABILITY>012299</AVAILABILITY> </PLANT> <PLANT> <COMMON>Phlox, Blue</COMMON> <BOTANICAL>Phlox divaricata</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$5.59</PRICE> <AVAILABILITY>021699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Spring-Beauty</COMMON> <BOTANICAL>Claytonia Virginica</BOTANICAL> <ZONE>7</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$6.59</PRICE> <AVAILABILITY>020199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Trillium</COMMON> <BOTANICAL>Trillium grandiflorum</BOTANICAL> <ZONE>5</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$3.90</PRICE> <AVAILABILITY>042999</AVAILABILITY> </PLANT> <PLANT> <COMMON>Wake Robin</COMMON> <BOTANICAL>Trillium grandiflorum</BOTANICAL> <ZONE>5</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$3.20</PRICE> <AVAILABILITY>022199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Violet, Dog-Tooth</COMMON> <BOTANICAL>Erythronium americanum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.04</PRICE> <AVAILABILITY>020199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Trout Lily</COMMON> <BOTANICAL>Erythronium americanum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$6.94</PRICE> <AVAILABILITY>032499</AVAILABILITY> </PLANT> <PLANT> <COMMON>Adder&apos;s-Tongue</COMMON> <BOTANICAL>Erythronium americanum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.58</PRICE> <AVAILABILITY>041399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Anemone</COMMON> <BOTANICAL>Anemone blanda</BOTANICAL> <ZONE>6</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$8.86</PRICE> <AVAILABILITY>122698</AVAILABILITY> </PLANT> <PLANT> <COMMON>Grecian Windflower</COMMON> <BOTANICAL>Anemone blanda</BOTANICAL> <ZONE>6</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.16</PRICE> <AVAILABILITY>071099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Bee Balm</COMMON> <BOTANICAL>Monarda didyma</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$4.59</PRICE> <AVAILABILITY>050399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Bergamot</COMMON> <BOTANICAL>Monarda didyma</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$7.16</PRICE> <AVAILABILITY>042799</AVAILABILITY> </PLANT> <PLANT> <COMMON>Black-Eyed Susan</COMMON> <BOTANICAL>Rudbeckia hirta</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Sunny</LIGHT> <PRICE>$9.80</PRICE> <AVAILABILITY>061899</AVAILABILITY> </PLANT> <PLANT> <COMMON>Buttercup</COMMON> <BOTANICAL>Ranunculus</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$2.57</PRICE> <AVAILABILITY>061099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Crowfoot</COMMON> <BOTANICAL>Ranunculus</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.34</PRICE> <AVAILABILITY>040399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Butterfly Weed</COMMON> <BOTANICAL>Asclepias tuberosa</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Sunny</LIGHT> <PRICE>$2.78</PRICE> <AVAILABILITY>063099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Cinquefoil</COMMON> <BOTANICAL>Potentilla</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$7.06</PRICE> <AVAILABILITY>052599</AVAILABILITY> </PLANT> <PLANT> <COMMON>Primrose</COMMON> <BOTANICAL>Oenothera</BOTANICAL> <ZONE>3 - 5</ZONE> <LIGHT>Sunny</LIGHT> <PRICE>$6.56</PRICE> <AVAILABILITY>013099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Gentian</COMMON> <BOTANICAL>Gentiana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$7.81</PRICE> <AVAILABILITY>051899</AVAILABILITY> </PLANT> <PLANT> <COMMON>Blue Gentian</COMMON> <BOTANICAL>Gentiana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$8.56</PRICE> <AVAILABILITY>050299</AVAILABILITY> </PLANT> <PLANT> <COMMON>Jacob&apos;s Ladder</COMMON> <BOTANICAL>Polemonium caeruleum</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.26</PRICE> <AVAILABILITY>022199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Greek Valerian</COMMON> <BOTANICAL>Polemonium caeruleum</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$4.36</PRICE> <AVAILABILITY>071499</AVAILABILITY> </PLANT> <PLANT> <COMMON>California Poppy</COMMON> <BOTANICAL>Eschscholzia californica</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Sun</LIGHT> <PRICE>$7.89</PRICE> <AVAILABILITY>032799</AVAILABILITY> </PLANT> <PLANT> <COMMON>Shooting Star</COMMON> <BOTANICAL>Dodecatheon</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$8.60</PRICE> <AVAILABILITY>051399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Snakeroot</COMMON> <BOTANICAL>Cimicifuga</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$5.63</PRICE> <AVAILABILITY>071199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Cardinal Flower</COMMON> <BOTANICAL>Lobelia cardinalis</BOTANICAL> <ZONE>2</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$3.02</PRICE> <AVAILABILITY>022299</AVAILABILITY> </PLANT> </CATALOG> attr(,\"class\") [1] \"XMLDocumentContent\" $dtd $external NULL $internal NULL attr(,\"class\") [1] \"DTDList\" attr(,\"class\") [1] \"XMLDocument\" \"XMLAbstractDocument\" # access the top node, skipping over DTD and the comment. In other words, get straight to the data and ignore the other stuff. xmltop <- xmlRoot ( xmlfile ); xmltop <CATALOG> <PLANT> <COMMON>Bloodroot</COMMON> <BOTANICAL>Sanguinaria canadensis</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$2.44</PRICE> <AVAILABILITY>031599</AVAILABILITY> </PLANT> <PLANT> <COMMON>Columbine</COMMON> <BOTANICAL>Aquilegia canadensis</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.37</PRICE> <AVAILABILITY>030699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Marsh Marigold</COMMON> <BOTANICAL>Caltha palustris</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Sunny</LIGHT> <PRICE>$6.81</PRICE> <AVAILABILITY>051799</AVAILABILITY> </PLANT> <PLANT> <COMMON>Cowslip</COMMON> <BOTANICAL>Caltha palustris</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.90</PRICE> <AVAILABILITY>030699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Dutchman&apos;s-Breeches</COMMON> <BOTANICAL>Dicentra cucullaria</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$6.44</PRICE> <AVAILABILITY>012099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Ginger, Wild</COMMON> <BOTANICAL>Asarum canadense</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.03</PRICE> <AVAILABILITY>041899</AVAILABILITY> </PLANT> <PLANT> <COMMON>Hepatica</COMMON> <BOTANICAL>Hepatica americana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$4.45</PRICE> <AVAILABILITY>012699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Liverleaf</COMMON> <BOTANICAL>Hepatica americana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$3.99</PRICE> <AVAILABILITY>010299</AVAILABILITY> </PLANT> <PLANT> <COMMON>Jack-In-The-Pulpit</COMMON> <BOTANICAL>Arisaema triphyllum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$3.23</PRICE> <AVAILABILITY>020199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Mayapple</COMMON> <BOTANICAL>Podophyllum peltatum</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$2.98</PRICE> <AVAILABILITY>060599</AVAILABILITY> </PLANT> <PLANT> <COMMON>Phlox, Woodland</COMMON> <BOTANICAL>Phlox divaricata</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$2.80</PRICE> <AVAILABILITY>012299</AVAILABILITY> </PLANT> <PLANT> <COMMON>Phlox, Blue</COMMON> <BOTANICAL>Phlox divaricata</BOTANICAL> <ZONE>3</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$5.59</PRICE> <AVAILABILITY>021699</AVAILABILITY> </PLANT> <PLANT> <COMMON>Spring-Beauty</COMMON> <BOTANICAL>Claytonia Virginica</BOTANICAL> <ZONE>7</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$6.59</PRICE> <AVAILABILITY>020199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Trillium</COMMON> <BOTANICAL>Trillium grandiflorum</BOTANICAL> <ZONE>5</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$3.90</PRICE> <AVAILABILITY>042999</AVAILABILITY> </PLANT> <PLANT> <COMMON>Wake Robin</COMMON> <BOTANICAL>Trillium grandiflorum</BOTANICAL> <ZONE>5</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$3.20</PRICE> <AVAILABILITY>022199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Violet, Dog-Tooth</COMMON> <BOTANICAL>Erythronium americanum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.04</PRICE> <AVAILABILITY>020199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Trout Lily</COMMON> <BOTANICAL>Erythronium americanum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$6.94</PRICE> <AVAILABILITY>032499</AVAILABILITY> </PLANT> <PLANT> <COMMON>Adder&apos;s-Tongue</COMMON> <BOTANICAL>Erythronium americanum</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.58</PRICE> <AVAILABILITY>041399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Anemone</COMMON> <BOTANICAL>Anemone blanda</BOTANICAL> <ZONE>6</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$8.86</PRICE> <AVAILABILITY>122698</AVAILABILITY> </PLANT> <PLANT> <COMMON>Grecian Windflower</COMMON> <BOTANICAL>Anemone blanda</BOTANICAL> <ZONE>6</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$9.16</PRICE> <AVAILABILITY>071099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Bee Balm</COMMON> <BOTANICAL>Monarda didyma</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$4.59</PRICE> <AVAILABILITY>050399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Bergamot</COMMON> <BOTANICAL>Monarda didyma</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$7.16</PRICE> <AVAILABILITY>042799</AVAILABILITY> </PLANT> <PLANT> <COMMON>Black-Eyed Susan</COMMON> <BOTANICAL>Rudbeckia hirta</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Sunny</LIGHT> <PRICE>$9.80</PRICE> <AVAILABILITY>061899</AVAILABILITY> </PLANT> <PLANT> <COMMON>Buttercup</COMMON> <BOTANICAL>Ranunculus</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$2.57</PRICE> <AVAILABILITY>061099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Crowfoot</COMMON> <BOTANICAL>Ranunculus</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.34</PRICE> <AVAILABILITY>040399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Butterfly Weed</COMMON> <BOTANICAL>Asclepias tuberosa</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Sunny</LIGHT> <PRICE>$2.78</PRICE> <AVAILABILITY>063099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Cinquefoil</COMMON> <BOTANICAL>Potentilla</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$7.06</PRICE> <AVAILABILITY>052599</AVAILABILITY> </PLANT> <PLANT> <COMMON>Primrose</COMMON> <BOTANICAL>Oenothera</BOTANICAL> <ZONE>3 - 5</ZONE> <LIGHT>Sunny</LIGHT> <PRICE>$6.56</PRICE> <AVAILABILITY>013099</AVAILABILITY> </PLANT> <PLANT> <COMMON>Gentian</COMMON> <BOTANICAL>Gentiana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$7.81</PRICE> <AVAILABILITY>051899</AVAILABILITY> </PLANT> <PLANT> <COMMON>Blue Gentian</COMMON> <BOTANICAL>Gentiana</BOTANICAL> <ZONE>4</ZONE> <LIGHT>Sun or Shade</LIGHT> <PRICE>$8.56</PRICE> <AVAILABILITY>050299</AVAILABILITY> </PLANT> <PLANT> <COMMON>Jacob&apos;s Ladder</COMMON> <BOTANICAL>Polemonium caeruleum</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$9.26</PRICE> <AVAILABILITY>022199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Greek Valerian</COMMON> <BOTANICAL>Polemonium caeruleum</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$4.36</PRICE> <AVAILABILITY>071499</AVAILABILITY> </PLANT> <PLANT> <COMMON>California Poppy</COMMON> <BOTANICAL>Eschscholzia californica</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Sun</LIGHT> <PRICE>$7.89</PRICE> <AVAILABILITY>032799</AVAILABILITY> </PLANT> <PLANT> <COMMON>Shooting Star</COMMON> <BOTANICAL>Dodecatheon</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Mostly Shady</LIGHT> <PRICE>$8.60</PRICE> <AVAILABILITY>051399</AVAILABILITY> </PLANT> <PLANT> <COMMON>Snakeroot</COMMON> <BOTANICAL>Cimicifuga</BOTANICAL> <ZONE>Annual</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$5.63</PRICE> <AVAILABILITY>071199</AVAILABILITY> </PLANT> <PLANT> <COMMON>Cardinal Flower</COMMON> <BOTANICAL>Lobelia cardinalis</BOTANICAL> <ZONE>2</ZONE> <LIGHT>Shade</LIGHT> <PRICE>$3.02</PRICE> <AVAILABILITY>022299</AVAILABILITY> </PLANT> </CATALOG> # extract XML-values with xmlSApply. More specifically, this turns each $PLANT node into it's own row, each tag into a column, and each value within the tag into an element plantcat <- xmlSApply ( xmltop , function ( x ) xmlSApply ( x , xmlValue )); plantcat PLANT PLANT PLANT COMMON \"Bloodroot\" \"Columbine\" \"Marsh Marigold\" BOTANICAL \"Sanguinaria canadensis\" \"Aquilegia canadensis\" \"Caltha palustris\" ZONE \"4\" \"3\" \"4\" LIGHT \"Mostly Shady\" \"Mostly Shady\" \"Mostly Sunny\" PRICE \"$2.44\" \"$9.37\" \"$6.81\" AVAILABILITY \"031599\" \"030699\" \"051799\" PLANT PLANT PLANT COMMON \"Cowslip\" \"Dutchman's-Breeches\" \"Ginger, Wild\" BOTANICAL \"Caltha palustris\" \"Dicentra cucullaria\" \"Asarum canadense\" ZONE \"4\" \"3\" \"3\" LIGHT \"Mostly Shady\" \"Mostly Shady\" \"Mostly Shady\" PRICE \"$9.90\" \"$6.44\" \"$9.03\" AVAILABILITY \"030699\" \"012099\" \"041899\" PLANT PLANT PLANT COMMON \"Hepatica\" \"Liverleaf\" \"Jack-In-The-Pulpit\" BOTANICAL \"Hepatica americana\" \"Hepatica americana\" \"Arisaema triphyllum\" ZONE \"4\" \"4\" \"4\" LIGHT \"Mostly Shady\" \"Mostly Shady\" \"Mostly Shady\" PRICE \"$4.45\" \"$3.99\" \"$3.23\" AVAILABILITY \"012699\" \"010299\" \"020199\" PLANT PLANT PLANT COMMON \"Mayapple\" \"Phlox, Woodland\" \"Phlox, Blue\" BOTANICAL \"Podophyllum peltatum\" \"Phlox divaricata\" \"Phlox divaricata\" ZONE \"3\" \"3\" \"3\" LIGHT \"Mostly Shady\" \"Sun or Shade\" \"Sun or Shade\" PRICE \"$2.98\" \"$2.80\" \"$5.59\" AVAILABILITY \"060599\" \"012299\" \"021699\" PLANT PLANT COMMON \"Spring-Beauty\" \"Trillium\" BOTANICAL \"Claytonia Virginica\" \"Trillium grandiflorum\" ZONE \"7\" \"5\" LIGHT \"Mostly Shady\" \"Sun or Shade\" PRICE \"$6.59\" \"$3.90\" AVAILABILITY \"020199\" \"042999\" PLANT PLANT COMMON \"Wake Robin\" \"Violet, Dog-Tooth\" BOTANICAL \"Trillium grandiflorum\" \"Erythronium americanum\" ZONE \"5\" \"4\" LIGHT \"Sun or Shade\" \"Shade\" PRICE \"$3.20\" \"$9.04\" AVAILABILITY \"022199\" \"020199\" PLANT PLANT PLANT COMMON \"Trout Lily\" \"Adder's-Tongue\" \"Anemone\" BOTANICAL \"Erythronium americanum\" \"Erythronium americanum\" \"Anemone blanda\" ZONE \"4\" \"4\" \"6\" LIGHT \"Shade\" \"Shade\" \"Mostly Shady\" PRICE \"$6.94\" \"$9.58\" \"$8.86\" AVAILABILITY \"032499\" \"041399\" \"122698\" PLANT PLANT PLANT COMMON \"Grecian Windflower\" \"Bee Balm\" \"Bergamot\" BOTANICAL \"Anemone blanda\" \"Monarda didyma\" \"Monarda didyma\" ZONE \"6\" \"4\" \"4\" LIGHT \"Mostly Shady\" \"Shade\" \"Shade\" PRICE \"$9.16\" \"$4.59\" \"$7.16\" AVAILABILITY \"071099\" \"050399\" \"042799\" PLANT PLANT PLANT PLANT COMMON \"Black-Eyed Susan\" \"Buttercup\" \"Crowfoot\" \"Butterfly Weed\" BOTANICAL \"Rudbeckia hirta\" \"Ranunculus\" \"Ranunculus\" \"Asclepias tuberosa\" ZONE \"Annual\" \"4\" \"4\" \"Annual\" LIGHT \"Sunny\" \"Shade\" \"Shade\" \"Sunny\" PRICE \"$9.80\" \"$2.57\" \"$9.34\" \"$2.78\" AVAILABILITY \"061899\" \"061099\" \"040399\" \"063099\" PLANT PLANT PLANT PLANT COMMON \"Cinquefoil\" \"Primrose\" \"Gentian\" \"Blue Gentian\" BOTANICAL \"Potentilla\" \"Oenothera\" \"Gentiana\" \"Gentiana\" ZONE \"Annual\" \"3 - 5\" \"4\" \"4\" LIGHT \"Shade\" \"Sunny\" \"Sun or Shade\" \"Sun or Shade\" PRICE \"$7.06\" \"$6.56\" \"$7.81\" \"$8.56\" AVAILABILITY \"052599\" \"013099\" \"051899\" \"050299\" PLANT PLANT COMMON \"Jacob's Ladder\" \"Greek Valerian\" BOTANICAL \"Polemonium caeruleum\" \"Polemonium caeruleum\" ZONE \"Annual\" \"Annual\" LIGHT \"Shade\" \"Shade\" PRICE \"$9.26\" \"$4.36\" AVAILABILITY \"022199\" \"071499\" PLANT PLANT PLANT COMMON \"California Poppy\" \"Shooting Star\" \"Snakeroot\" BOTANICAL \"Eschscholzia californica\" \"Dodecatheon\" \"Cimicifuga\" ZONE \"Annual\" \"Annual\" \"Annual\" LIGHT \"Sun\" \"Mostly Shady\" \"Shade\" PRICE \"$7.89\" \"$8.60\" \"$5.63\" AVAILABILITY \"032799\" \"051399\" \"071199\" PLANT COMMON \"Cardinal Flower\" BOTANICAL \"Lobelia cardinalis\" ZONE \"2\" LIGHT \"Shade\" PRICE \"$3.02\" AVAILABILITY \"022299\" # get the data in a data-frame and have a look at the first rows and columns. The t() function transposes the plantcat variable plantcat_df <- data.frame ( t ( plantcat ), row.names = NULL ) head ( plantcat_df ) COMMON BOTANICAL ZONE LIGHT PRICE 1 Bloodroot Sanguinaria canadensis 4 Mostly Shady $2.44 2 Columbine Aquilegia canadensis 3 Mostly Shady $9.37 3 Marsh Marigold Caltha palustris 4 Mostly Sunny $6.81 4 Cowslip Caltha palustris 4 Mostly Shady $9.90 5 Dutchman's-Breeches Dicentra cucullaria 3 Mostly Shady $6.44 6 Ginger, Wild Asarum canadense 3 Mostly Shady $9.03 AVAILABILITY 1 031599 2 030699 3 051799 4 030699 5 012099 6 041899","tags":"R Stats","url":"http://chrisalbon.com/r-stats/import-xml.html"},{"title":"Importing CSV and Delimited Data","loc":"http://chrisalbon.com/r-stats/importing-csv-and-delimited-data.html","text":"Original source: Learning R Note: This tutorial won't work until you change the URL to the CSV and remove the comment tags. # Flat Files Offline # testdata <- read.table(\"d:/data/mydata.dat\", header=TRUE) # valid ## Read.csv is like read.table but the default seperator and adds a header # Flat File Online # testdata2 <- read.table(\"http://science.nature.nps.gov/im/monitor/stats/R/data/ MyData.csv\", header=TRUE, sep=\",\") # Specify . As Denoting A Missing Value # testdata <- read.table(\"mydata.csv\", header=TRUE, sep=\",\", na.strings=\".\")","tags":"R Stats","url":"http://chrisalbon.com/r-stats/importing-csv-and-delimited-data.html"},{"title":"Indexing","loc":"http://chrisalbon.com/r-stats/indexing.html","text":"How indexing is formatted: variable[ row(s), column(s) ] # Create a variables of five elements x <- c ( 1 , 4 , 9 , 16 , 25 ) y <- c ( 1 , 4 , 4 , 36 , 24 ) m <- data.frame ( x , y ) # Select the first, third, and fifth elements x [ c ( 1 , 3 , 5 )] [1] 1 9 25 # Select of all EXCEPT the second and fourth elements x [ c ( -2 , -4 )] [1] 1 9 25 # Select of all EXCEPT the second and fourth elements x [ c ( TRUE , FALSE , TRUE , FALSE , TRUE )] [1] 1 9 25 # Select the entire first row of a matrix, array, or dataframe m [ 1 , ] x y 1 1 1 # Select an the entire first column of a matrix, array, or dataframe m [, 1 ] [1] 1 4 9 16 25","tags":"R Stats","url":"http://chrisalbon.com/r-stats/indexing.html"},{"title":"Indexing Tricks","loc":"http://chrisalbon.com/r-stats/indexing-tricks.html","text":"Original source: the r book # create a dataframe with simulated values x <- runif ( 1000 ) y <- runif ( 1000 ) z <- runif ( 1000 ) a <- runif ( 1000 ) m <- runif ( 1000 ) i <- runif ( 1000 ) j <- runif ( 1000 ) k <- runif ( 1000 ) data <- data.frame ( x , y , z , a , m , i , j , k ) rm ( x , y , z , a , m , i , j , k ) # select all of the columns from row 3 of the dataframe data [ 3 ,] x y z a m i j 3 0.6313948 0.5873463 0.4276207 0.4490631 0.2161926 0.382726 0.5897734 k 3 0.1803608 # drop the whole of row 4 from the dataframe data [ -4 ,] x y z a m 1 0.9370949692 0.3904391259 0.2530045395 0.577547544 3.311816e-01 2 0.8169912936 0.7717700286 0.1949706976 0.431474575 4.468994e-01 3 0.6313947993 0.5873462579 0.4276207259 0.449063063 2.161926e-01 5 0.4706879971 0.2900230386 0.8256189562 0.047032020 8.461648e-01 6 0.3116779991 0.4097562127 0.2170954866 0.047558205 8.109110e-01 7 0.7180650725 0.8154219799 0.2912252382 0.941795656 8.794287e-01 8 0.1410273889 0.2734181802 0.0728253024 0.958947799 7.707356e-01 9 0.1039375949 0.6066154244 0.7298178351 0.237605995 9.278795e-01 10 0.7831988228 0.3816812821 0.0445121613 0.878714589 8.033606e-01 11 0.7780771402 0.4888597687 0.4218451059 0.541726347 3.588103e-01 12 0.8549661185 0.6202294999 0.1322223113 0.378038323 6.000676e-01 13 0.7936905599 0.8205492713 0.5068847497 0.344106114 5.298757e-01 14 0.0497731785 0.4382137246 0.3183242229 0.206805342 4.169467e-01 15 0.1041691788 0.2583737297 0.8669875974 0.409303598 8.821301e-01 16 0.5839785282 0.9458635810 0.1451297677 0.124021082 4.282974e-01 17 0.6301283748 0.8938101421 0.4633655346 0.863932969 8.371921e-01 18 0.0538908630 0.7993367051 0.1662894906 0.337508279 3.171606e-01 19 0.4699743150 0.2179973929 0.9039500949 0.542787326 7.940503e-01 20 0.8096281542 0.4553503187 0.5965530800 0.498133432 6.258840e-01 21 0.6269215296 0.7560675466 0.7957581254 0.840931672 5.378133e-01 22 0.4254209346 0.6859814830 0.8225671803 0.186610433 5.037850e-01 23 0.1013888507 0.2460164321 0.7510759791 0.237333820 7.254038e-01 24 0.9101979593 0.4163650593 0.5020412898 0.633828752 2.637897e-02 25 0.7577146925 0.1546449522 0.4458510922 0.467762029 2.085573e-01 26 0.8808603676 0.8900883312 0.6414468170 0.336437304 1.080850e-01 27 0.2358829738 0.6140667766 0.6801568472 0.451031481 2.824390e-01 28 0.3214730755 0.8646407460 0.2427712576 0.178794772 1.326834e-01 29 0.7767506840 0.9355752321 0.4893625444 0.524841033 5.627676e-01 30 0.2756272962 0.7027894342 0.4434864377 0.173538475 7.091635e-01 31 0.7999626761 0.1894985719 0.6973984847 0.389671007 1.434607e-01 32 0.4605515839 0.1737330768 0.2664361666 0.525079773 3.887599e-01 33 0.7878976951 0.9669068004 0.2513810424 0.279101572 6.431891e-01 34 0.0911097701 0.0928617879 0.2627664267 0.123210540 8.142502e-01 35 0.0275765241 0.6088669079 0.7417254893 0.389373654 6.733047e-02 36 0.7061024618 0.8505606914 0.6830506644 0.284471479 3.249732e-01 37 0.6172355278 0.4425195777 0.8666974416 0.469516267 6.445455e-01 38 0.0632016384 0.0005237793 0.7255827556 0.371542407 5.937821e-01 39 0.8622419508 0.7715821874 0.3958992928 0.667401103 2.421245e-01 40 0.7231777157 0.8044306301 0.0778860459 0.172507857 8.447065e-01 41 0.4927164968 0.1889234560 0.4412154641 0.650509367 9.200443e-01 42 0.5703471822 0.8337357312 0.4939526306 0.031746348 8.359302e-01 43 0.8936006089 0.4499214077 0.0217764010 0.729141285 2.380587e-01 44 0.6202150190 0.3614634674 0.9754849086 0.612729790 3.214372e-01 45 0.7912620052 0.1988568865 0.7979321314 0.248550579 4.465604e-01 46 0.5302869780 0.8204511884 0.5899471235 0.236167135 4.993286e-01 47 0.3760423027 0.6197578758 0.9657687983 0.816466525 4.290283e-01 48 0.4486860153 0.7791226325 0.9174016686 0.599676585 8.931907e-01 49 0.4307841142 0.0385082576 0.8245737706 0.897940309 1.788797e-01 50 0.8595965470 0.3248715224 0.7299323552 0.886358950 8.883161e-01 51 0.0252711680 0.6122176033 0.9074188452 0.897394733 4.080858e-01 52 0.1002211126 0.2690452361 0.3296885516 0.345949224 6.038600e-01 53 0.0807856577 0.0998913255 0.8701844758 0.269625058 3.758807e-01 54 0.7170568479 0.4884574136 0.5018407770 0.403440406 8.281473e-01 55 0.8823796129 0.3786438145 0.3983667553 0.804715395 6.636888e-01 56 0.5683260462 0.7532928397 0.6130672316 0.679588470 1.800565e-01 57 0.8688152484 0.2847076976 0.3810614778 0.824492445 1.734603e-01 58 0.6965453394 0.6635359814 0.1475261322 0.804251817 8.262297e-01 59 0.0802273641 0.3858547176 0.6009099570 0.519138621 5.420346e-01 60 0.0140079728 0.1603731690 0.9542593956 0.364582830 4.444947e-01 61 0.4727872738 0.4798752652 0.6213749733 0.598921384 3.226989e-01 62 0.0459575092 0.0832084191 0.6381078002 0.244956968 3.561886e-01 63 0.1385185381 0.6099833576 0.4599676093 0.350077863 6.626158e-01 64 0.8986839550 0.6535483883 0.4232784463 0.657276285 5.436132e-01 65 0.5309889077 0.4783877260 0.0462058538 0.839267493 4.156813e-01 66 0.0409239323 0.2436686188 0.9950620085 0.206467974 5.036194e-01 67 0.7674356611 0.6750660734 0.6059721843 0.020811806 9.566451e-01 68 0.3960749088 0.2688281150 0.0242629193 0.732992485 2.720517e-02 69 0.3985890197 0.7642159278 0.8804058409 0.930405641 3.359340e-01 70 0.3705691348 0.0467719736 0.1671624330 0.061075286 6.948139e-01 71 0.2754363231 0.9146933509 0.5973722835 0.277121296 2.015421e-01 72 0.5407387190 0.9995588632 0.1497331320 0.887567588 9.082803e-01 73 0.5314532970 0.0659897071 0.3875588439 0.617622069 2.811714e-01 74 0.0803362590 0.4144914525 0.2956018788 0.316371846 2.652967e-01 75 0.8250675497 0.7498207777 0.1106022566 0.318800121 5.904470e-01 76 0.1926816499 0.7235343596 0.7979764920 0.609599486 7.587092e-01 77 0.0425402028 0.0603169515 0.7010821693 0.264385223 3.563557e-01 78 0.3964480858 0.9809383829 0.1755605682 0.750374798 1.333250e-01 79 0.4223065777 0.0653125530 0.9426773726 0.399970984 1.586909e-01 80 0.6523972722 0.2083718709 0.4009221988 0.142288364 9.384446e-01 81 0.3475282227 0.1577986199 0.2000566917 0.625475666 9.390198e-01 82 0.5286904851 0.8383829324 0.3611871728 0.550002784 5.167771e-01 83 0.1822423232 0.3047285695 0.2869544714 0.530641184 2.719163e-01 84 0.0047013757 0.3647190086 0.0340154995 0.298535647 5.472704e-01 85 0.3293453683 0.9229505726 0.7833694841 0.632987165 7.515297e-01 86 0.1202686552 0.4016410608 0.3511313878 0.629080631 8.448410e-01 87 0.8493540538 0.6416005327 0.0867994754 0.628401057 2.774768e-01 88 0.7695235745 0.5029771237 0.6142535997 0.687793811 6.338755e-01 89 0.6904120625 0.8910172952 0.5705231265 0.182845271 7.624777e-01 90 0.3335720669 0.2518689649 0.7833944124 0.695882965 5.926175e-01 91 0.7919240277 0.1950826326 0.6733682633 0.899178525 8.392912e-01 92 0.6215966814 0.8693118924 0.6596714961 0.815885935 1.616443e-01 93 0.5720835368 0.9879097429 0.7489858174 0.896938740 7.624196e-01 94 0.4893391300 0.5527650674 0.8465101460 0.806428441 8.202123e-02 95 0.3000833855 0.4928710270 0.7013142661 0.701419483 7.109783e-01 96 0.1436036394 0.3200193204 0.3412105888 0.606645083 1.577007e-01 97 0.5476652577 0.2205543050 0.3517985044 0.930552275 2.001931e-01 98 0.9131340059 0.0837841036 0.7133782210 0.719024671 7.160846e-02 99 0.8401359348 0.0370367223 0.4614702794 0.079629550 8.718041e-03 100 0.4504831121 0.2381348317 0.4102221406 0.520588716 8.591019e-01 101 0.6254940971 0.5222004815 0.8796439485 0.827647213 5.727372e-02 102 0.7448368245 0.6181551786 0.4219239037 0.467653792 6.276878e-02 103 0.9457870251 0.4207581431 0.3550725051 0.006337496 1.791115e-01 104 0.1310959703 0.4963657029 0.1765153394 0.359040588 6.913411e-01 105 0.8484062718 0.9034369027 0.6787415841 0.586619119 5.452538e-01 106 0.4371888463 0.4749349221 0.6336629451 0.044809929 7.567161e-01 107 0.9108372824 0.8643784064 0.8615905407 0.148015360 1.638905e-01 108 0.4796622975 0.0227832580 0.5454826001 0.527876457 2.512274e-01 109 0.1027305629 0.1072980338 0.3083832473 0.525531210 6.378927e-01 110 0.2847951869 0.4515920919 0.3195284132 0.648003328 2.805665e-01 111 0.9316735922 0.0052489392 0.7868197537 0.505368755 4.154278e-01 112 0.0924596551 0.8432675595 0.4399540033 0.789810633 8.377471e-01 113 0.5673171869 0.2813623182 0.6962677925 0.029624238 3.589913e-01 114 0.8735257180 0.5234353847 0.2538753240 0.232593814 9.064808e-02 115 0.4290504728 0.4694526973 0.2661211840 0.907042397 1.552227e-01 116 0.8996819127 0.0621974380 0.3251096997 0.853816796 2.705220e-01 117 0.0579402959 0.0302452876 0.9352964088 0.585931442 9.032564e-01 118 0.1014418246 0.8006376394 0.2328273214 0.820464912 5.355829e-01 119 0.4509296967 0.1611418875 0.2132689748 0.153000990 1.609333e-01 120 0.2546673652 0.5602219575 0.5468407678 0.947769340 4.493483e-01 121 0.7113273235 0.8592565532 0.1178469069 0.498519980 5.536828e-01 122 0.7692845000 0.0566702003 0.9734982299 0.122954023 6.887685e-01 123 0.3481028744 0.4445713721 0.3522821588 0.787033934 7.327817e-02 124 0.9676634180 0.8296126574 0.9950421427 0.103310564 3.178156e-01 125 0.7331326583 0.0971798035 0.2761500876 0.127017952 4.720898e-01 126 0.0599658110 0.7115563331 0.0540546447 0.106573350 8.228742e-01 127 0.1248628269 0.7587779057 0.1176031553 0.934332104 5.604531e-02 128 0.2991520732 0.3776659367 0.5170687952 0.392918030 3.367465e-01 129 0.6645840129 0.7026710063 0.7148548134 0.970120105 8.332619e-01 130 0.1244262301 0.9337320582 0.1367022363 0.913373896 6.899806e-01 131 0.1236656245 0.0614476257 0.9959184171 0.158355799 8.971463e-01 132 0.0696135804 0.3393731464 0.3305444729 0.654447101 9.089449e-01 133 0.3501586101 0.5802877264 0.3816649450 0.935220445 7.389805e-02 134 0.5799933816 0.7677533829 0.1642848232 0.147463179 3.779670e-01 135 0.1702301877 0.9032238321 0.7545808731 0.886644857 7.954767e-01 136 0.9765062209 0.9296272635 0.8273214900 0.506397160 7.001717e-01 137 0.7148984962 0.8073559222 0.2429534406 0.094754952 4.551603e-01 138 0.3888362625 0.3348044397 0.1438325557 0.191665294 2.816520e-01 139 0.0689336834 0.3640246105 0.3012798512 0.644458028 4.497799e-01 140 0.9315936987 0.7868268418 0.6475578381 0.858399672 8.137392e-01 141 0.7053553022 0.0936103060 0.3798077465 0.989375187 9.145746e-01 142 0.7641971211 0.1513883369 0.6868384471 0.037770061 4.302203e-01 143 0.7521351997 0.9857477553 0.5470604110 0.860812797 9.163151e-01 144 0.1143717000 0.5475322953 0.3682726121 0.548567706 5.226119e-01 145 0.1755395108 0.1196474847 0.8436278808 0.067070395 5.931327e-01 146 0.7314520737 0.8785161506 0.3468810113 0.807048478 1.057343e-01 147 0.0840596480 0.0384507929 0.0654803049 0.837991760 3.502395e-01 148 0.6503178864 0.4422026391 0.9759091735 0.641077164 5.667044e-01 149 0.8325164884 0.1573755434 0.4993437151 0.488269411 5.032156e-01 150 0.6056464242 0.9779126472 0.0997745579 0.719270619 4.198256e-01 151 0.1708778741 0.0848242871 0.1683927856 0.489719765 1.845283e-01 152 0.7192412303 0.4588409185 0.8876504118 0.228980459 8.690613e-01 153 0.7769590721 0.8675205393 0.6314575365 0.143649914 6.875175e-01 154 0.3580576980 0.3434970572 0.9685689912 0.824826804 6.023344e-01 155 0.1536754405 0.7642185981 0.7190012864 0.920236114 5.640764e-01 156 0.8876610205 0.2691004938 0.9682509759 0.009020783 7.899509e-01 157 0.9968752931 0.8581504109 0.4439522170 0.612094169 9.859647e-01 158 0.5289386453 0.6496191269 0.2969512208 0.413060909 3.806414e-01 159 0.6329279244 0.0386093326 0.3381252782 0.360855727 3.545589e-02 160 0.0034896852 0.1933919925 0.7016994103 0.594918283 8.101724e-01 161 0.8976498600 0.1778210327 0.4423170090 0.982059956 4.761609e-01 162 0.2601647917 0.5901049029 0.1208666377 0.791583062 8.117129e-01 163 0.3986346393 0.8277773953 0.4906746778 0.731339336 7.174461e-01 164 0.5764877701 0.6677092968 0.3722350411 0.684600540 4.484358e-01 165 0.7178556903 0.1503093268 0.0881471203 0.700219793 5.278507e-01 166 0.1486660023 0.3068179993 0.6048259446 0.367801031 8.009471e-01 167 0.2934395475 0.3574407734 0.1875770059 0.305140699 1.799086e-01 168 0.8550011665 0.9941004280 0.6293192946 0.323219864 9.496474e-01 169 0.9138944144 0.1918540595 0.7362092163 0.129870965 7.500945e-01 170 0.5692845336 0.0576184834 0.9621483448 0.946476570 3.975674e-01 171 0.2180541193 0.5792422167 0.7194545367 0.424145223 2.506493e-01 172 0.8578979487 0.7784323918 0.1277054867 0.572289248 2.549385e-01 173 0.0568097783 0.8187201719 0.5725159331 0.619513634 2.115072e-01 174 0.9992063609 0.8830820697 0.0208103533 0.271749542 2.407214e-01 175 0.4691610557 0.1709448709 0.5638727772 0.205742766 7.858254e-01 176 0.9072538142 0.8136773086 0.7826462083 0.661408056 9.535864e-01 177 0.1765437901 0.9630451202 0.0341300871 0.065843770 6.814811e-01 178 0.2081198960 0.5721260966 0.8691763720 0.623829032 7.277410e-01 179 0.9020622084 0.7788336426 0.9740692992 0.897657196 6.079173e-01 180 0.5577519296 0.8870667333 0.0559192663 0.917418429 1.502496e-01 181 0.3174477716 0.5591419074 0.9091531408 0.689324741 3.603843e-01 182 0.3988291954 0.9360842388 0.9412023835 0.151519891 4.725731e-01 183 0.6296822894 0.2734462984 0.5310084445 0.435572824 3.481015e-01 184 0.8532116541 0.3789087695 0.4137729306 0.792026432 6.575724e-01 185 0.0019157159 0.9480523288 0.0558498637 0.690238338 5.564615e-01 186 0.2165099829 0.2713331431 0.7154832364 0.642090820 6.811029e-01 187 0.0528591487 0.4438034375 0.6167128414 0.823401734 5.188929e-01 188 0.2140440524 0.9970479552 0.5048359402 0.773988509 3.399884e-01 189 0.6131354324 0.2978879120 0.5568822010 0.096377717 8.602817e-01 190 0.3969264787 0.1390109258 0.3459203229 0.515168304 2.417488e-01 191 0.8599386800 0.0208728844 0.6721742607 0.940914826 1.805654e-01 192 0.3918516901 0.8487497859 0.5915288015 0.326262000 3.761560e-01 193 0.0001515732 0.9925137931 0.0870158968 0.911938774 1.018230e-01 194 0.9137823463 0.9482441705 0.6661104674 0.009660149 6.747976e-01 195 0.3185021740 0.4742731366 0.6751790117 0.216926428 7.693648e-01 196 0.5787437696 0.0562161675 0.0022941325 0.981683360 9.763876e-01 197 0.7559065737 0.3867585564 0.3821420241 0.401733570 4.982639e-01 198 0.5163620966 0.6031210783 0.5991793235 0.768042127 7.776593e-01 199 0.3772833832 0.2417656146 0.0676334375 0.179017943 5.219626e-01 200 0.8303419396 0.6909590135 0.2075274838 0.982338600 2.271104e-01 201 0.7211326503 0.6640000213 0.9592173509 0.649954116 7.933037e-01 202 0.5639012777 0.2657555621 0.3693294188 0.653942175 9.238735e-01 203 0.2644100867 0.4423922328 0.5288352752 0.791906926 9.936834e-01 204 0.4798865798 0.3394446452 0.4974069681 0.758591116 6.578006e-01 205 0.3535982657 0.3782534536 0.2380272320 0.377593074 7.628336e-01 206 0.7971998851 0.0415869437 0.9998667797 0.292781072 5.019907e-02 207 0.9474380957 0.1888907838 0.6537682402 0.442967691 5.412440e-01 208 0.8931702694 0.6448802592 0.6577227118 0.882041997 9.742949e-01 209 0.3331450461 0.0773383095 0.3559794263 0.825966841 5.472149e-01 210 0.3673691975 0.0164993445 0.3723789565 0.994233836 9.561312e-01 211 0.0161621387 0.4990861469 0.4040669564 0.980775977 9.391028e-01 212 0.6557840866 0.7605664600 0.5519499718 0.770046454 9.594055e-01 213 0.2010356160 0.9608684320 0.2316770859 0.630396901 9.695247e-01 214 0.5701537321 0.2108435808 0.0258280484 0.855769821 3.193294e-01 215 0.2032754768 0.1362372376 0.3567790452 0.898438573 6.869137e-01 216 0.4177006818 0.8037221890 0.7529435763 0.010452493 1.591609e-01 217 0.2006519388 0.8048699212 0.8357756212 0.413250533 4.062866e-01 218 0.1065086264 0.6491837867 0.2524011680 0.955415523 7.185965e-01 219 0.9091413908 0.4442297982 0.6562724640 0.923782989 3.390019e-01 220 0.0242540571 0.9883826412 0.8197069752 0.814794991 5.391943e-01 221 0.5542232401 0.8130007542 0.5624401649 0.064768330 9.854170e-01 222 0.6204521400 0.4963250675 0.2481274225 0.389114767 8.654488e-01 223 0.5394885978 0.2772327969 0.1048048458 0.480904928 6.280108e-01 224 0.0745619617 0.1558587539 0.7687460070 0.786436838 2.117698e-01 225 0.2163315983 0.6801391370 0.8055323400 0.686167828 5.312500e-01 226 0.0676476448 0.8733174466 0.3134679864 0.369815800 9.743214e-01 227 0.5916824974 0.0191610819 0.3581683834 0.237326151 8.822456e-02 228 0.2836414992 0.8274080611 0.8160740463 0.393665259 6.703462e-01 229 0.2384083509 0.3266589346 0.6799719462 0.637329965 6.830281e-01 230 0.8608596043 0.3201513030 0.1370995173 0.974307173 3.404101e-01 231 0.5796163038 0.9357685193 0.7526069139 0.197657710 1.402370e-01 232 0.2412788384 0.1476416474 0.1673243914 0.086085135 7.060136e-01 233 0.3736466474 0.0715341098 0.1843706726 0.281694533 8.314010e-01 234 0.3288877220 0.3422220629 0.3864410103 0.129961584 3.187321e-02 235 0.2161439408 0.8040320897 0.1244801774 0.744883351 3.924180e-01 236 0.6735424299 0.7795171738 0.4039235790 0.011590910 9.798672e-01 237 0.1124215857 0.4681109225 0.7216107910 0.829600236 8.380484e-02 238 0.8839060992 0.7237063833 0.2847857845 0.846192709 5.581730e-01 239 0.8819734182 0.1144621901 0.0116158570 0.725888924 3.569096e-01 240 0.9354603821 0.4831247553 0.5211163305 0.773195356 5.565796e-01 241 0.5767845511 0.2684109176 0.6986416469 0.510211729 6.383459e-01 242 0.5818626797 0.9073878161 0.1752908952 0.758165545 5.812421e-01 243 0.8275333415 0.5267427347 0.9090091116 0.344543359 5.143553e-01 244 0.4781270074 0.4272068595 0.4470525885 0.347394072 9.982698e-01 245 0.7396636771 0.5585044932 0.3863007929 0.517322354 9.656356e-01 246 0.8310291567 0.5190551940 0.0384265762 0.150503708 2.366544e-01 247 0.3675826853 0.3041335819 0.6259368560 0.022825431 5.922777e-01 248 0.1966541104 0.5881767110 0.3664567897 0.618014229 9.890586e-01 249 0.9975247402 0.8564226844 0.0035603254 0.301549508 1.862727e-01 250 0.8214518884 0.7014278157 0.5982679054 0.192868671 8.084818e-01 251 0.5489195844 0.6956572754 0.4245661907 0.657216802 6.434412e-02 252 0.9222708542 0.4495614534 0.4786704215 0.608280219 7.829196e-01 253 0.6170779034 0.1915762192 0.7051625785 0.100203936 7.363104e-01 254 0.7798010246 0.8297415115 0.9719362056 0.293687331 5.095400e-02 255 0.8295236412 0.7012862060 0.7040839291 0.076756692 4.320663e-01 256 0.7921326044 0.6680933705 0.7468525905 0.221332673 8.397830e-01 257 0.9725491845 0.8900939021 0.9264180593 0.579828278 4.414026e-01 258 0.5389117019 0.2145371423 0.7034534132 0.916396324 8.566715e-01 259 0.7039349154 0.5264377154 0.2763565620 0.608822979 4.653691e-01 260 0.9149633690 0.8434623827 0.6857282268 0.921606234 1.788714e-01 261 0.1539534209 0.5374164968 0.4353943137 0.066600877 9.459214e-01 262 0.2444682512 0.9467447461 0.2996765305 0.993174250 4.733773e-01 263 0.3417980904 0.6108143430 0.1764438688 0.070948276 4.806769e-01 264 0.4999335201 0.1733239812 0.0905771561 0.800303387 5.200107e-01 265 0.5066742979 0.2890426961 0.5416354267 0.498188548 2.367029e-01 266 0.5838516832 0.3779571787 0.4468531371 0.298747589 1.695074e-01 267 0.8129630184 0.4197585413 0.2865401660 0.384935115 4.562674e-01 268 0.2090866042 0.0312464098 0.6281993804 0.184038194 3.677554e-01 269 0.6440592636 0.1469305602 0.7661810701 0.838563889 9.731990e-01 270 0.7281985290 0.5473523897 0.6157018538 0.700302164 6.428662e-01 271 0.6646557718 0.2829667975 0.6044241658 0.799832808 3.056976e-01 272 0.9352728250 0.7136875202 0.0289973079 0.965694083 3.239665e-01 273 0.0728821249 0.7892174108 0.1717887009 0.521993847 4.899518e-01 274 0.0346095334 0.2692810481 0.7561781360 0.926916890 6.162302e-01 275 0.9504906929 0.7526574284 0.9379664683 0.068118802 8.603263e-01 276 0.5145728933 0.4559605923 0.9176913910 0.004695723 2.866595e-01 277 0.3976553013 0.3987538943 0.2839024831 0.293565525 7.702066e-01 278 0.7009254443 0.0133173962 0.9493629304 0.233285994 4.994754e-01 279 0.6224461349 0.6137198601 0.7054065103 0.907167699 6.948691e-05 280 0.3599138320 0.9875511052 0.2561543379 0.838496193 1.224435e-01 281 0.6635836901 0.4874325164 0.8553156825 0.733857719 5.639178e-01 282 0.4764982043 0.1185725085 0.6769570536 0.422804818 2.323357e-01 283 0.0649527388 0.3997544402 0.5082662296 0.536742677 8.150507e-01 284 0.5637654911 0.5065516052 0.3812452687 0.325095090 7.836266e-01 285 0.6024410788 0.7558844115 0.9651189120 0.758352714 8.845431e-01 286 0.0865011127 0.6272108338 0.0864172361 0.372212857 3.729183e-01 287 0.7726058052 0.8873297924 0.9845032052 0.926265871 3.398011e-02 288 0.2364514670 0.1204383390 0.6467557393 0.499837749 1.033158e-01 289 0.5903890356 0.2814705893 0.6270863349 0.544416004 7.191814e-01 290 0.0362968044 0.6435941649 0.8072538176 0.628011037 1.350844e-01 291 0.4776646101 0.9162236417 0.0477408909 0.195150346 9.917569e-01 292 0.9850660686 0.8860944102 0.8053446305 0.690133943 1.764347e-01 293 0.5599008847 0.8146942554 0.7810434583 0.804373655 5.683600e-01 294 0.5014732699 0.0134419615 0.0673355807 0.265304377 6.757556e-01 295 0.1202006293 0.8631854309 0.9486900396 0.120854634 5.592875e-01 296 0.2335651731 0.1641146073 0.5350339490 0.555488213 9.398174e-01 297 0.9651232909 0.4984271645 0.2115402070 0.966395828 4.653873e-01 298 0.1128552298 0.1122542853 0.5322054287 0.839569817 5.417432e-01 299 0.8394041124 0.2560082425 0.1669750074 0.328798706 8.281256e-01 300 0.3041334234 0.7385249771 0.7241052580 0.278196611 8.285159e-02 301 0.4291426220 0.8197907398 0.0797945955 0.593694864 2.967888e-01 302 0.4523428532 0.7067194383 0.8770909235 0.743807043 8.027994e-02 303 0.7211188022 0.3782203167 0.6527704687 0.173949218 3.157459e-01 304 0.5325777873 0.2437774444 0.8282919656 0.346972036 8.068654e-01 305 0.2279141766 0.5629842561 0.9597962128 0.896484934 1.616834e-02 306 0.9273555675 0.0055806914 0.6181486198 0.898301309 4.427710e-01 307 0.0714621588 0.1762667750 0.0954093228 0.078771684 3.226275e-01 308 0.0913349085 0.1022600732 0.3445813630 0.739462374 9.278758e-01 309 0.8353813579 0.0021331101 0.6448888553 0.433755042 6.941956e-01 310 0.8717507403 0.6973458694 0.9974252689 0.954311525 3.399146e-01 311 0.3402612589 0.0207138949 0.7791891245 0.138285730 9.553810e-01 312 0.1030242851 0.6788007997 0.0091685629 0.080596239 6.100400e-01 313 0.8796044991 0.7409640790 0.5888034666 0.482930689 9.066809e-01 314 0.4184627694 0.1518161653 0.9470163304 0.640979418 7.321634e-01 315 0.4914691530 0.4347692737 0.8240045996 0.113172951 7.336182e-01 316 0.1891047948 0.9546289963 0.9326387078 0.084773930 7.528037e-01 317 0.4091451312 0.4061187478 0.4935212114 0.063884293 6.897970e-01 318 0.1740348001 0.7072475406 0.7000157465 0.471941573 4.158820e-01 319 0.2931793360 0.2669634703 0.4962130494 0.383236896 9.967678e-01 320 0.2781334203 0.2826841313 0.7971504361 0.431237619 1.332127e-01 321 0.6324098671 0.9961036481 0.7603126075 0.319680428 6.017842e-01 322 0.5705256499 0.3483372750 0.9408938496 0.273614281 7.343426e-01 323 0.1463123327 0.1304147257 0.0196428671 0.466923666 8.377781e-02 324 0.3067270615 0.0682549430 0.0426511776 0.288729959 6.353246e-01 325 0.8781642185 0.3042296777 0.4770269010 0.298304684 6.386353e-01 326 0.0776095102 0.0545013193 0.6625969552 0.251515612 3.353573e-01 327 0.4279278640 0.6352059934 0.2660763490 0.261018408 9.899702e-01 328 0.0869317167 0.5169583762 0.3695479026 0.720178066 3.620540e-01 329 0.8999204701 0.7806742371 0.7687030011 0.375773629 6.745213e-01 330 0.8751550377 0.3289561400 0.4191904066 0.427931608 1.081395e-01 331 0.0761351229 0.5991958415 0.2431351477 0.914929265 5.326279e-01 332 0.9381568427 0.4902186608 0.7542906129 0.672044334 3.062858e-01 333 0.8256581253 0.6891283139 0.8559265318 0.364524166 5.110359e-01 334 0.7706560523 0.2872826036 0.2328567246 0.504470113 7.242145e-01 335 0.6093809877 0.9180088332 0.2086923716 0.571133680 3.943475e-01 336 0.6783998506 0.8340922757 0.5525323339 0.361998960 1.303991e-01 337 0.8349843030 0.8474897714 0.5272933764 0.799968964 2.671729e-01 338 0.6991907230 0.4609518785 0.2405392446 0.923717583 4.763034e-01 339 0.8896550976 0.1483478416 0.9771987102 0.383924895 4.411620e-01 340 0.7373167558 0.2668425364 0.4646588794 0.290820904 5.789469e-01 341 0.7777801065 0.1640932390 0.6075391858 0.002127058 7.864690e-01 342 0.1142391388 0.6719365513 0.4796372068 0.594974790 4.127344e-02 343 0.9957590452 0.6051899521 0.3506976240 0.360922266 5.062548e-01 344 0.8975299238 0.8794793843 0.4141596090 0.132358324 5.256242e-02 345 0.3591199489 0.8844800305 0.0099952731 0.824255990 2.001769e-01 346 0.5356684977 0.0438097841 0.3991168796 0.057423466 3.378591e-01 347 0.8537007736 0.3376973567 0.7265061915 0.990835784 4.065400e-01 348 0.6791082718 0.3562199960 0.9257631865 0.600217237 7.820375e-01 349 0.5602437346 0.2175566691 0.5589008050 0.029843965 2.610109e-01 350 0.6625470396 0.7728067513 0.3640043263 0.444563451 8.506265e-01 351 0.0657600570 0.4441296456 0.8104015475 0.270668493 7.601029e-01 352 0.8657236823 0.7491532350 0.8000338343 0.789722052 2.227382e-01 353 0.8693265212 0.1323963902 0.0203081183 0.422943778 8.704134e-01 354 0.6427183610 0.4065263735 0.2248546493 0.780684578 2.740526e-01 355 0.5759076793 0.6806553851 0.8696183837 0.845955456 9.591285e-01 356 0.6498449650 0.2195812284 0.1431319460 0.681847408 5.779802e-01 357 0.3165733689 0.2931888984 0.3416584737 0.114969182 1.011104e-01 358 0.8982761570 0.9184429229 0.7322741628 0.443741165 4.404219e-01 359 0.4469846345 0.5782766959 0.2848459983 0.706270285 4.893691e-01 360 0.1091156455 0.1869821378 0.5436482152 0.204792706 9.771976e-01 361 0.3657996678 0.5740619248 0.3449009201 0.116321134 7.446984e-01 362 0.1806305090 0.2856565488 0.5067477161 0.821413193 3.083452e-01 363 0.1164003408 0.0459067044 0.2686422307 0.954782017 4.855251e-01 364 0.8020089313 0.7331166179 0.9361567646 0.789247320 3.798315e-01 365 0.1938385800 0.5443917396 0.7268236890 0.714485556 8.691682e-02 366 0.7173356623 0.7768663752 0.9018600665 0.963223044 2.476719e-01 367 0.5296206458 0.5590656023 0.0882430347 0.370041748 6.353881e-01 368 0.0742895349 0.3294574337 0.3487890468 0.698375692 3.642144e-01 369 0.0096567774 0.1633153663 0.2697227211 0.612484920 3.893997e-01 370 0.0007421148 0.3710664017 0.3654992618 0.858424162 9.942708e-02 371 0.0144137791 0.4277930632 0.1251683424 0.439849179 5.469326e-01 372 0.6370710037 0.0225923632 0.9081482203 0.981841748 5.412906e-01 373 0.1093050879 0.2239815341 0.9036308534 0.475337584 7.634477e-01 374 0.0706267995 0.6466684432 0.8195659919 0.827542959 7.745236e-02 375 0.7718457142 0.8100467415 0.1238228502 0.858416016 9.465533e-01 376 0.1993048138 0.7963158851 0.9567167670 0.136876087 7.876701e-01 377 0.8673933318 0.4083252754 0.5028514632 0.270459414 4.284302e-01 378 0.0327129478 0.6679480514 0.6981502497 0.241763491 5.291740e-01 379 0.1309007367 0.8956746466 0.4353174695 0.311975433 2.617224e-02 380 0.6862117657 0.0567090861 0.2643035925 0.947567889 3.024849e-01 381 0.9820180680 0.7590667398 0.4497556726 0.826217088 2.475621e-01 382 0.4473482983 0.3026340993 0.2187455115 0.525228607 8.370121e-01 383 0.7837314866 0.9404827747 0.6447940255 0.692938740 6.625183e-01 384 0.4695104088 0.3393136084 0.3539974955 0.076961612 7.125074e-01 385 0.4953724076 0.3190722512 0.9209394744 0.593576920 6.993314e-01 386 0.7110873724 0.5765278214 0.8290389117 0.975892123 4.599566e-01 387 0.2224491003 0.2692736811 0.2377511768 0.800969388 3.679071e-01 388 0.7823642467 0.8590320838 0.5696742025 0.587688962 7.797665e-01 389 0.0418753801 0.8708307620 0.1299097429 0.686934761 3.754749e-01 390 0.3174569593 0.9145249254 0.3968546884 0.463118966 2.330016e-01 391 0.4599263112 0.9685912782 0.2395614071 0.702558178 9.325322e-01 392 0.3235118547 0.7471794712 0.9446711387 0.471918447 3.287914e-01 393 0.0868029166 0.5084290067 0.2607131617 0.270282100 9.171089e-01 394 0.6068624684 0.9036543421 0.5734851684 0.183628874 2.812521e-01 395 0.8760592777 0.3600250112 0.4945351148 0.963925146 8.247189e-01 396 0.1094516823 0.6648093811 0.0002113678 0.381599879 9.678066e-01 397 0.4957438977 0.3097762633 0.6246782839 0.210444017 5.248227e-01 398 0.8213692890 0.6233094407 0.2159364189 0.406675840 4.955265e-01 399 0.0229937893 0.6928474947 0.7193192698 0.296294143 5.317764e-01 400 0.6782247606 0.7866539420 0.7101236880 0.041422781 2.519602e-01 401 0.5135679946 0.4825126145 0.2595921732 0.038293313 5.709323e-01 402 0.2084596925 0.2265437515 0.3525143140 0.768503838 4.490639e-01 403 0.4135049034 0.4232750146 0.5646171367 0.069003359 9.856659e-01 404 0.5864820946 0.5760116780 0.6705476989 0.292407878 1.860817e-01 405 0.2408297034 0.0869413330 0.7113251509 0.030035041 2.684469e-02 406 0.4996919823 0.2686212256 0.2869927129 0.443136572 5.427097e-01 407 0.6976517173 0.5914358348 0.4772423580 0.645434657 5.177249e-01 408 0.6071811004 0.0431133511 0.2221226352 0.558366798 6.757148e-01 409 0.4892499996 0.5023605570 0.1043147992 0.252860805 7.848952e-01 410 0.1347163925 0.8824594235 0.2700915858 0.396543218 5.882499e-01 411 0.4283737643 0.0430903563 0.1562245784 0.110033886 9.573125e-02 412 0.6296868890 0.8916814390 0.5608932087 0.514378582 4.608000e-01 413 0.7040708861 0.4345444480 0.4374845265 0.656432379 6.506390e-01 414 0.6781820890 0.1922134126 0.9248232390 0.437797368 3.952176e-01 415 0.0526506035 0.4131393754 0.8803833262 0.649732224 6.173803e-01 416 0.7959502731 0.9659142359 0.1647356863 0.043900094 2.414191e-01 417 0.0500476982 0.7080166151 0.8463139313 0.121874181 7.449012e-01 418 0.5258683611 0.7866339283 0.2648841939 0.825389879 6.909416e-01 419 0.1958894294 0.3899555679 0.8682446461 0.996587435 3.139904e-01 420 0.0855531532 0.7530330606 0.7656025854 0.180940423 4.797922e-01 421 0.4479133089 0.7504540395 0.8510300457 0.917122714 8.110157e-01 422 0.5358806332 0.1840613915 0.8404297752 0.695809044 4.860580e-01 423 0.8574897067 0.4740581512 0.9840828453 0.388464857 4.838642e-01 424 0.7508345970 0.6199492190 0.3873363633 0.212975476 6.846711e-01 425 0.3002946340 0.2649874613 0.4472604445 0.670382250 7.239693e-01 426 0.8132292426 0.5420388943 0.4577483626 0.621782153 8.033528e-01 427 0.4620008818 0.2777338503 0.4325356120 0.413525639 9.745306e-01 428 0.3230311878 0.6950940713 0.2689813899 0.850076854 3.209517e-01 429 0.4746141164 0.4966602982 0.8828971023 0.542695990 5.727530e-01 430 0.4600041225 0.9534178204 0.8522334506 0.302213856 8.852952e-01 431 0.8426773683 0.9747501956 0.0451706389 0.388267011 4.896859e-01 432 0.4915075402 0.4375492195 0.5891421444 0.690327780 6.521933e-01 433 0.7539070100 0.1259638229 0.5254330765 0.614419028 3.255130e-01 434 0.1207034974 0.1975698317 0.0973578235 0.084314038 8.940771e-04 435 0.6456332053 0.7307007704 0.2282598969 0.646824725 8.661351e-01 436 0.5734037792 0.5650301496 0.1870769637 0.165129308 5.357737e-01 437 0.0494311822 0.9188226117 0.7185625699 0.432994389 5.269118e-01 438 0.0277321704 0.9803931457 0.2070943683 0.710366176 2.701794e-01 439 0.0178599542 0.6922797281 0.7564428258 0.667626088 1.513596e-01 440 0.4946952646 0.4058102395 0.1119535868 0.220409061 5.919344e-01 441 0.8835002473 0.5677845615 0.1081138004 0.675165835 4.661971e-03 442 0.6331623469 0.7801744484 0.1189808135 0.414237151 1.799904e-01 443 0.5561390182 0.3673749596 0.7417061792 0.821517335 6.608645e-01 444 0.1873597139 0.9747207032 0.8465773889 0.046875914 3.714898e-01 445 0.9465421280 0.0706294128 0.0714444439 0.945401258 7.603055e-01 446 0.7546351640 0.8103325381 0.7578929288 0.073837923 3.450114e-01 447 0.8472598379 0.5312037698 0.7027316322 0.347222402 8.442074e-01 448 0.9271843736 0.4976025869 0.2601484831 0.881693521 8.947889e-01 449 0.6015616008 0.0193471697 0.5047459286 0.771554465 6.620372e-01 450 0.3465780267 0.4549790232 0.5562954093 0.799919833 6.022381e-01 451 0.4260562521 0.7672826869 0.2570241319 0.157866101 9.280257e-01 452 0.3984505145 0.4370649243 0.3836146572 0.346790810 8.788060e-01 453 0.2113784782 0.5443354140 0.3122317644 0.552217386 6.442702e-01 454 0.0376454894 0.0106201086 0.4465854045 0.577008852 9.825328e-01 455 0.4784657105 0.8741496983 0.1614553235 0.541224399 3.360020e-01 456 0.0600217197 0.5047202792 0.0530216661 0.561827486 7.207989e-01 457 0.4019439009 0.7448705870 0.6991459318 0.858834204 2.629050e-01 458 0.5859271495 0.8682614388 0.2194845770 0.433029509 8.678002e-01 459 0.7683075613 0.8662802191 0.0412932397 0.332626628 3.531620e-01 460 0.8506514311 0.7222218062 0.8751874901 0.020963814 7.421935e-02 461 0.2668082241 0.9159252078 0.5414669486 0.993659624 5.213159e-01 462 0.8532688692 0.5136732112 0.6805261644 0.969336346 1.181152e-01 463 0.3914166926 0.1625697238 0.7115389123 0.613171886 5.563475e-02 464 0.9178691409 0.5312489253 0.1411212357 0.921012539 5.166795e-01 465 0.3401386235 0.7289904719 0.1812944608 0.052570121 1.991569e-01 466 0.9952564673 0.4240395194 0.3330458766 0.654532294 4.914733e-01 467 0.2361274951 0.7220603130 0.1801518665 0.226354291 2.317657e-01 468 0.9484631482 0.1171784431 0.3814109282 0.151987331 1.259127e-01 469 0.1971958587 0.5476996582 0.8278281968 0.006280510 1.377210e-02 470 0.4303416610 0.7595192476 0.2089759717 0.666954212 4.647386e-01 471 0.3610162260 0.8105125707 0.5239570760 0.831454685 2.229034e-01 472 0.4729091411 0.9529160575 0.1585315673 0.368948445 2.876274e-01 473 0.4228429645 0.1028229753 0.1130753367 0.712432174 7.977939e-01 474 0.4466349385 0.8416559217 0.2405777008 0.997086962 1.029476e-01 475 0.7607556819 0.4972156934 0.0339827556 0.196671375 5.321757e-01 476 0.2251486822 0.0994993590 0.7663894410 0.888381128 5.743740e-02 477 0.7277364382 0.9162787397 0.1941889157 0.516441212 3.768768e-01 478 0.4330620549 0.7756441284 0.6987062974 0.021771210 8.454757e-01 479 0.2342932397 0.4582609411 0.1460496057 0.370035785 1.053588e-01 480 0.4623888060 0.4177094412 0.3688633731 0.421409196 8.330933e-03 481 0.7287827786 0.5170731510 0.5754181799 0.829369209 4.207426e-01 482 0.2487997608 0.7418034966 0.0145854305 0.140747132 9.087611e-01 483 0.1584943631 0.4967057044 0.1377874245 0.246570810 5.367151e-01 484 0.8771969816 0.3497019310 0.5086943456 0.857494939 1.820041e-01 485 0.3275236553 0.6556553524 0.8936726141 0.695424152 5.797824e-01 486 0.3876958531 0.7141255415 0.7401520242 0.730453796 8.218417e-01 487 0.0859429245 0.3932208200 0.4963257634 0.023122997 7.484599e-01 488 0.0478853509 0.5820773568 0.4347384747 0.495591412 2.514400e-01 489 0.7640398338 0.6219047029 0.9018226827 0.367291192 6.662036e-01 490 0.7738003614 0.7122801496 0.3401500941 0.813406494 6.130019e-01 491 0.2811919176 0.1084264976 0.8185484011 0.524333827 5.448712e-01 492 0.4107365129 0.4592057569 0.4585471388 0.939687159 9.684651e-01 493 0.6586276228 0.2361251698 0.3982894844 0.736938675 7.158374e-01 494 0.8373886999 0.6760577103 0.4539175981 0.140440820 1.205565e-01 495 0.3794597711 0.0379660081 0.8831326116 0.119593804 3.948079e-01 496 0.7899617921 0.3214019027 0.7558209519 0.178459910 4.474716e-01 497 0.6494789242 0.1239918764 0.7879019941 0.566567896 2.093041e-01 498 0.4682361067 0.8054523449 0.0313273268 0.917019749 3.626654e-01 499 0.8111232512 0.4842337884 0.9905550019 0.443862415 7.314548e-01 500 0.7845828277 0.6678469195 0.0099117353 0.938890793 8.898346e-01 501 0.8596741629 0.0578216626 0.1373633842 0.242686614 3.390618e-01 502 0.0336864635 0.9917070670 0.0058358226 0.576859908 7.295833e-01 503 0.0090650320 0.1259467914 0.5226027011 0.221464730 8.944133e-01 504 0.7393250617 0.4164622889 0.2562337969 0.309255973 1.536401e-01 505 0.1968866454 0.6010752646 0.8266404339 0.241861298 8.562246e-01 506 0.5490019484 0.0597066092 0.6746688485 0.988395112 2.257646e-01 507 0.6120238367 0.8842251308 0.0106372475 0.657762127 4.932302e-01 508 0.5809495263 0.6934050722 0.7337883902 0.881573875 2.639341e-01 509 0.5484295946 0.7735354542 0.8462061526 0.510266755 8.889722e-01 510 0.0912326679 0.0196348897 0.8939744835 0.996975462 5.472280e-01 511 0.0193336261 0.3792007137 0.3810819855 0.306738382 6.755923e-01 512 0.9039375873 0.6896352544 0.2266765735 0.024039615 8.621554e-01 513 0.8407693366 0.1688590143 0.8865542472 0.448400312 5.756914e-01 514 0.3070470826 0.9788418170 0.5187856420 0.449613948 2.021171e-01 515 0.7346552406 0.4564591136 0.0886887216 0.213024601 1.288321e-01 516 0.1057571096 0.1084782106 0.8899245381 0.383389150 6.462186e-01 517 0.6396863880 0.7724902784 0.1619776264 0.529304153 4.767796e-01 518 0.5444227399 0.5911405408 0.0523558804 0.344859310 3.515537e-01 519 0.5455354273 0.4614260718 0.7244580921 0.583712463 3.205922e-01 520 0.1358930182 0.7673941576 0.4223320177 0.252359887 4.387520e-01 521 0.4758632802 0.5606147940 0.3586316842 0.686842689 8.451977e-01 522 0.1343210761 0.4462387625 0.1000703084 0.198462720 1.241996e-01 523 0.1480307344 0.2055510634 0.9462425506 0.091771150 7.064920e-01 524 0.2279162470 0.5673055674 0.9074591370 0.592499505 5.908225e-01 525 0.3441012220 0.3722269435 0.7897777304 0.841536568 8.157750e-02 526 0.1684615102 0.4908088224 0.6635147301 0.656268670 1.541303e-01 527 0.3644688802 0.6197951366 0.3999979883 0.961548903 2.104624e-01 528 0.3026080551 0.0421625217 0.9051791250 0.189580926 5.986782e-01 529 0.7649763497 0.8567843242 0.4777257033 0.060635211 4.449084e-01 530 0.5515219311 0.3415239272 0.3919794145 0.821882782 8.111506e-01 531 0.6972977808 0.7972091183 0.3467263887 0.019405104 3.704000e-02 532 0.0387044246 0.2070823517 0.8566835134 0.785161634 7.065999e-01 533 0.9039842428 0.6613530181 0.4024148083 0.620648935 1.516588e-01 534 0.4088809304 0.6672650862 0.5995888044 0.338171331 4.423938e-02 535 0.5378535052 0.9996105607 0.5368517251 0.006045962 3.168881e-02 536 0.3881504608 0.0245178782 0.3426053128 0.328514404 5.110480e-01 537 0.3916057695 0.6706551816 0.8657774576 0.036358000 4.571243e-01 538 0.0028829025 0.4516399151 0.7030717239 0.455910068 4.933923e-01 539 0.4001601264 0.1733659208 0.2999020617 0.905883759 5.860176e-01 540 0.8600010681 0.4321305510 0.5080972612 0.443594804 3.532380e-01 541 0.8715729243 0.9938263360 0.5023601486 0.837488207 8.630757e-01 542 0.9454444251 0.1035464187 0.2287527740 0.099763853 9.503204e-02 543 0.2098055060 0.3725548622 0.5470067055 0.847507765 6.545298e-01 544 0.4703532252 0.9060373341 0.2752195084 0.092471990 5.282526e-01 545 0.2748672708 0.3570935251 0.8742467877 0.607690911 3.772721e-01 546 0.0433487017 0.5896492933 0.9129612078 0.755463495 4.825281e-01 547 0.9404139488 0.0481122537 0.6314002185 0.407415794 5.865745e-01 548 0.6125317849 0.3405352619 0.9933596929 0.618332138 4.827819e-01 549 0.4461862082 0.5074156527 0.0893472889 0.997433731 2.901876e-01 550 0.2108046531 0.4348503361 0.3725137783 0.732523779 9.000062e-02 551 0.4465482270 0.7043836070 0.3406603502 0.058125994 8.740000e-01 552 0.5953917825 0.2501409757 0.4145573166 0.527952925 8.065012e-01 553 0.7835872069 0.9648590535 0.6512023690 0.225035774 4.342820e-01 554 0.5583098917 0.9271848723 0.2747644160 0.034011265 3.326598e-01 555 0.0164725739 0.1276548684 0.3477022895 0.940272366 6.519734e-01 556 0.1401534586 0.9556691791 0.8838821112 0.082095703 1.772069e-01 557 0.6577758456 0.9237059227 0.7349070930 0.675827107 8.381635e-01 558 0.3120215600 0.8630618234 0.3595919712 0.646322087 5.722389e-01 559 0.2710284372 0.3668614225 0.5772195486 0.732232192 9.656908e-01 560 0.3835913802 0.8062818865 0.2430923579 0.391548166 1.499482e-03 561 0.2820639270 0.9741106499 0.0780810083 0.262342765 3.725171e-01 562 0.6476774705 0.6113039821 0.3379194401 0.283364764 2.244343e-02 563 0.8264224837 0.6389387369 0.2133443239 0.304230640 6.614637e-02 564 0.1684982514 0.5108453108 0.8730994116 0.952562007 2.711628e-01 565 0.4681478492 0.7290914275 0.6655910178 0.059033521 2.882402e-01 566 0.9135101677 0.7844511706 0.6940691071 0.521119098 6.174569e-01 567 0.5153865137 0.6280066038 0.7529852346 0.926406451 3.512761e-01 568 0.1224220723 0.1532868505 0.6064663581 0.035015481 9.069524e-02 569 0.3729660993 0.3849294826 0.4162248687 0.170865999 3.499442e-01 570 0.6174877922 0.4830182882 0.8551577611 0.948893960 3.666638e-01 571 0.7798820683 0.8074540063 0.0701762645 0.811303859 6.321102e-01 572 0.8141681084 0.4763889555 0.6402247904 0.118680977 8.215278e-01 573 0.8072200266 0.4033720838 0.5565765412 0.933369923 2.928074e-01 574 0.4192860171 0.5062515556 0.9369511518 0.923694299 6.381643e-02 575 0.3336501552 0.7181739374 0.3037285402 0.663063108 3.577619e-02 576 0.2333190406 0.6384027032 0.1179364619 0.621289150 7.826201e-02 577 0.6390773912 0.0315628725 0.0034079296 0.544522583 6.646238e-01 578 0.9833549089 0.5804860534 0.4200558870 0.108356553 9.020294e-01 579 0.2008158572 0.3473298543 0.3942718485 0.882981715 5.132987e-01 580 0.6873001435 0.5002243656 0.8098510485 0.548252749 7.759410e-01 581 0.5232526087 0.5436942442 0.7394590937 0.986454384 7.662074e-01 582 0.7243103618 0.4520305509 0.1652481027 0.533395649 2.761340e-01 583 0.5373778436 0.6290667017 0.0718271255 0.221661672 5.003754e-01 584 0.2417254543 0.3433562904 0.8498937809 0.864886062 8.246796e-02 585 0.0760574646 0.7703167831 0.6272670222 0.711507613 7.299976e-01 586 0.6699323892 0.2712414272 0.8532035290 0.287744463 9.483736e-01 587 0.5009339042 0.9915771615 0.3771723702 0.246797540 7.356113e-01 588 0.3031111252 0.6791734453 0.1358731689 0.205903129 2.677441e-01 589 0.8653934507 0.3075919577 0.9591189560 0.164791528 3.823351e-02 590 0.9512251944 0.8444743052 0.6169150190 0.151851988 2.594963e-01 591 0.2299444869 0.2699593364 0.0633073712 0.337680844 8.593229e-01 592 0.8090767509 0.3491266835 0.9062857677 0.731222640 1.136799e-01 593 0.8875121858 0.7116214407 0.3816257541 0.229801290 2.956320e-01 594 0.0956568804 0.0888715349 0.8685165911 0.571338476 9.496462e-01 595 0.9680835889 0.6196359964 0.6548132098 0.281034419 4.816263e-01 596 0.2339390421 0.1134912746 0.1338451568 0.632273507 6.795302e-02 597 0.1876290324 0.6431201152 0.6688683571 0.352891829 7.041394e-01 598 0.1103235064 0.7108283620 0.2381389178 0.177926907 5.603788e-01 599 0.9005565967 0.7975010527 0.6413422818 0.317935640 1.315685e-01 600 0.0959906711 0.2517449632 0.6953449131 0.886165070 3.520248e-01 601 0.1814462831 0.2464492123 0.2428896630 0.629397307 3.648666e-01 602 0.7031895248 0.8790988030 0.5745146840 0.192540054 8.839200e-02 603 0.5910929132 0.8323000772 0.2072549914 0.794825767 7.403007e-01 604 0.9162794969 0.4879544913 0.5999331572 0.553021319 6.472658e-02 605 0.9888230127 0.2273209461 0.1320640503 0.583122615 2.292195e-01 606 0.4466462396 0.7951611818 0.7163238716 0.111165432 6.223114e-01 607 0.4109430443 0.4526698415 0.6658871539 0.055975484 5.476906e-01 608 0.1328400464 0.7751484194 0.8014833727 0.026586966 6.009309e-01 609 0.7664368560 0.6650214796 0.8394974649 0.796361954 5.203079e-01 610 0.1854454621 0.1539397850 0.5727987136 0.859107810 9.724846e-01 611 0.7509588073 0.2780542122 0.1968538989 0.138429189 7.375585e-02 612 0.9074370672 0.3251694136 0.1056423029 0.376905116 9.260506e-01 613 0.1178869291 0.1247338115 0.9974537201 0.091709557 3.123266e-01 614 0.0469024812 0.6889863366 0.6949616608 0.587234502 1.082442e-01 615 0.1484412735 0.5968929187 0.1998363563 0.185659013 6.482614e-01 616 0.8210094008 0.0725176199 0.6460069709 0.372610050 3.817773e-01 617 0.5549458107 0.7114344193 0.7727056891 0.842564460 4.825940e-01 618 0.3284571527 0.1717800761 0.9150849401 0.928022968 2.964681e-01 619 0.5617234516 0.4060857871 0.5447506779 0.991730924 4.116851e-01 620 0.2218145537 0.6428353186 0.4462690975 0.180419530 9.495265e-01 621 0.6815757749 0.4869625089 0.7706935983 0.124916652 2.543316e-01 622 0.3976176258 0.4112871403 0.2232044286 0.601970080 5.856749e-01 623 0.9302335104 0.7058372160 0.2738621628 0.357624098 4.464640e-01 624 0.1203466251 0.6778112825 0.8039241112 0.283917557 5.086606e-01 625 0.5158565340 0.8589938758 0.7603616987 0.757245618 5.810929e-01 626 0.5419992688 0.1409501764 0.5972314011 0.378633605 1.498809e-01 627 0.4644107393 0.2047461346 0.4112030938 0.081525275 3.996346e-01 628 0.8369497349 0.7150264331 0.6943341750 0.449338492 4.065572e-01 629 0.1867417134 0.2349077109 0.5118668289 0.008123271 7.465035e-01 630 0.5425374825 0.7320022769 0.7225689276 0.056100357 9.743324e-01 631 0.9241157686 0.4882950098 0.5148334769 0.294380933 2.235191e-01 632 0.7712359671 0.9125933389 0.1133061245 0.017098532 7.013883e-01 633 0.0460593526 0.9790674066 0.5507777117 0.216266027 9.388441e-02 634 0.1461966210 0.3678647901 0.9606390365 0.256912482 9.466607e-01 635 0.8583672526 0.4514849845 0.5924144168 0.275959739 8.218458e-01 636 0.3796334988 0.5228201388 0.8796267982 0.914260088 8.577997e-02 637 0.9610019554 0.2582042993 0.2585091207 0.248356227 9.530522e-01 638 0.2226643402 0.5756969464 0.4010980607 0.370071233 7.605457e-01 639 0.8878771043 0.4456070380 0.1770016337 0.079971947 7.741485e-01 640 0.2048562940 0.9132214689 0.5876504485 0.911009921 4.087940e-01 641 0.1734649974 0.4730995838 0.8521742683 0.283892886 7.160428e-01 642 0.2770436495 0.4960478283 0.6533297694 0.337937349 3.102812e-01 643 0.9079919949 0.3234175995 0.1345435700 0.276223309 3.642992e-01 644 0.1404491214 0.3107680771 0.2690007351 0.785765260 8.150575e-01 645 0.7797426870 0.5068352411 0.9562994044 0.341920928 9.123921e-01 646 0.2014403057 0.3889138778 0.6121131913 0.920091983 7.444889e-01 647 0.6695721492 0.0883990279 0.2482306608 0.965957357 2.299187e-01 648 0.8657230681 0.7457466202 0.6249744541 0.324577653 3.874338e-01 649 0.5862377898 0.1959867699 0.2212410027 0.563064340 6.686219e-01 650 0.4096797309 0.1860773508 0.5328029690 0.314105657 7.770745e-01 651 0.0554621299 0.0790727723 0.3894717460 0.840018364 6.212918e-02 652 0.6932099925 0.3149603689 0.0017989841 0.561205384 5.865121e-01 653 0.0772926114 0.1772280850 0.9093107912 0.192656148 8.233684e-01 654 0.5872192709 0.9547599677 0.2526479778 0.381830367 4.776642e-01 655 0.2018798466 0.0876697104 0.1663716871 0.600859793 9.508464e-01 656 0.5674000036 0.0061985739 0.9192362998 0.882563850 8.127025e-01 657 0.2606946430 0.3112595235 0.6329974558 0.790751529 6.720512e-01 658 0.6728667193 0.1917230377 0.7566862372 0.909346010 4.726861e-01 659 0.6331451386 0.2484863934 0.0995883788 0.343220247 3.827711e-01 660 0.0108745436 0.6954743220 0.4698274387 0.426440760 6.104461e-01 661 0.2674112793 0.0800102965 0.2643972533 0.478243147 5.113606e-01 662 0.8316930223 0.2490380928 0.2804101042 0.903230454 1.032151e-01 663 0.1727495792 0.7230207766 0.5985144922 0.054255187 5.225881e-01 664 0.0849175956 0.5175191101 0.3752188238 0.669995113 5.585850e-01 665 0.0127395198 0.8534449597 0.3917715163 0.141239679 7.482148e-01 666 0.9716106884 0.9878049642 0.5612602045 0.210602174 7.658354e-01 667 0.8348956225 0.8170615705 0.3934594379 0.369965748 6.439354e-01 668 0.6081504258 0.4918447125 0.8814690269 0.436927830 8.248758e-01 669 0.9960278347 0.6785293575 0.2428395639 0.235090057 2.776699e-01 670 0.8508732293 0.2890700537 0.9828776065 0.441170447 6.066821e-01 671 0.0809824888 0.1729177088 0.0553947210 0.410132100 6.038980e-01 672 0.5399313546 0.3313864928 0.6883659216 0.780303753 2.586869e-01 673 0.6266353892 0.1214897714 0.1065400506 0.279638008 9.539392e-01 674 0.5492108434 0.1729518115 0.0980195184 0.091090638 9.093982e-01 675 0.2353585865 0.3663545211 0.2753355789 0.370521680 9.157402e-02 676 0.9186354124 0.1570131038 0.7628696139 0.732964899 9.533825e-01 677 0.7215894382 0.3251391754 0.7349442197 0.390790382 8.812004e-01 678 0.4072342522 0.5741532256 0.2547490085 0.101868185 8.243583e-01 679 0.0320119020 0.6594430758 0.6453604414 0.593536775 8.153255e-01 680 0.1372886975 0.5797297091 0.0693826037 0.581811009 8.454477e-01 681 0.2251470848 0.5863237504 0.2929798686 0.064685504 2.603856e-01 682 0.2438405859 0.7252879883 0.2253684865 0.775818338 3.057060e-01 683 0.0316104067 0.4859786753 0.8827559745 0.927763019 8.996976e-01 684 0.8333529353 0.8842396680 0.4077094197 0.951228141 4.519069e-01 685 0.9327697556 0.0735161284 0.8669178819 0.646814072 2.359719e-01 686 0.8194053553 0.0149510549 0.3264283007 0.027954239 9.877356e-01 687 0.4739673156 0.6126506121 0.2575328399 0.626472661 9.376579e-01 688 0.7064988560 0.4339322769 0.0102567382 0.463410470 1.281101e-01 689 0.3043627338 0.3500925838 0.7521409774 0.416593156 6.536726e-01 690 0.2842362507 0.1287338857 0.2068039351 0.479221793 7.736288e-01 691 0.3959755872 0.1325092788 0.0840746027 0.595622467 6.474641e-01 692 0.4561319870 0.7985444975 0.2368864699 0.953477525 6.616659e-01 693 0.0085370028 0.1395568491 0.1916677223 0.754163359 7.051648e-01 694 0.3519615321 0.7734501422 0.3515878150 0.644991686 7.748586e-01 695 0.1480507238 0.6036012371 0.1661285183 0.666506897 1.195510e-01 696 0.4659092973 0.5883147453 0.9563295639 0.460264088 9.507760e-03 697 0.2261838936 0.2115620363 0.1406688350 0.502033343 4.627967e-01 698 0.0966137387 0.7810345865 0.1228184656 0.399053896 6.973598e-02 699 0.0595416413 0.9259417355 0.4779730642 0.556131563 7.944641e-01 700 0.2826902184 0.2328596772 0.3735294535 0.980496759 4.502118e-01 701 0.6457913155 0.9648015301 0.0866789357 0.340338276 3.152243e-01 702 0.8387226122 0.9889767615 0.5429523443 0.467376266 9.904297e-02 703 0.3480575341 0.5784635388 0.8746951353 0.090592088 3.027450e-01 704 0.0110021131 0.7382813357 0.8855151345 0.097531161 6.932548e-01 705 0.4369894997 0.4602281952 0.1504359660 0.592339120 2.363977e-01 706 0.3866372849 0.9427196847 0.8209371674 0.750375597 1.012006e-02 707 0.2527379994 0.2612617402 0.4743072900 0.701019078 1.618566e-01 708 0.0489493115 0.5330059354 0.2273620877 0.057604318 6.559704e-01 709 0.1701454427 0.5906300214 0.1530095886 0.885532326 9.451794e-02 710 0.0542228029 0.4391671510 0.0454088813 0.130954660 3.661530e-01 711 0.9801739862 0.9737242053 0.3088543373 0.292188345 2.974664e-01 712 0.0955795809 0.7315077246 0.0739612088 0.306935967 8.830078e-01 713 0.4907403311 0.3944720011 0.7725880863 0.986870253 5.451626e-01 714 0.6269930771 0.0671702924 0.6396098298 0.257390226 4.399738e-01 715 0.8188843785 0.8974256085 0.2105948704 0.497911549 8.912714e-01 716 0.1428737247 0.7066940332 0.5905662815 0.013904960 8.051229e-01 717 0.3795925688 0.5371038395 0.6400907186 0.396177840 2.185821e-01 718 0.2576724272 0.2867634567 0.5625600184 0.227866938 6.552381e-01 719 0.0705480152 0.7199489921 0.4384352528 0.213859265 5.562379e-01 720 0.5952966632 0.1331510691 0.3311057268 0.812910554 4.576846e-01 721 0.2078502625 0.2657186741 0.1369717133 0.966520458 6.201260e-01 722 0.5607043237 0.7773463074 0.9060784536 0.136882777 5.718761e-01 723 0.1418856278 0.3193872187 0.0067140581 0.402681939 2.011022e-01 724 0.1668371137 0.6748321333 0.7357757199 0.286822234 6.788829e-01 725 0.6645078519 0.4957872450 0.6253226784 0.045322990 4.713916e-01 726 0.2852513075 0.0419426917 0.4170354577 0.144804272 8.927030e-02 727 0.5211206493 0.6694120474 0.2225864746 0.496572894 8.498413e-01 728 0.1769501129 0.6802692772 0.5119223034 0.810593840 3.094975e-01 729 0.6272501328 0.1008226273 0.0221058365 0.660919987 7.107244e-01 730 0.4807866269 0.4724463990 0.8551723175 0.946134662 5.702034e-01 731 0.0405958679 0.6419971804 0.8001623955 0.059512498 5.834628e-01 732 0.1040727708 0.4574132226 0.8362864284 0.119848038 9.500916e-01 733 0.1034192743 0.5646959795 0.4774509370 0.100162905 4.218027e-01 734 0.9854815747 0.9536970994 0.5731427802 0.445599995 9.751907e-01 735 0.5504544294 0.8856823838 0.5006169302 0.326138788 2.829155e-01 736 0.1668087766 0.0087919568 0.2469461611 0.124296868 9.120047e-01 737 0.6289759041 0.3003166930 0.0746609843 0.277244968 7.745181e-01 738 0.8146107760 0.7891051264 0.7982830841 0.843328346 1.065173e-01 739 0.7342906827 0.5856943072 0.3800514466 0.651758524 6.560058e-01 740 0.1165104364 0.4262742412 0.9380351442 0.720093509 5.285259e-02 741 0.4589043292 0.9510245435 0.1878450911 0.010018110 3.818362e-01 742 0.8540700742 0.4387043286 0.8535325744 0.704237403 1.438948e-01 743 0.9853149375 0.3135367928 0.0273535808 0.401024840 8.124945e-01 744 0.2686594662 0.0067920866 0.4991884257 0.925610454 1.121184e-01 745 0.2199555885 0.4688793917 0.0312401301 0.890368610 3.474478e-01 746 0.6150329802 0.3032249976 0.0330985945 0.160531911 5.085447e-01 747 0.5294507435 0.1677674388 0.2016299928 0.780310904 9.195844e-01 748 0.3173471964 0.3425450716 0.3296101303 0.231268557 1.281241e-01 749 0.7328500336 0.8980467783 0.3283071520 0.323327990 5.388319e-03 750 0.8194361259 0.9905332893 0.5591681972 0.266169388 4.770701e-01 751 0.6656111178 0.2108571122 0.1919852202 0.527454810 3.347841e-01 752 0.0899907222 0.1446414352 0.8986281594 0.806791155 9.988664e-01 753 0.4024253956 0.8001327894 0.4767216074 0.267760365 7.607286e-01 754 0.1910845076 0.3644843656 0.7766964245 0.061330716 9.895875e-01 755 0.5832469633 0.1375580644 0.6565976313 0.690765419 7.303636e-01 756 0.8330274366 0.1368968731 0.7017091217 0.198802563 6.753607e-01 757 0.1212478145 0.8072033660 0.3884129189 0.510344740 9.609224e-01 758 0.6087156688 0.3369593923 0.7562846150 0.510526375 5.928940e-02 759 0.7821644433 0.8510950310 0.9395253323 0.166566865 5.062242e-01 760 0.7225771444 0.0084626777 0.2886167157 0.375914235 1.782048e-01 761 0.4327145740 0.3436067994 0.0624468897 0.655956657 4.799398e-01 762 0.3311250603 0.5018027748 0.3175616367 0.909446071 3.785639e-01 763 0.5488906924 0.9179980736 0.5937463716 0.824942662 4.687987e-01 764 0.1809079153 0.1684568480 0.4394692946 0.091948393 1.571436e-01 765 0.3125010028 0.2808426481 0.4364514167 0.871625515 5.357669e-01 766 0.2322662107 0.4296523267 0.2301211185 0.984538529 5.416512e-01 767 0.8883634240 0.5374647570 0.0706931108 0.798453904 4.194775e-01 768 0.5146165742 0.2363249704 0.6942034576 0.255742336 3.743201e-01 769 0.3510731300 0.2868859540 0.2493360748 0.406186156 2.858131e-01 770 0.5866833304 0.5382801190 0.3648477907 0.102987665 7.186622e-01 771 0.5168380113 0.2537941472 0.5216529446 0.386116420 3.009853e-01 772 0.0169250246 0.6376643232 0.5035872550 0.192309255 4.536078e-01 773 0.9882035200 0.4582637991 0.6574109502 0.724014118 4.872410e-01 774 0.5653496098 0.4760491254 0.2371778297 0.042768712 9.784826e-01 775 0.5821441454 0.3042122070 0.0609040807 0.390714176 2.524162e-01 776 0.0911039554 0.8299246619 0.1350643106 0.867280200 1.776634e-01 777 0.1199192528 0.5218563776 0.5861633071 0.499435835 7.811131e-02 778 0.1269978655 0.3822219400 0.3625663975 0.180269022 7.917490e-02 779 0.5197857721 0.0951860771 0.5939297057 0.487396815 3.944825e-01 780 0.7007307441 0.3687405463 0.2572553901 0.813576587 4.809790e-01 781 0.3218411119 0.7246845297 0.7287631440 0.905135344 9.021088e-02 782 0.1232672483 0.4581358170 0.7231167776 0.155507295 4.767607e-01 783 0.8657550549 0.7917757533 0.2241408930 0.840674793 2.868034e-01 784 0.5233116525 0.3896034497 0.7217376744 0.472973718 1.765475e-01 785 0.0661836988 0.1935713969 0.0214482821 0.087035662 9.176547e-01 786 0.0389345712 0.2243677888 0.5614796390 0.309720548 2.056354e-02 787 0.0492720462 0.1595348492 0.8530885784 0.178120089 8.737463e-01 788 0.3390803235 0.2362157856 0.4276224766 0.455062811 7.755917e-02 789 0.3281026052 0.4601694574 0.0599775666 0.718546729 3.753534e-01 790 0.6873349790 0.4976751781 0.8280542898 0.994703966 2.440879e-01 791 0.5504687894 0.2988064871 0.7713529249 0.641040120 5.693950e-02 792 0.5503732006 0.5960831509 0.8664672766 0.476776771 4.819354e-01 793 0.3711241737 0.6343850996 0.8537406514 0.109487688 6.908167e-01 794 0.5504067924 0.1058719386 0.6400646684 0.081498371 8.541506e-01 795 0.6950991168 0.7604977151 0.5514039169 0.081682724 4.830404e-01 796 0.3357875820 0.1431916794 0.0349251456 0.263697140 5.391837e-01 797 0.2259819009 0.7031412621 0.2148530942 0.511179863 9.868655e-01 798 0.6837921543 0.2173088472 0.6941937325 0.415529779 2.726136e-01 799 0.0327904762 0.2515361288 0.3132338880 0.383568716 6.489722e-01 800 0.4708686613 0.9216939155 0.2693657901 0.127065991 2.182584e-01 801 0.6667835233 0.6971665192 0.1054696967 0.693743361 1.188034e-01 802 0.8530509556 0.9432144242 0.0041820426 0.169260422 3.253670e-01 803 0.3427545812 0.1718738738 0.7625045318 0.362998036 7.145535e-01 804 0.7191447199 0.8004429014 0.9849620434 0.050722711 4.945911e-01 805 0.9965520247 0.4413624234 0.0623347035 0.432994378 2.793668e-01 806 0.9527486961 0.5245378804 0.8925296222 0.037442494 9.853151e-01 807 0.8085598280 0.7501264329 0.1495426074 0.275200297 7.410981e-01 808 0.6196632015 0.7699690936 0.7003352877 0.750953684 6.796842e-01 809 0.1371374123 0.2860390744 0.4732657552 0.337846551 6.279401e-01 810 0.2705621868 0.1728939763 0.3674579775 0.646730392 2.963514e-01 811 0.9150904114 0.7387685184 0.4425446887 0.717890505 7.044907e-01 812 0.9508713693 0.2058457162 0.3327484170 0.048251316 8.233207e-02 813 0.7664615572 0.2933199112 0.2387260119 0.455275297 9.507067e-01 814 0.7263937038 0.5308477988 0.7668302804 0.151941190 2.872785e-01 815 0.3706638364 0.0815223935 0.6523769449 0.861204863 9.701118e-01 816 0.3731770006 0.1418519248 0.6558721811 0.392759632 8.911583e-01 817 0.8660431546 0.0405283556 0.2191681382 0.671238937 4.604006e-01 818 0.5390274378 0.1355421511 0.7730570342 0.392236311 4.018251e-01 819 0.8056316061 0.4232798924 0.8585525232 0.273011497 2.771170e-01 820 0.4778904547 0.8586307105 0.3877773187 0.301605025 2.120700e-01 821 0.3268022928 0.9401343844 0.7134420641 0.820058862 6.129209e-01 822 0.4336912178 0.6601184118 0.7966466683 0.753500970 8.045000e-01 823 0.4331766844 0.4773011776 0.2554806194 0.506299197 7.484734e-01 824 0.8422790130 0.9728820582 0.5530510051 0.674755441 3.098467e-01 825 0.1702686194 0.5197307284 0.1117506877 0.576723189 6.939165e-01 826 0.8912892947 0.8173142259 0.2254784491 0.990182979 8.775264e-01 827 0.5795565543 0.3450774932 0.6231800388 0.511314409 7.229131e-01 828 0.2747776266 0.3432068359 0.9651196841 0.361325060 9.342489e-01 829 0.2476471073 0.9399656593 0.7509542222 0.040544073 5.951181e-01 830 0.0888829241 0.2625810562 0.9616506596 0.164866655 5.282961e-01 831 0.0618546458 0.2170708573 0.6758510496 0.806005144 5.418574e-01 832 0.2215657665 0.5892768123 0.9687829807 0.970458879 1.469074e-01 833 0.0160984755 0.6008945550 0.2456119955 0.651282727 7.154888e-01 834 0.9552937921 0.4041952416 0.2487599570 0.636312764 4.222105e-01 835 0.6553695966 0.5982882574 0.4690349526 0.850788158 9.853207e-01 836 0.5880596407 0.1025756893 0.3663468168 0.602122332 4.146614e-01 837 0.2446371287 0.3886715963 0.7592353383 0.304655447 4.547841e-01 838 0.2204478385 0.9776127548 0.8441695466 0.342979762 1.793916e-01 839 0.3436731452 0.3158153961 0.8450364436 0.017197104 5.342961e-02 840 0.9654810687 0.6897880316 0.2408264768 0.725365726 6.414571e-01 841 0.7806982426 0.6866874089 0.1406750046 0.415963592 4.494836e-01 842 0.6681353534 0.9749579220 0.5696881823 0.270559045 9.334318e-01 843 0.6673664290 0.8776515799 0.1567756762 0.577515450 6.526554e-02 844 0.8577287272 0.8350734247 0.8539518388 0.599658105 3.774422e-01 845 0.8676475156 0.2449205024 0.7057895265 0.336051099 2.469455e-01 846 0.9010199986 0.7300883937 0.9259289976 0.107742261 7.782273e-01 847 0.7096090924 0.0396126274 0.1711437271 0.912411376 5.841550e-01 848 0.4057656860 0.9635527530 0.4667839229 0.717135843 5.471613e-01 849 0.7586153776 0.8188142958 0.7115409470 0.265018949 4.173710e-01 850 0.1514678800 0.4931847306 0.1066040788 0.420480186 2.265498e-01 851 0.8852276236 0.2273079150 0.6126720684 0.124443571 5.439975e-01 852 0.5744567150 0.2276047985 0.6053443069 0.305235157 1.352699e-01 853 0.7365684998 0.7416466051 0.1987565625 0.522348852 1.889310e-01 854 0.4682052871 0.3656761451 0.5374135601 0.834061932 4.192377e-01 855 0.6883175839 0.1932561044 0.1337969434 0.655146906 6.233718e-01 856 0.7630665472 0.1755269326 0.8618537828 0.065738074 4.840831e-01 857 0.9105806397 0.0796877367 0.0654560952 0.970875686 6.588444e-01 858 0.1453003632 0.8365933388 0.8908922726 0.261465481 1.285208e-01 859 0.3542107171 0.6549203501 0.8706293327 0.595211849 6.201236e-01 860 0.8858120129 0.6786385714 0.0019265474 0.633881345 6.488960e-01 861 0.3534604481 0.4462263004 0.3821332473 0.116964221 4.485467e-01 862 0.9199138735 0.0237773440 0.5560805255 0.789658329 7.332141e-01 863 0.9699242383 0.5838220422 0.0731530387 0.854077701 1.952940e-01 864 0.7197193392 0.7666043034 0.6545700596 0.109955078 9.903314e-03 865 0.0818659512 0.6813366392 0.4428950842 0.709426838 7.134775e-01 866 0.6979543704 0.4409124681 0.2596999193 0.435666425 7.401312e-01 867 0.7083249867 0.4796856840 0.3304600932 0.663771980 9.346799e-01 868 0.4236047838 0.7879075778 0.0797441986 0.362275834 1.116769e-01 869 0.9025010683 0.0370089954 0.0607495804 0.455845351 7.383802e-01 870 0.8385839090 0.8592968574 0.3819316330 0.732232572 6.211616e-02 871 0.1577102917 0.4951161523 0.2630529224 0.717813409 3.060167e-01 872 0.9411812874 0.2867955500 0.2457986970 0.908677576 3.361926e-01 873 0.3959288662 0.2671015316 0.0694725793 0.657614531 2.711497e-01 874 0.1749171666 0.1481003091 0.5796920711 0.745290652 5.726620e-01 875 0.9484079392 0.7067262111 0.3593763858 0.696661414 6.557866e-01 876 0.6690065281 0.8686357243 0.9274752026 0.429747088 9.805910e-01 877 0.9932561866 0.6987060872 0.7170277392 0.572124985 7.858925e-01 878 0.4792634952 0.2627820971 0.7591925235 0.129092884 3.584519e-01 879 0.3339846164 0.4879570440 0.5219738027 0.981418908 4.780838e-01 880 0.1384664588 0.6071906351 0.5598796897 0.989587379 6.047264e-01 881 0.7710127225 0.6588871563 0.7000522728 0.787040020 8.951891e-01 882 0.5183066048 0.6633623212 0.8556959080 0.823626497 9.873870e-01 883 0.2342772360 0.6825878853 0.9765392048 0.422077550 8.276469e-01 884 0.3819961718 0.0703841767 0.4545353968 0.190806217 4.993768e-01 885 0.9671158306 0.7996307290 0.2528239412 0.821535721 4.520912e-02 886 0.0136003532 0.6550489797 0.0971307924 0.569252931 4.415581e-01 887 0.6918485614 0.2938269705 0.3830597901 0.642045519 9.181501e-01 888 0.3883587052 0.5746545340 0.2149518812 0.544876507 9.811128e-01 889 0.9782044799 0.0871506219 0.1721066297 0.638855571 2.163524e-01 890 0.3884682686 0.8530711008 0.1081484584 0.618824126 1.981257e-02 891 0.1955370163 0.7554930146 0.4383497564 0.132186846 6.693339e-01 892 0.2558058237 0.8910853162 0.6054714422 0.106588792 8.727763e-01 893 0.8466363044 0.8384201061 0.7106585323 0.261846789 3.145544e-01 894 0.9117230456 0.7739696172 0.4066036423 0.836243367 9.512813e-01 895 0.6891804636 0.3827441628 0.1441087264 0.428895871 5.805177e-01 896 0.8616765880 0.7499236313 0.2040350866 0.042978100 2.002000e-01 897 0.9397967570 0.0891257052 0.7237532646 0.214750154 9.383528e-02 898 0.2836855208 0.5146681564 0.9748851419 0.309161969 5.718429e-01 899 0.7020970217 0.3102047946 0.6902426740 0.594432420 1.961699e-01 900 0.7543787274 0.9368917353 0.2379066127 0.744932082 6.898677e-01 901 0.7100764371 0.6819450008 0.5354397860 0.666972563 2.406382e-01 902 0.0359942482 0.0883137148 0.1095670776 0.368549353 8.317809e-01 903 0.7027321772 0.3958795723 0.8308744195 0.667122809 2.450692e-01 904 0.1076769722 0.0905030516 0.4746082048 0.579430669 3.238268e-01 905 0.5423400379 0.2575905705 0.7975925563 0.839304272 3.700970e-01 906 0.0559626976 0.7763351945 0.5180712640 0.131845527 3.509410e-01 907 0.7400617898 0.5512401585 0.2532550723 0.822768779 8.241569e-01 908 0.4324181397 0.7704832107 0.2057194079 0.622663645 8.665828e-01 909 0.6370878578 0.7114341015 0.0513802432 0.275731774 3.926777e-01 910 0.5504355514 0.8300711268 0.7932873622 0.971701424 4.464620e-01 911 0.7207374696 0.9681204916 0.1152818713 0.205870625 2.824269e-01 912 0.5694926230 0.1999330653 0.9196312737 0.321336865 1.276558e-01 913 0.4082449640 0.1262337428 0.7942077753 0.763436640 5.887029e-01 914 0.4719120276 0.9897765913 0.5567615679 0.235130594 4.128340e-01 915 0.7132404083 0.5091732969 0.2665097436 0.872430083 9.947829e-02 916 0.1123925871 0.9880523295 0.7234942676 0.710679841 9.196550e-01 917 0.2404889199 0.3670683077 0.8855743406 0.572891420 9.907376e-01 918 0.1648135930 0.7147001112 0.2808103329 0.826797771 8.433905e-01 919 0.8316822357 0.3342582213 0.9897506847 0.129931385 7.228052e-01 920 0.1540134929 0.4937861396 0.6946977631 0.642777493 9.363438e-01 921 0.4543502852 0.8231381234 0.5084731737 0.392673834 9.929688e-01 922 0.3393653259 0.3207413722 0.2487738428 0.036524708 1.525176e-01 923 0.3792336930 0.4274902237 0.3213149323 0.111199606 9.098137e-01 924 0.5646767376 0.5653868199 0.1198367970 0.422641320 3.241015e-01 925 0.1275393234 0.9610971808 0.8526588532 0.547694317 1.969815e-01 926 0.8284457396 0.0512343606 0.0969893662 0.565572405 6.216574e-01 927 0.7716819895 0.4303607254 0.1500063352 0.585179174 4.304587e-01 928 0.3144028510 0.9650541637 0.2011498599 0.832636225 7.382756e-01 929 0.1932181078 0.6512502013 0.2314884884 0.927042429 4.021827e-02 930 0.7715724281 0.4867733018 0.1722336062 0.592240821 4.878809e-01 931 0.7515096315 0.5237351716 0.9337820550 0.175378338 5.437635e-01 932 0.8508526227 0.0862903530 0.7518694196 0.392386721 5.070725e-01 933 0.1897171729 0.1816355581 0.4939800212 0.508380143 2.517736e-01 934 0.5888304978 0.4325272697 0.0950893415 0.437182602 7.073556e-01 935 0.2581488835 0.4775416015 0.1880199569 0.091574369 9.642912e-01 936 0.6145524746 0.8077542640 0.8791012922 0.404682925 2.885075e-01 937 0.5502511666 0.2100446864 0.5053161618 0.570401419 7.775891e-01 938 0.1422360775 0.3714883709 0.1705700466 0.292427428 5.072701e-01 939 0.8544583651 0.3779014812 0.7818198670 0.209379298 4.591110e-01 940 0.2279679456 0.1332134781 0.7430715247 0.263717869 9.951537e-01 941 0.4878764427 0.0266515280 0.0557566388 0.827095761 6.739393e-01 942 0.8901376666 0.2336050419 0.1721143238 0.955066324 7.380528e-01 943 0.1214600294 0.4506204431 0.7488469749 0.722901567 9.061563e-01 944 0.7492403670 0.2964121955 0.4126732238 0.134044858 4.084631e-01 945 0.6883822854 0.3348744807 0.7348977642 0.435535827 3.775312e-01 946 0.9731870238 0.6234922898 0.6055679652 0.011814762 8.236566e-01 947 0.9803779942 0.0189021132 0.1514285270 0.779741396 7.189808e-02 948 0.5742041636 0.4015159667 0.2583783229 0.319161324 3.810088e-02 949 0.9520442467 0.3840122933 0.3215753071 0.721180730 8.162514e-01 950 0.4607166387 0.5502329234 0.0424744408 0.470151842 4.324335e-01 951 0.7038704425 0.7253101626 0.1991410654 0.663179845 7.651094e-01 952 0.2755301245 0.1031086305 0.9857126495 0.406166487 7.041206e-01 953 0.6193735383 0.6403544305 0.4304613241 0.992695211 5.010757e-01 954 0.4249681248 0.5705294732 0.2173429558 0.736243507 2.958308e-01 955 0.3728593271 0.9821952912 0.7585317735 0.592364626 4.824808e-01 956 0.7996276352 0.7557521961 0.6896903608 0.032121529 9.333821e-01 957 0.2530086664 0.7840213208 0.9472654869 0.663430023 7.808786e-03 958 0.0272058961 0.2785600286 0.3803765716 0.835774167 8.285646e-01 959 0.2937615011 0.0414462413 0.0293130069 0.170471911 5.053126e-01 960 0.7656114011 0.0089509301 0.2224720593 0.078633697 2.267945e-01 961 0.3921685980 0.2418958903 0.9058356252 0.499363041 4.301341e-01 962 0.3203731477 0.4558730088 0.9356109097 0.637428897 4.638048e-01 963 0.5701269405 0.6541033043 0.4553435929 0.659193589 7.637788e-01 964 0.0837780091 0.9852263576 0.2699921329 0.889402745 7.838773e-01 965 0.7902619520 0.1120501796 0.0643302598 0.015220200 7.573140e-01 966 0.7188133628 0.7126189214 0.5146951526 0.265695591 2.805009e-01 967 0.8039738089 0.0866877660 0.3811699026 0.007797114 9.017973e-01 968 0.6556652877 0.4472971286 0.0009898420 0.250218173 8.946227e-01 969 0.6478033902 0.0806381418 0.8741640016 0.686927262 3.710893e-01 970 0.6908786618 0.6214244557 0.5556211434 0.115327901 3.217848e-01 971 0.6140362916 0.7585889928 0.4772790568 0.943758306 5.007378e-01 972 0.9190024359 0.9279428197 0.9869498806 0.354942342 7.683242e-01 973 0.1376092818 0.5861151936 0.9461021069 0.957316337 1.416249e-01 974 0.4968745462 0.1127922863 0.0896480761 0.140535582 3.406880e-01 975 0.5084140948 0.6140513800 0.0141397875 0.618412951 2.553456e-01 976 0.2373713481 0.5146937706 0.3362690192 0.534919100 2.279837e-01 977 0.5096209319 0.5837802538 0.4960927456 0.120788795 3.330714e-02 978 0.1317708287 0.6946347281 0.8838458196 0.551703196 8.626118e-01 979 0.4485086391 0.4274698249 0.3073356152 0.731657210 1.683407e-01 980 0.6195011809 0.9784419455 0.3550067239 0.086869737 8.103572e-01 981 0.4437196946 0.3406247022 0.4528926273 0.325831012 9.969143e-01 982 0.0834825563 0.0264296336 0.8081067060 0.268322631 2.651312e-01 983 0.7067420189 0.3201212729 0.8177060552 0.896760443 6.064301e-01 984 0.3496549178 0.1091470283 0.1311323550 0.889285605 8.667603e-01 985 0.9456903765 0.3948074104 0.0551960305 0.270541456 1.841105e-01 986 0.9264556284 0.0210456059 0.7544529487 0.329619684 1.666222e-01 987 0.5862394304 0.6943669352 0.3462126998 0.527938245 4.539635e-01 988 0.9264138332 0.9935972001 0.4456170811 0.276350456 1.458880e-01 989 0.7399739220 0.6072702506 0.5072083352 0.379948666 2.127185e-01 990 0.0597490836 0.7014014195 0.8693522580 0.502023081 1.920523e-01 991 0.2525925478 0.4642360706 0.5797812503 0.206372628 2.255902e-01 992 0.5294484263 0.5832591089 0.9141128107 0.763919299 4.386716e-01 993 0.1451663070 0.9450887532 0.3267974902 0.163087269 4.304320e-01 994 0.5973799566 0.6611484769 0.2045684843 0.530934318 9.091520e-01 995 0.4801052473 0.1244018658 0.6161254332 0.985765266 4.358157e-01 996 0.8676580756 0.6908842688 0.2463870917 0.558830278 8.897796e-01 997 0.8354824937 0.7563201888 0.4218094908 0.991251539 1.910084e-01 998 0.5086493925 0.5916594807 0.7986441688 0.352129334 9.052197e-01 999 0.7984968482 0.6484692944 0.1080854419 0.114980506 4.598162e-01 1000 0.3461663174 0.5929790989 0.4973262595 0.372802859 6.241228e-01 i j k 1 0.9087631686 0.5128349718 0.433474417 2 0.5681274838 0.7094579476 0.868427460 3 0.3827259571 0.5897734014 0.180360822 5 0.8411586101 0.3029566994 0.585858709 6 0.6222287491 0.1019961545 0.630805569 7 0.5166449379 0.2911676636 0.261239435 8 0.5022442560 0.6165078580 0.410738634 9 0.3282486817 0.3663124072 0.802557074 10 0.8184540244 0.3216555105 0.994237013 11 0.5606055886 0.5415306007 0.117883356 12 0.1668717416 0.3277200800 0.320070869 13 0.9821718861 0.3913873483 0.735874106 14 0.1250124935 0.1662794249 0.617329021 15 0.4052526173 0.9588967944 0.125180201 16 0.3963778000 0.4164709412 0.982822154 17 0.7747443833 0.2370111805 0.867924212 18 0.4656205408 0.9236596262 0.231345516 19 0.4267182045 0.0398125688 0.276681161 20 0.4886503392 0.1524191860 0.040110499 21 0.7435912108 0.9295226706 0.971871639 22 0.3373999882 0.3702988313 0.919676143 23 0.5863423934 0.8232238216 0.723063651 24 0.4206146235 0.6212165076 0.282144273 25 0.1873300616 0.4141433262 0.553026780 26 0.8554507741 0.1028212600 0.855329942 27 0.6119108447 0.8649808620 0.325192982 28 0.2997493446 0.3446830176 0.702674859 29 0.6681878306 0.4829978379 0.869019716 30 0.5278012829 0.6876936750 0.849721408 31 0.7944821278 0.2671868550 0.872588813 32 0.3656178131 0.3114310633 0.124459583 33 0.9967463582 0.1866228166 0.459630596 34 0.3590255990 0.9903327457 0.988608231 35 0.2418361048 0.8444164961 0.473984825 36 0.5806110287 0.4397265683 0.489710954 37 0.6584511327 0.6120709099 0.122655185 38 0.9218367229 0.5005969931 0.271223680 39 0.8944407739 0.8447717724 0.088994969 40 0.7279412807 0.1609291413 0.455329117 41 0.2609252520 0.8845530343 0.210940112 42 0.1416404401 0.3299012582 0.467406417 43 0.2746811195 0.7520817884 0.792780091 44 0.0553447758 0.3767659578 0.026498032 45 0.5352210163 0.0118170890 0.846525728 46 0.0120629487 0.0339999525 0.166745803 47 0.4314778787 0.3114104606 0.714891932 48 0.2245725456 0.1469827355 0.018858860 49 0.7272778642 0.5995649623 0.449367810 50 0.6059436901 0.7773033865 0.684086278 51 0.4036247542 0.7489980566 0.366653045 52 0.5969315961 0.5205937014 0.886234621 53 0.0006523626 0.4913336923 0.175098365 54 0.2632054156 0.9477295359 0.419257508 55 0.3361853021 0.3503014820 0.565365737 56 0.3226710237 0.5014365071 0.358642312 57 0.4974331714 0.7017149401 0.191931669 58 0.1178657564 0.4157309311 0.556759141 59 0.3016790133 0.2646031538 0.755432571 60 0.0308654625 0.2068238084 0.846652366 61 0.4426214227 0.3930100391 0.313803358 62 0.7519367803 0.1991266685 0.373874883 63 0.0827221849 0.4457892301 0.021788683 64 0.8904469314 0.6041067331 0.965939397 65 0.2234720613 0.1910297391 0.286601910 66 0.8156521297 0.6457851853 0.387838364 67 0.4547833218 0.5390125816 0.146064876 68 0.2293814677 0.0510656524 0.234307113 69 0.4554125951 0.0007678478 0.579237812 70 0.9778966792 0.6642413577 0.981427724 71 0.4937450592 0.5499402732 0.159018812 72 0.1111480240 0.0701609459 0.231798086 73 0.4515543673 0.0222580845 0.169742819 74 0.3860736378 0.5143827731 0.339853824 75 0.6689951743 0.8895948618 0.331853422 76 0.5848921356 0.2110785230 0.208752149 77 0.8040093554 0.5435449358 0.155490631 78 0.8703865418 0.0060005118 0.535063164 79 0.6247375568 0.6896314544 0.220268407 80 0.5502351066 0.9226864551 0.188979229 81 0.8907975026 0.1401077805 0.012584140 82 0.0914727584 0.8452462007 0.567303002 83 0.1601004598 0.7238995091 0.347789467 84 0.3930475307 0.8140243778 0.828480477 85 0.2362872674 0.5648667500 0.536917268 86 0.0663146342 0.7406738328 0.967070757 87 0.6229225849 0.2372979226 0.681185639 88 0.5117010218 0.3952996521 0.237138671 89 0.8064635289 0.3335397751 0.565703884 90 0.3967762347 0.2208842041 0.354194236 91 0.6337079392 0.4096777418 0.079051379 92 0.1203049868 0.9191357759 0.658497361 93 0.9676421033 0.2858615983 0.453540877 94 0.2220023738 0.6622591268 0.935944299 95 0.0554578195 0.1503409792 0.458454961 96 0.0830594508 0.2259703281 0.460871697 97 0.8682666586 0.6073553285 0.618568527 98 0.6663306805 0.8752778720 0.774079820 99 0.4043017544 0.8350057611 0.125036883 100 0.1059333733 0.4167972640 0.458615416 101 0.6536240370 0.2890427141 0.524334089 102 0.6261212248 0.5107074548 0.048700516 103 0.2947792842 0.0883161519 0.316675600 104 0.6815966177 0.0293583267 0.242670482 105 0.9335999803 0.1404198715 0.179384844 106 0.6278110058 0.7190329526 0.200458986 107 0.5040061027 0.5368715730 0.488505151 108 0.5013903244 0.8250459556 0.930052144 109 0.1733192850 0.1501131626 0.766793029 110 0.3508784429 0.2671870328 0.727526051 111 0.1484719319 0.1510998618 0.631773586 112 0.0966769739 0.4766372084 0.924724548 113 0.7525902183 0.5807615130 0.878133470 114 0.8149411348 0.5931538984 0.529054842 115 0.9948693211 0.5770180277 0.497331441 116 0.8509829661 0.8519616781 0.331109564 117 0.1651958560 0.0783651082 0.222836789 118 0.8542163239 0.9436419141 0.642416314 119 0.2957376034 0.5372309394 0.328969282 120 0.1680100688 0.3125867224 0.536918208 121 0.7444190066 0.4906409730 0.539057107 122 0.9736629406 0.3209659122 0.420134468 123 0.4210970441 0.0146621745 0.281792870 124 0.4054784656 0.5823995634 0.021190652 125 0.1213328205 0.1566025014 0.499803931 126 0.3509171081 0.2642908753 0.828844704 127 0.8019870594 0.3684905737 0.597643980 128 0.3392660415 0.5528952766 0.362731453 129 0.2755800916 0.4141004644 0.263369496 130 0.0099175237 0.5354448098 0.867901461 131 0.5877318792 0.7913212397 0.356186446 132 0.7294213187 0.9341020375 0.426399379 133 0.3597076745 0.8540280554 0.690510602 134 0.1923128793 0.8539461282 0.470327384 135 0.4146025018 0.3285077966 0.174154247 136 0.2408502251 0.7539579268 0.974151978 137 0.6713664781 0.7490618394 0.018064682 138 0.5027397233 0.0004517024 0.371284921 139 0.1293624879 0.4354867004 0.719204444 140 0.1697100580 0.2979691955 0.825164750 141 0.1851382013 0.0322515192 0.209856055 142 0.9614997103 0.3880613439 0.210859042 143 0.2341886915 0.9113518861 0.569904638 144 0.3903274005 0.0529591336 0.977473462 145 0.5128328698 0.8339039281 0.460075917 146 0.6121268612 0.1546220388 0.120337063 147 0.5986534422 0.2321277794 0.255277535 148 0.2247179085 0.7172596608 0.901589938 149 0.7513462589 0.4561471040 0.682734193 150 0.4696188967 0.7050004448 0.276803780 151 0.9800768141 0.1836125506 0.087107321 152 0.9729995320 0.1371118471 0.786187609 153 0.0847310543 0.6312713376 0.981879490 154 0.5851558545 0.6476236130 0.915172418 155 0.0922475369 0.4266638111 0.594584180 156 0.5917444066 0.7326848062 0.536128583 157 0.8340488868 0.7819989014 0.743682370 158 0.6174082272 0.5634941428 0.103459783 159 0.1671749738 0.6828045091 0.075492039 160 0.6154925302 0.9422851873 0.305467341 161 0.6246360233 0.6056861058 0.008500006 162 0.6745371576 0.6684385163 0.103568461 163 0.1869240811 0.0430012536 0.847895277 164 0.6141133632 0.3480020654 0.071074953 165 0.7515403559 0.4299957363 0.545414267 166 0.4849006436 0.8814194989 0.211329428 167 0.2106576399 0.7893810470 0.213353900 168 0.5196030815 0.7781596312 0.544735801 169 0.1838931392 0.5161872935 0.615889838 170 0.8333251288 0.7455936016 0.366780687 171 0.0395859268 0.8005671275 0.916855233 172 0.4805583220 0.9664080332 0.414999548 173 0.5490431390 0.1338877140 0.664842228 174 0.0264012944 0.1551172663 0.822628885 175 0.0048946154 0.9222773779 0.955932898 176 0.9043120472 0.1706953272 0.072777700 177 0.8809468760 0.6098237170 0.719657137 178 0.8611115997 0.3326158209 0.364571405 179 0.4787534853 0.9453399039 0.835941072 180 0.2313803730 0.2082363386 0.499824949 181 0.6148180722 0.6784512543 0.777743101 182 0.0572387041 0.7870746215 0.120125221 183 0.8088287492 0.8559843907 0.259347438 184 0.9785300081 0.8136936259 0.709260552 185 0.3395234370 0.9604597690 0.022556264 186 0.2212379947 0.5665022719 0.951045089 187 0.3356777702 0.3005251158 0.084501502 188 0.3548264690 0.9781491724 0.799717161 189 0.7405867146 0.8685159674 0.821510137 190 0.0839215950 0.2648300638 0.943535396 191 0.8840797534 0.4699844518 0.236652090 192 0.8545890038 0.1840025745 0.977806848 193 0.5177294672 0.6102390429 0.245517909 194 0.8757470779 0.6909048136 0.651942725 195 0.6882468495 0.0161601556 0.784394057 196 0.1693982442 0.7199928972 0.756506318 197 0.3532229802 0.4630767726 0.897241289 198 0.3698309476 0.2420675876 0.738088635 199 0.1395692111 0.9148447975 0.002973847 200 0.5528753118 0.9722143693 0.530145853 201 0.0911104653 0.3148403978 0.858897036 202 0.1397922952 0.1736123802 0.794187781 203 0.9248953597 0.9052255033 0.426479524 204 0.3468633790 0.9629188555 0.392073041 205 0.2623246550 0.5600378469 0.582281488 206 0.5293257805 0.3108575726 0.160060341 207 0.5093579234 0.5014171167 0.736309079 208 0.4495433341 0.4862429700 0.392430910 209 0.9123518646 0.3051007891 0.203720998 210 0.1214102015 0.7437134380 0.704168370 211 0.7131768700 0.4649773457 0.543138901 212 0.9961626902 0.9921279163 0.345299343 213 0.1603799369 0.6949981558 0.597703475 214 0.6117657335 0.1799172421 0.127134755 215 0.2350276292 0.0418957674 0.210518366 216 0.3027018649 0.2641500751 0.995105395 217 0.7299060032 0.6030209041 0.274804636 218 0.6489603042 0.5311673591 0.184956109 219 0.2516879274 0.9647897545 0.341185981 220 0.2851197992 0.1207269041 0.697508165 221 0.5125428722 0.9168800218 0.117690779 222 0.0346664493 0.1519337283 0.779520737 223 0.7842136300 0.0845155257 0.978813627 224 0.3757599506 0.8362768008 0.939809005 225 0.5080087527 0.5440474791 0.235630636 226 0.7891359089 0.2309179760 0.190111594 227 0.6982576286 0.2912786996 0.414994985 228 0.4591538832 0.1998717184 0.226559677 229 0.0087905687 0.1122070975 0.679795615 230 0.2509475215 0.6780429292 0.506995059 231 0.0223060360 0.8224451512 0.976316434 232 0.1185056434 0.3756171942 0.999038306 233 0.0395731088 0.6464639278 0.744022369 234 0.5190228012 0.1079823810 0.281804436 235 0.8141576792 0.0351354424 0.302080039 236 0.2354405455 0.3621697398 0.462507161 237 0.1280833837 0.0429123768 0.452132628 238 0.9522720049 0.5174656950 0.667757805 239 0.8626499793 0.6394905939 0.116929316 240 0.2211448003 0.4862877068 0.187651631 241 0.2826683724 0.7685764139 0.427660304 242 0.2217451246 0.3901322596 0.527077607 243 0.9123899231 0.3484480807 0.940976249 244 0.7760570277 0.0196964843 0.994308030 245 0.7245896910 0.8625496353 0.669495775 246 0.3545963238 0.4262103755 0.925164024 247 0.8029720054 0.3310154788 0.190491454 248 0.5781933405 0.9248862842 0.819874573 249 0.7588987695 0.3347283669 0.723955127 250 0.4717798149 0.6719663588 0.033997452 251 0.5023258904 0.3928283551 0.863187823 252 0.8355372436 0.0686451662 0.909715006 253 0.8593941671 0.0926849283 0.817367764 254 0.5077188036 0.3954995340 0.768992276 255 0.2380844436 0.0159355055 0.582561954 256 0.5875904860 0.2722851331 0.175310384 257 0.4994263807 0.5650870153 0.936873940 258 0.0801673585 0.1591223918 0.240441793 259 0.5156380439 0.7538698432 0.836027631 260 0.2213389149 0.7571562580 0.967936541 261 0.5441098993 0.4593958042 0.259123230 262 0.8928442860 0.9421351061 0.866291614 263 0.6189724577 0.5877450497 0.485781986 264 0.7953684707 0.0485105908 0.582787094 265 0.7586312888 0.0050816601 0.654969776 266 0.9054281176 0.5962432623 0.478027690 267 0.9187670490 0.3565554691 0.993413433 268 0.2827955044 0.6572014024 0.191259727 269 0.7256454087 0.0734363843 0.191539427 270 0.7012636382 0.3691736532 0.951903309 271 0.7524833072 0.7902583296 0.406234259 272 0.1661315192 0.1737899601 0.991651452 273 0.8801912477 0.6872708595 0.278342104 274 0.1365374811 0.7761638940 0.887483506 275 0.7663580186 0.1873531982 0.729194304 276 0.6235552917 0.8355714106 0.314209814 277 0.5843322033 0.5417277587 0.167233856 278 0.8195801824 0.0575507390 0.816330421 279 0.4727422609 0.7954525829 0.935344556 280 0.4483542065 0.0807209269 0.061240495 281 0.3400470598 0.2865212108 0.406699197 282 0.9732844774 0.0074201201 0.504856665 283 0.3633071035 0.0794652763 0.697210894 284 0.8816565587 0.5742267463 0.926123858 285 0.4987751339 0.8233126488 0.320464924 286 0.3310508910 0.7643538795 0.709534760 287 0.2243376386 0.9196181563 0.627416339 288 0.7893124865 0.4731599311 0.791820529 289 0.3402598179 0.5928448008 0.836453283 290 0.2806643799 0.9530142890 0.190024565 291 0.8866525306 0.6742309285 0.305205277 292 0.6935368618 0.0542066889 0.255308195 293 0.9925473691 0.3263957959 0.660519107 294 0.7840081144 0.7939104903 0.554709732 295 0.8879083896 0.9838657875 0.047163502 296 0.9471069211 0.8180908239 0.667315026 297 0.7652483105 0.5011755559 0.672978379 298 0.1034767593 0.4843148482 0.765021885 299 0.5173658554 0.0807071435 0.149461499 300 0.2892686401 0.4461832349 0.774942254 301 0.4666807710 0.0640776940 0.937765861 302 0.1666596923 0.7189921502 0.176432163 303 0.6778312610 0.1221541967 0.505464738 304 0.7125912458 0.8826425248 0.330806704 305 0.2068997175 0.8807366712 0.525438205 306 0.8083262281 0.8745090137 0.359391946 307 0.0551950403 0.6054255778 0.010514143 308 0.1101146743 0.2348861066 0.428129612 309 0.3846570568 0.2119303760 0.407141645 310 0.9082448669 0.6626014228 0.642785348 311 0.5171715580 0.4968071438 0.974462328 312 0.4125887151 0.7784681309 0.425039892 313 0.5129293425 0.2333928708 0.442837440 314 0.3842024251 0.2738116076 0.910525002 315 0.8241676399 0.2300581113 0.871515020 316 0.7714029867 0.2671443268 0.820216401 317 0.1911138035 0.9620553681 0.130298762 318 0.9307477702 0.0430600131 0.650889409 319 0.8699085701 0.5043442317 0.645747940 320 0.1354798172 0.6577726228 0.614540099 321 0.9219136874 0.9839676502 0.527190544 322 0.2234225660 0.1896401674 0.277660568 323 0.7306426747 0.3842383027 0.147647479 324 0.1554096898 0.5335272748 0.714302043 325 0.7123912408 0.0143899969 0.362525354 326 0.4453180153 0.3131151772 0.545730561 327 0.6089746670 0.0491327595 0.556184235 328 0.7024399380 0.4329056886 0.196519810 329 0.7986177169 0.9928155858 0.675117668 330 0.9083080657 0.4011237149 0.939470744 331 0.2855855885 0.9352167114 0.064763133 332 0.0581693540 0.3703997759 0.489970324 333 0.2188624633 0.1252656823 0.082836872 334 0.3849162261 0.0405746803 0.599802457 335 0.5753972360 0.5074641234 0.025409097 336 0.8585140959 0.9870252733 0.960869779 337 0.3733973808 0.6797194134 0.456802760 338 0.4922717544 0.2692998312 0.738611541 339 0.7769749579 0.4674859173 0.649227513 340 0.9744688380 0.7548743489 0.444152255 341 0.5815969589 0.1518399711 0.602368516 342 0.7353012746 0.9282031991 0.711713375 343 0.9093837244 0.2169220045 0.387050143 344 0.2522708455 0.4072291572 0.123953464 345 0.1319291440 0.6047342650 0.586918667 346 0.4362283603 0.6418692195 0.057993301 347 0.7414943012 0.5013274332 0.708908894 348 0.4571445726 0.3489230359 0.690105600 349 0.1595911114 0.6860206951 0.762169352 350 0.4302436933 0.3642897324 0.242127506 351 0.5977981715 0.0813042349 0.103939542 352 0.5270542719 0.6627145151 0.735818434 353 0.6737627659 0.0368648355 0.026491203 354 0.9854462768 0.7834856187 0.373826430 355 0.4471681069 0.1585723683 0.202335776 356 0.4414274348 0.3658822568 0.603922377 357 0.3329481015 0.8038419769 0.171805945 358 0.3547660108 0.5783449381 0.172469585 359 0.5894069702 0.7482325078 0.388472585 360 0.0325384494 0.1603930925 0.764646099 361 0.4140505295 0.9774947141 0.973752263 362 0.3705947967 0.7234468276 0.418361759 363 0.1936284041 0.7779606243 0.166575791 364 0.2225097425 0.1565632462 0.553050242 365 0.2961740468 0.9200274399 0.913567509 366 0.0282904631 0.3159458183 0.394937945 367 0.9362333464 0.9701456351 0.449891258 368 0.5500495192 0.9458807199 0.152785856 369 0.9320686467 0.7501517776 0.102089221 370 0.3137839800 0.7753677391 0.062030176 371 0.5316903319 0.2782962408 0.946551740 372 0.9503071038 0.1564524630 0.373653485 373 0.8605535319 0.7530576503 0.381815319 374 0.4187593728 0.0342201130 0.807256441 375 0.7662927075 0.5564019093 0.099438679 376 0.4496172934 0.5887439523 0.059386802 377 0.4658499782 0.8791806730 0.505105258 378 0.1408295704 0.3202534819 0.866802158 379 0.6859266665 0.0451234847 0.183402799 380 0.4025848354 0.1113692916 0.956379740 381 0.5544540114 0.0858751207 0.142778292 382 0.5648256443 0.5336159950 0.924890156 383 0.6727542018 0.8860401020 0.915325739 384 0.6970705483 0.2500414085 0.768281287 385 0.3993773803 0.4700438085 0.316371213 386 0.7216110707 0.4414328204 0.099874506 387 0.7341371970 0.5050938698 0.653269146 388 0.1684000157 0.3985682614 0.863572443 389 0.6248012667 0.2710192350 0.276954715 390 0.8690601182 0.9936061928 0.115726013 391 0.8773299251 0.6416055518 0.339723316 392 0.8473763240 0.3966204205 0.818845225 393 0.6296141907 0.3974838224 0.466163284 394 0.0357694868 0.0644413177 0.624232494 395 0.4002664983 0.4900650519 0.125103871 396 0.5582493504 0.9668820433 0.078048980 397 0.5576146126 0.0716046500 0.190499994 398 0.3291113013 0.7323917581 0.997338800 399 0.5653475181 0.7782082327 0.569697561 400 0.5139552359 0.8691298212 0.528138879 401 0.1029199765 0.6415922479 0.738865932 402 0.9017956555 0.3681989533 0.014788399 403 0.1300081748 0.3727634479 0.084816476 404 0.5028760482 0.2834281570 0.753392662 405 0.6167644931 0.7392662682 0.451063136 406 0.7917854108 0.7611131743 0.813065219 407 0.1439542745 0.0858153393 0.649235810 408 0.0336355709 0.5852627060 0.615088174 409 0.6107383363 0.7312218356 0.835827948 410 0.2447151740 0.9148567214 0.042306389 411 0.5208800221 0.4018730107 0.781439138 412 0.3595253055 0.4949001716 0.004967534 413 0.6461015765 0.8702182316 0.973611509 414 0.1529908949 0.2444074138 0.765582877 415 0.1566277174 0.7693172845 0.068892283 416 0.5521451884 0.5519532149 0.177417176 417 0.6843436547 0.3240076548 0.900702647 418 0.1856415893 0.3318564647 0.096866918 419 0.0975673480 0.4760786693 0.471986917 420 0.7290326329 0.6597836316 0.955772876 421 0.5366336612 0.5549164847 0.553102183 422 0.6114070823 0.6587186693 0.007953998 423 0.0937127022 0.7113661575 0.876898227 424 0.0803781056 0.1035210942 0.598406682 425 0.3068068121 0.6794941770 0.188838216 426 0.4531581693 0.5950453898 0.409847462 427 0.3094340521 0.5425589513 0.650776462 428 0.2318717774 0.2191956949 0.648946756 429 0.5163603048 0.6094980745 0.447946401 430 0.5681897087 0.2336364791 0.295214692 431 0.4685455731 0.3028877801 0.068841520 432 0.7069857062 0.0218567387 0.233716408 433 0.3515282888 0.8465444185 0.803573804 434 0.4385973148 0.3473629637 0.617883441 435 0.4749626925 0.0890574595 0.188085022 436 0.3547779708 0.7379974185 0.791526760 437 0.1942676017 0.0079394246 0.997477157 438 0.0256246962 0.3535924666 0.698489423 439 0.8884308005 0.7628535840 0.809369163 440 0.0471414689 0.2817637813 0.904198337 441 0.1474339725 0.2587661603 0.242719773 442 0.7021593908 0.2530528193 0.524919445 443 0.4807156723 0.8349318991 0.070271240 444 0.8432966908 0.8468397048 0.470134102 445 0.3482438952 0.6815490499 0.064031017 446 0.1242048866 0.8927252733 0.522188024 447 0.6574711774 0.6846775450 0.890985590 448 0.7541771296 0.7206762349 0.774996555 449 0.7070043853 0.7600170255 0.267518109 450 0.9933493754 0.8513029092 0.694688216 451 0.5402909513 0.5576848101 0.181010432 452 0.0522799960 0.6944604991 0.640672027 453 0.5086942879 0.9485459770 0.182666690 454 0.2795328584 0.9230681325 0.071007218 455 0.2925957285 0.0272559715 0.396009628 456 0.4208056864 0.2842155597 0.657231304 457 0.9405288654 0.3529494354 0.067483188 458 0.3875212078 0.8732754863 0.913377441 459 0.2597758120 0.3927863508 0.867984843 460 0.2291357683 0.7875319326 0.051149221 461 0.1484227192 0.8090391355 0.705117003 462 0.4866195151 0.0734129762 0.442824473 463 0.7892362867 0.4453539846 0.121954784 464 0.5683366538 0.6619364661 0.186760232 465 0.2412845306 0.1936834501 0.890848876 466 0.1244373966 0.5603391614 0.533789977 467 0.4005386035 0.4215240139 0.446426343 468 0.6987896913 0.6909501497 0.352884758 469 0.8079235938 0.4607041548 0.625612257 470 0.5136809410 0.2786674271 0.611246112 471 0.1271985730 0.4093848013 0.702205135 472 0.4425768794 0.1256974903 0.462740155 473 0.2701393508 0.0287763786 0.963752114 474 0.0852672718 0.3647785818 0.785544906 475 0.8383072678 0.3441293056 0.229997027 476 0.5987685372 0.6069351842 0.916894967 477 0.3016595787 0.8286003179 0.098079569 478 0.9367990731 0.7363867506 0.099831548 479 0.8027157031 0.3406797077 0.764953703 480 0.5650394219 0.7784218604 0.571389669 481 0.8543107586 0.2325923357 0.177473234 482 0.5957680093 0.2863060525 0.621867881 483 0.5386852284 0.4697146376 0.561348902 484 0.9937895050 0.9476125687 0.195175298 485 0.8102212714 0.2234492698 0.496894690 486 0.3026532985 0.2417237794 0.904216400 487 0.8369588512 0.4713993466 0.017311705 488 0.3915421951 0.0331468829 0.286754538 489 0.8201161565 0.3621713065 0.306632450 490 0.0397220741 0.6384723403 0.862288375 491 0.6482997711 0.6928636658 0.498058393 492 0.5576362605 0.2238930671 0.915707878 493 0.6607955222 0.3236755331 0.891231451 494 0.4501192681 0.8065570174 0.833592845 495 0.9771914661 0.2452873208 0.684543002 496 0.7574431095 0.2715495902 0.929892377 497 0.1792897196 0.1909950075 0.430867825 498 0.6345388771 0.6050982235 0.213430690 499 0.7307245585 0.2590056360 0.491157572 500 0.9550017666 0.6125783336 0.430126610 501 0.4132789590 0.5587415013 0.707540820 502 0.1261792867 0.3612853771 0.656284911 503 0.9251572632 0.4820273374 0.021563028 504 0.4478524600 0.6163896762 0.859326350 505 0.7165222999 0.4614907785 0.907702564 506 0.6541780606 0.2412444674 0.490135853 507 0.9442219913 0.6415377590 0.456914689 508 0.9467369465 0.8996300348 0.901641032 509 0.9640912083 0.0410869946 0.980695251 510 0.3732388653 0.4830861366 0.464831451 511 0.1478042577 0.7105469978 0.361243103 512 0.7865168659 0.0258799670 0.610352629 513 0.4026820601 0.1141515123 0.073774567 514 0.9134162779 0.1482030996 0.838317783 515 0.9313118907 0.7333646615 0.186578115 516 0.7229551040 0.0172408349 0.876216590 517 0.1166268843 0.2034378981 0.622242737 518 0.5041656836 0.2613980127 0.729145212 519 0.0489721072 0.5793831972 0.163670554 520 0.9104810045 0.4284867200 0.348988740 521 0.8480586212 0.6895436323 0.741701515 522 0.4400068077 0.9950883300 0.472101074 523 0.5311336038 0.3148409440 0.508500267 524 0.4360992794 0.8477999608 0.074460126 525 0.5760545996 0.0326191103 0.332112870 526 0.6865018832 0.5540374245 0.623823412 527 0.5416533023 0.4976719327 0.197002878 528 0.4076520230 0.2285872947 0.757847161 529 0.2378068483 0.2019681430 0.869347882 530 0.6370063778 0.0563453555 0.534364452 531 0.6947946933 0.7111502644 0.327195446 532 0.8854648860 0.9748951907 0.583619741 533 0.5297113250 0.1012475549 0.639655538 534 0.5360497350 0.7042040324 0.703935319 535 0.8615456868 0.9087736702 0.259641427 536 0.3270071556 0.9687156640 0.510379517 537 0.5498819028 0.7116793904 0.800633240 538 0.3981204438 0.1055317647 0.332887672 539 0.1884881954 0.8991657628 0.651935739 540 0.9430633688 0.8470065235 0.985933232 541 0.2957795497 0.4845798714 0.620495155 542 0.5941085669 0.7551016384 0.626696358 543 0.6616673837 0.4585884756 0.332763716 544 0.2252898337 0.8735535287 0.250021933 545 0.8492385307 0.6050229876 0.976643395 546 0.3284978110 0.0208509073 0.552758015 547 0.5258779998 0.0941185125 0.712287780 548 0.0221860658 0.9694864992 0.365406210 549 0.9999005587 0.8640711615 0.799345604 550 0.2249033919 0.5597511516 0.985683206 551 0.2581388494 0.5881800281 0.297886255 552 0.0824325080 0.6931518228 0.818499598 553 0.2511851306 0.1219530550 0.073947041 554 0.5003025474 0.9877631674 0.345837254 555 0.1924816635 0.2932026354 0.722906101 556 0.9450151417 0.1164777910 0.899795833 557 0.0556947265 0.3653142145 0.833011540 558 0.7865477495 0.5493993000 0.922149335 559 0.3554171724 0.9726744783 0.872534773 560 0.3390859307 0.9219060326 0.820741070 561 0.7379814328 0.2120381724 0.693108915 562 0.0683563130 0.5029085143 0.827244471 563 0.1917957552 0.1268493799 0.267469959 564 0.9083188039 0.0586665533 0.942383993 565 0.9460777743 0.8651171569 0.837674263 566 0.5658098406 0.4958966884 0.791546705 567 0.3632471245 0.2307066286 0.804219844 568 0.8588721370 0.8229775466 0.228302267 569 0.5019012033 0.8366414213 0.455467683 570 0.2970695554 0.8778691809 0.141149248 571 0.3248096840 0.5907368434 0.104615112 572 0.5410586749 0.4280349151 0.594926988 573 0.2025833130 0.4288823237 0.677922638 574 0.9951313485 0.8449413823 0.481459997 575 0.5276287389 0.0723635927 0.321710046 576 0.1055847048 0.7462469260 0.912850004 577 0.7024908115 0.3999357389 0.186478262 578 0.7943248851 0.5395993334 0.920239061 579 0.0276398500 0.3101629145 0.118177905 580 0.1592246417 0.6026619161 0.576358764 581 0.7662116871 0.5950969979 0.813566438 582 0.4213486142 0.3764504716 0.827479247 583 0.7426504176 0.8308128808 0.507187529 584 0.6318207965 0.6347201027 0.125925017 585 0.7360877271 0.9745964138 0.068576889 586 0.3384028485 0.9400807733 0.005205019 587 0.4784741311 0.2316760146 0.342613811 588 0.4661606846 0.5499884067 0.588007716 589 0.2016149708 0.2244607336 0.990291373 590 0.1429585121 0.3208649538 0.698472717 591 0.5382393540 0.0899150085 0.088298887 592 0.6516490525 0.5870290762 0.460073830 593 0.7825980638 0.6463206944 0.834109541 594 0.0360894774 0.9555895720 0.374218001 595 0.0227680141 0.4798172456 0.230312304 596 0.6223328020 0.8188983048 0.259263913 597 0.0481542398 0.8776977926 0.928196097 598 0.4031829375 0.8861788490 0.386161405 599 0.3800641324 0.8961149775 0.744411892 600 0.9373641352 0.2962837527 0.228169761 601 0.1529641864 0.4246568638 0.561339771 602 0.3132382028 0.7459880235 0.701694527 603 0.6395901495 0.5864937957 0.118726775 604 0.3812313871 0.9908500304 0.687834089 605 0.6585895927 0.6807643520 0.807358899 606 0.0407362625 0.3835575725 0.941637933 607 0.6343256603 0.9410840643 0.201974260 608 0.7297780693 0.7785401796 0.685937374 609 0.8169210239 0.0184312759 0.995562205 610 0.6954687983 0.6902421599 0.107519840 611 0.3587358908 0.8174600382 0.887226315 612 0.4950156456 0.9988616975 0.460948817 613 0.5124422519 0.9000427283 0.311366444 614 0.4193161500 0.2330806397 0.331944056 615 0.1372738420 0.8558078955 0.634889112 616 0.6487600016 0.4483145340 0.200628157 617 0.6930406457 0.1405613457 0.946231753 618 0.5379465765 0.7251331760 0.693917649 619 0.3548438435 0.0162725917 0.684812097 620 0.5906515468 0.7329806052 0.904670520 621 0.6603484906 0.6076894940 0.091763721 622 0.3636276750 0.0564669154 0.066468826 623 0.8929951529 0.0173468851 0.913456712 624 0.5193466372 0.4707073863 0.934256916 625 0.6822469798 0.3953355562 0.289309436 626 0.0824959101 0.2921910062 0.044693490 627 0.7019017276 0.9668182791 0.750223392 628 0.4275053341 0.8367497751 0.332937845 629 0.0922464526 0.4102489238 0.812326842 630 0.4956276130 0.8354235855 0.744443205 631 0.5282284787 0.3624600177 0.437094010 632 0.4597527271 0.8304499635 0.176761237 633 0.0696412656 0.6655701364 0.389712420 634 0.3637543309 0.0200155203 0.947725208 635 0.1510170416 0.2281706810 0.351725206 636 0.6223586730 0.2150866047 0.854345126 637 0.7047801397 0.0133177778 0.107109227 638 0.0828919602 0.5865104373 0.861315191 639 0.3410899027 0.5765409295 0.923982420 640 0.1366543320 0.7885096699 0.751887410 641 0.2085322624 0.8079397904 0.664778188 642 0.4698440309 0.3802732481 0.143456051 643 0.3877236466 0.6920087750 0.195913500 644 0.5385239455 0.7067448399 0.208434227 645 0.7802318172 0.4299515062 0.629565495 646 0.0606531394 0.1196082544 0.712476384 647 0.7366263098 0.1238772105 0.722834192 648 0.4990216279 0.9745745591 0.335687502 649 0.1208609531 0.7079302194 0.138499342 650 0.5504307044 0.3144544242 0.634861803 651 0.5357888581 0.1576599039 0.895698367 652 0.8372551946 0.6539439561 0.121514479 653 0.8663264213 0.9303858271 0.101986314 654 0.8151593243 0.7399452543 0.564897440 655 0.5766503746 0.7543237149 0.178147843 656 0.7617674056 0.3053654318 0.211049212 657 0.5318685232 0.5440830449 0.438073355 658 0.3935762031 0.1543934196 0.792410720 659 0.6021584028 0.1720456653 0.780616953 660 0.4456974030 0.2089627727 0.670902713 661 0.1230438619 0.8262171666 0.825892488 662 0.1998506242 0.1914696826 0.688846716 663 0.8685076090 0.0552638469 0.592013019 664 0.1232273690 0.4114682395 0.111715549 665 0.5658827955 0.9385932428 0.630991100 666 0.0928694389 0.3512963140 0.551601721 667 0.6724121408 0.6340143762 0.619171673 668 0.7189048890 0.4819496642 0.394437478 669 0.7450684533 0.6334214911 0.120663491 670 0.4686049288 0.3850550903 0.225975410 671 0.3261675073 0.5564902842 0.439655563 672 0.7405954481 0.1667849601 0.778386813 673 0.5571954157 0.3600399296 0.341081738 674 0.3966088423 0.4169549660 0.984560798 675 0.8554791166 0.3829855390 0.851567330 676 0.2645492698 0.3424455107 0.810737652 677 0.9462965208 0.7442224536 0.081737134 678 0.2153703177 0.2868734032 0.846269100 679 0.3989983632 0.6966406952 0.934992647 680 0.2243203325 0.8841968733 0.506934221 681 0.7125027745 0.7271546698 0.810377810 682 0.0787354475 0.2923811390 0.245231815 683 0.2284422005 0.7890823116 0.217577334 684 0.2295348563 0.3330317193 0.373440096 685 0.7260223969 0.7212731063 0.153669810 686 0.2057637249 0.2594939065 0.245352808 687 0.5401250874 0.8030458153 0.469113623 688 0.4596121595 0.3054662987 0.468811055 689 0.3042413592 0.1917874399 0.576269256 690 0.2890700931 0.9998134219 0.942503915 691 0.8854543604 0.1116818776 0.383314030 692 0.6776874792 0.5922678534 0.693309897 693 0.5526406986 0.1233293717 0.087034084 694 0.1912126953 0.0772855950 0.887516976 695 0.7959685409 0.3028606996 0.898052296 696 0.3024255738 0.5259479519 0.319401479 697 0.6322551961 0.9411505456 0.893078855 698 0.0522503981 0.2155010924 0.074356175 699 0.4422330016 0.0192078033 0.237554085 700 0.2619074290 0.6082642223 0.982594245 701 0.9869507232 0.8930197172 0.273582796 702 0.4603780659 0.4993943570 0.096071468 703 0.2949062802 0.8040989162 0.918891617 704 0.4938740865 0.5205094554 0.076841577 705 0.0972780201 0.2060259327 0.607541317 706 0.8583569909 0.7992311558 0.047517068 707 0.7276338739 0.5565277084 0.241794257 708 0.4677256306 0.1712370019 0.228214926 709 0.0024954216 0.2747133188 0.322586709 710 0.3169592826 0.9069585775 0.423306790 711 0.7098196479 0.9432440118 0.358426370 712 0.2738314560 0.0971547274 0.470391238 713 0.3008469422 0.0383837584 0.365313627 714 0.7889818386 0.9164815550 0.875330874 715 0.2325974195 0.1971113200 0.561480183 716 0.5846996335 0.2200600684 0.825061466 717 0.5494406375 0.6702300981 0.063295416 718 0.1967900842 0.6804504094 0.817557294 719 0.9293358885 0.9159706607 0.612163059 720 0.4464485829 0.1391113657 0.016358729 721 0.1984538860 0.9922945008 0.260804367 722 0.6011682674 0.0078241592 0.994232268 723 0.4368371693 0.6018165236 0.271021605 724 0.0359789054 0.0309819141 0.291022451 725 0.4358850084 0.3571438494 0.672495972 726 0.0843085411 0.7200475056 0.543844765 727 0.9935347575 0.2231217008 0.162310042 728 0.8694156781 0.7412591046 0.116366554 729 0.3230546513 0.6869672353 0.355232729 730 0.2692568868 0.7453184524 0.318895462 731 0.9205692166 0.0721968049 0.747437760 732 0.9830821056 0.9566366056 0.735812146 733 0.3031027087 0.1807147223 0.515523697 734 0.8074569593 0.7621163037 0.277175065 735 0.1328059128 0.2716216373 0.386344013 736 0.3731001110 0.4226538686 0.893595606 737 0.8101217598 0.1615734680 0.545682621 738 0.9005346450 0.9483917356 0.162716153 739 0.5641075023 0.9057268400 0.315164645 740 0.7819188943 0.0401506533 0.443097079 741 0.7546357475 0.8867348845 0.545873337 742 0.9840175221 0.1725108344 0.646062576 743 0.1318724579 0.0028678142 0.238467700 744 0.2567289958 0.1308603347 0.707534412 745 0.6086509489 0.3696970518 0.405707285 746 0.7872655632 0.5497150687 0.446210140 747 0.6726261911 0.8797215447 0.694525995 748 0.2360209036 0.1031810008 0.939940774 749 0.7058318090 0.8746639634 0.876881678 750 0.5646322125 0.5175348681 0.899540994 751 0.3558960068 0.0915856815 0.071928061 752 0.7108148087 0.9289045038 0.409138034 753 0.3208266883 0.6941444234 0.420427548 754 0.7587849370 0.8223717990 0.617787307 755 0.8554418513 0.6104312195 0.685158405 756 0.5451220218 0.1108894546 0.754115731 757 0.7196569699 0.8765047307 0.742762204 758 0.0215774111 0.7833220700 0.358652892 759 0.7936053767 0.1422403960 0.228015036 760 0.5894972838 0.6867280926 0.727455930 761 0.5965793543 0.2810900006 0.049796629 762 0.6823342303 0.5309740093 0.709166650 763 0.4851749572 0.8965564165 0.270272462 764 0.2186638459 0.4845173971 0.223458015 765 0.5240909741 0.1192635912 0.836980513 766 0.7615321467 0.2461064239 0.305156313 767 0.5604892469 0.1504576304 0.778905628 768 0.5576477910 0.1195758597 0.729724572 769 0.0298766780 0.3231485880 0.810902923 770 0.6399024175 0.1802191990 0.031875964 771 0.4483339144 0.8136381572 0.266554726 772 0.1456385537 0.6185407676 0.424736338 773 0.9980573370 0.4370639510 0.906303392 774 0.1771693868 0.6572026338 0.648354213 775 0.3075491870 0.2273569726 0.480896802 776 0.3877885581 0.1247133319 0.641370056 777 0.3255822414 0.4710750806 0.601308974 778 0.9333021557 0.2480874057 0.644261836 779 0.6875130839 0.1116264574 0.975304031 780 0.3070184211 0.5074775650 0.232165708 781 0.5424951171 0.2500219222 0.605900436 782 0.9170924036 0.3036775775 0.130279193 783 0.3457037862 0.1089829586 0.030755766 784 0.2805052300 0.3548592539 0.194080905 785 0.1462095752 0.0438470747 0.631534894 786 0.7462217780 0.2378483214 0.231059715 787 0.6584212827 0.7611358352 0.918849279 788 0.2268953726 0.0877234547 0.932185473 789 0.5527597035 0.3166887127 0.353739069 790 0.2126589350 0.4487409566 0.044028442 791 0.6011075461 0.7731962018 0.346391862 792 0.4955664894 0.4370704067 0.739043013 793 0.8423375196 0.1270604522 0.317217213 794 0.1852961355 0.2097269609 0.088107983 795 0.9665051368 0.2191235535 0.096969624 796 0.7390349819 0.7743024451 0.050907627 797 0.3564518855 0.6329019496 0.299431361 798 0.1001586283 0.9697424432 0.119666547 799 0.2597023719 0.6517637181 0.153376347 800 0.6991052001 0.9141410457 0.265971137 801 0.4336216610 0.6204367809 0.287241898 802 0.9722799619 0.9041905978 0.949226398 803 0.3676507603 0.7783370393 0.605557245 804 0.9472060276 0.1846980839 0.961454876 805 0.7027460397 0.0863679377 0.350598877 806 0.1309114473 0.5038668043 0.978715864 807 0.2550966237 0.5919990689 0.726682640 808 0.2965044272 0.7501525732 0.205615580 809 0.2213688432 0.3952954053 0.998190792 810 0.4808703209 0.4208385709 0.995545863 811 0.3624091498 0.4595744461 0.799096379 812 0.8317626240 0.2396454047 0.548187687 813 0.6742728841 0.1104490117 0.598536491 814 0.1261447764 0.1552675441 0.272370511 815 0.9580946744 0.4285296819 0.968113240 816 0.6149920234 0.8321298480 0.698046484 817 0.5011624463 0.6944097814 0.278748058 818 0.9852243848 0.2406683902 0.018022690 819 0.0772609098 0.7450529367 0.039625611 820 0.5415186076 0.4747119257 0.896825491 821 0.3348767199 0.6098611136 0.577773161 822 0.4783724891 0.3886900644 0.677212795 823 0.3442536590 0.7314892649 0.086071891 824 0.5835320374 0.4907233738 0.177435393 825 0.9339889446 0.3731712026 0.393874652 826 0.9251223728 0.6926013883 0.662840765 827 0.3417600116 0.5710211955 0.394355623 828 0.4704917639 0.5288532262 0.204753422 829 0.6319271945 0.8954059565 0.101903225 830 0.7197887672 0.7918050652 0.871796291 831 0.6301961564 0.2209867758 0.095405005 832 0.7079224242 0.9972060299 0.683585294 833 0.5663749434 0.4601809371 0.591159837 834 0.8835416073 0.9517572247 0.021346556 835 0.3805341318 0.7682048625 0.607100267 836 0.1149117290 0.3352859647 0.221796387 837 0.6165271252 0.1312675015 0.518161186 838 0.1176810323 0.2363307266 0.026905061 839 0.3870377922 0.6967954086 0.984365234 840 0.5219549667 0.0975140380 0.633635873 841 0.7895589520 0.0749162380 0.677225510 842 0.7397307083 0.4174277787 0.146708182 843 0.3478056667 0.4072185773 0.446643704 844 0.1604789551 0.2810919166 0.376148333 845 0.9251088433 0.8910745184 0.184053494 846 0.0136304810 0.8324750320 0.012508075 847 0.8104454086 0.9549242419 0.174654766 848 0.2838601123 0.4352603666 0.662441208 849 0.9989845387 0.4433427551 0.085998558 850 0.3344213252 0.8014390990 0.625002468 851 0.0715592853 0.2017857837 0.922036019 852 0.5103901837 0.6150838642 0.096749347 853 0.0633041605 0.1500133087 0.950916851 854 0.3187373579 0.5897086798 0.941266669 855 0.2308404257 0.6086097441 0.500319101 856 0.0412874152 0.1480539243 0.938956048 857 0.7764879493 0.2771860566 0.941295495 858 0.6102122869 0.8902521830 0.208078722 859 0.6847100409 0.1657704103 0.559949947 860 0.4759049457 0.2603934291 0.265524188 861 0.7726181280 0.9765032728 0.764477738 862 0.1444186131 0.4385247866 0.596427928 863 0.8826502324 0.5756433317 0.878524726 864 0.9136634860 0.1600622404 0.461303858 865 0.0380516683 0.7695483940 0.921434690 866 0.8698804867 0.4721365480 0.359045910 867 0.9890793655 0.4400611159 0.450260557 868 0.2904401754 0.1828926203 0.895283058 869 0.8492796938 0.0380649534 0.518706080 870 0.7597751443 0.3915797304 0.304194722 871 0.5994729039 0.2184883629 0.986610676 872 0.2150855558 0.0174954759 0.904102191 873 0.8826147132 0.5637028904 0.361423424 874 0.9353977251 0.9109342301 0.867015991 875 0.5928153037 0.4265120404 0.147806567 876 0.2736188730 0.3341847470 0.252160472 877 0.4527158521 0.2142571628 0.992705174 878 0.3849735709 0.0373925450 0.334103196 879 0.7877801368 0.9842010399 0.195866342 880 0.1202234565 0.1930400908 0.652330256 881 0.2110027948 0.5344378769 0.642685626 882 0.7905226327 0.3779459668 0.635173605 883 0.2934547048 0.3679792546 0.593422699 884 0.4573139457 0.1582164683 0.023487672 885 0.8029606193 0.7010082020 0.284318620 886 0.9665408591 0.6073600804 0.660012832 887 0.2198708144 0.1453944338 0.889665270 888 0.2914561022 0.7143639419 0.116632195 889 0.9410663755 0.6670784273 0.292615006 890 0.7086967225 0.8629533281 0.860167607 891 0.0894650980 0.1249317278 0.458329211 892 0.4917318639 0.5387740363 0.287814197 893 0.9280869819 0.7142212233 0.937446594 894 0.4264453109 0.0457125187 0.646521847 895 0.7130430103 0.2010276795 0.914412531 896 0.7420520796 0.7263595280 0.231711044 897 0.7444685814 0.4017101536 0.065026434 898 0.8719011499 0.9411909385 0.356957232 899 0.3430910874 0.3079510119 0.265917442 900 0.5024578348 0.2550702172 0.718842493 901 0.4036464305 0.0388136716 0.539094977 902 0.7733309979 0.7575183806 0.438675189 903 0.7544153144 0.0066090750 0.055222656 904 0.6703264886 0.6471884502 0.448854770 905 0.2629647448 0.9012367099 0.881672440 906 0.1803582727 0.3752786340 0.852202812 907 0.8431271741 0.5248582852 0.310761757 908 0.2284120221 0.1554611062 0.271397468 909 0.9815748923 0.3106818793 0.503561401 910 0.5820819039 0.8009110682 0.313130387 911 0.3962558024 0.5859679591 0.967218323 912 0.2827562944 0.0372282511 0.217953848 913 0.8501226422 0.3453022940 0.648826417 914 0.6198121691 0.5673128264 0.431295119 915 0.8348960320 0.0189236847 0.810088007 916 0.9729937878 0.1587446914 0.135784560 917 0.6216949238 0.5137279432 0.302157789 918 0.3172746876 0.7171976375 0.318524733 919 0.2923211546 0.7407354964 0.835473312 920 0.9858742347 0.5725584505 0.127078771 921 0.1445829689 0.2193578896 0.297499642 922 0.1202723817 0.9384786098 0.926139504 923 0.5611545546 0.5124028493 0.830471531 924 0.6683973325 0.0950436117 0.228388001 925 0.3101932399 0.1064313874 0.696718747 926 0.0065761928 0.5799916589 0.728362826 927 0.2705754482 0.6334289019 0.213996048 928 0.6163870748 0.9408771771 0.305505149 929 0.5825870181 0.7055658610 0.854551745 930 0.0560807060 0.2184536899 0.973126440 931 0.0305702714 0.2604252915 0.294440166 932 0.1147877721 0.2299285294 0.343987265 933 0.6380620983 0.7959253674 0.404231852 934 0.5519847185 0.9795302413 0.858354224 935 0.9288181656 0.8940306874 0.888728004 936 0.3502963928 0.8894755677 0.944722258 937 0.0143268388 0.0346940127 0.235338558 938 0.0341333786 0.7793426653 0.963776887 939 0.7106323622 0.5434363140 0.751127602 940 0.1490545860 0.4038637085 0.716740757 941 0.4734449964 0.4437388908 0.475056946 942 0.7208320657 0.9866875494 0.543951573 943 0.2622331488 0.2832836569 0.035830080 944 0.7833650876 0.7233259575 0.428406089 945 0.5218209329 0.1005121029 0.218219518 946 0.7760500603 0.6374043820 0.052455828 947 0.6739972050 0.0909351802 0.904614732 948 0.8914999729 0.1385136431 0.479137238 949 0.1951620504 0.8737525195 0.972547704 950 0.4718771121 0.2480492322 0.041709913 951 0.2666831575 0.6943463988 0.988687727 952 0.7738986546 0.1257526183 0.893773123 953 0.1496802690 0.3521917525 0.754022287 954 0.4720940117 0.6001135744 0.338170915 955 0.8082588040 0.8172353797 0.916966529 956 0.5283849533 0.7120828282 0.081214865 957 0.8441957091 0.4700241142 0.493242827 958 0.0938648465 0.6583943067 0.981820053 959 0.3471312686 0.0675094635 0.037047612 960 0.6650372180 0.3251894296 0.116741180 961 0.7140586991 0.0734348153 0.631955886 962 0.6655903978 0.7986309046 0.094634300 963 0.1438620712 0.3221085093 0.243953107 964 0.7278595115 0.2289379491 0.362771598 965 0.1802145545 0.8742005217 0.723245273 966 0.7823882208 0.3580379246 0.054962467 967 0.6995504668 0.0912349010 0.632198407 968 0.7469125430 0.3847571795 0.911859033 969 0.1873785281 0.8334370614 0.705362193 970 0.2881696546 0.5870317714 0.125191815 971 0.4128162214 0.0516679150 0.382608344 972 0.4137606274 0.0732873864 0.657100869 973 0.7626614401 0.3218054264 0.278459829 974 0.6579748753 0.9010156130 0.481620893 975 0.7438406860 0.4247153068 0.448899247 976 0.0268771388 0.9448438413 0.121263162 977 0.4899677269 0.7269265945 0.664452199 978 0.8597049168 0.5871146580 0.460823134 979 0.2966905709 0.5181974168 0.803420998 980 0.3388050934 0.8238629079 0.973437019 981 0.4403209696 0.8365467857 0.604767612 982 0.3581107967 0.7897584133 0.748093019 983 0.9181199386 0.7010477460 0.974371480 984 0.4855272081 0.5026488579 0.734248891 985 0.7215003686 0.7091594590 0.313281513 986 0.6219019089 0.2435479793 0.284571647 987 0.5285257522 0.3192963838 0.505222295 988 0.9388092165 0.8667535833 0.026928221 989 0.2814255413 0.7957225109 0.526132444 990 0.5068233504 0.2621329527 0.250766840 991 0.2548627399 0.4510898469 0.885296197 992 0.8310871634 0.1405788586 0.569291269 993 0.4782470900 0.0357218925 0.887661504 994 0.1346597481 0.9022380263 0.849145710 995 0.1659126263 0.7283752465 0.633044650 996 0.1651590609 0.6786643637 0.996066710 997 0.6562744505 0.5517038938 0.438446892 998 0.3805340903 0.9445086999 0.743781529 999 0.8062844079 0.7442193504 0.075846629 1000 0.9749252927 0.7738602485 0.271969781 # select all of the columns from rows 1 to 2 of the dataframe data [ 1 : 2 ,] x y z a m i j 1 0.9370950 0.3904391 0.2530045 0.5775475 0.3311816 0.9087632 0.5128350 2 0.8169913 0.7717700 0.1949707 0.4314746 0.4468994 0.5681275 0.7094579 k 1 0.4334744 2 0.8684275 # drop all of the columns from rows 8 to 10 of the dataframe data [ - ( 8 : 10 ),] x y z a m 1 0.9370949692 0.3904391259 0.2530045395 0.577547544 3.311816e-01 2 0.8169912936 0.7717700286 0.1949706976 0.431474575 4.468994e-01 3 0.6313947993 0.5873462579 0.4276207259 0.449063063 2.161926e-01 4 0.0716134447 0.4721571105 0.7047920560 0.530008280 8.032733e-01 5 0.4706879971 0.2900230386 0.8256189562 0.047032020 8.461648e-01 6 0.3116779991 0.4097562127 0.2170954866 0.047558205 8.109110e-01 7 0.7180650725 0.8154219799 0.2912252382 0.941795656 8.794287e-01 11 0.7780771402 0.4888597687 0.4218451059 0.541726347 3.588103e-01 12 0.8549661185 0.6202294999 0.1322223113 0.378038323 6.000676e-01 13 0.7936905599 0.8205492713 0.5068847497 0.344106114 5.298757e-01 14 0.0497731785 0.4382137246 0.3183242229 0.206805342 4.169467e-01 15 0.1041691788 0.2583737297 0.8669875974 0.409303598 8.821301e-01 16 0.5839785282 0.9458635810 0.1451297677 0.124021082 4.282974e-01 17 0.6301283748 0.8938101421 0.4633655346 0.863932969 8.371921e-01 18 0.0538908630 0.7993367051 0.1662894906 0.337508279 3.171606e-01 19 0.4699743150 0.2179973929 0.9039500949 0.542787326 7.940503e-01 20 0.8096281542 0.4553503187 0.5965530800 0.498133432 6.258840e-01 21 0.6269215296 0.7560675466 0.7957581254 0.840931672 5.378133e-01 22 0.4254209346 0.6859814830 0.8225671803 0.186610433 5.037850e-01 23 0.1013888507 0.2460164321 0.7510759791 0.237333820 7.254038e-01 24 0.9101979593 0.4163650593 0.5020412898 0.633828752 2.637897e-02 25 0.7577146925 0.1546449522 0.4458510922 0.467762029 2.085573e-01 26 0.8808603676 0.8900883312 0.6414468170 0.336437304 1.080850e-01 27 0.2358829738 0.6140667766 0.6801568472 0.451031481 2.824390e-01 28 0.3214730755 0.8646407460 0.2427712576 0.178794772 1.326834e-01 29 0.7767506840 0.9355752321 0.4893625444 0.524841033 5.627676e-01 30 0.2756272962 0.7027894342 0.4434864377 0.173538475 7.091635e-01 31 0.7999626761 0.1894985719 0.6973984847 0.389671007 1.434607e-01 32 0.4605515839 0.1737330768 0.2664361666 0.525079773 3.887599e-01 33 0.7878976951 0.9669068004 0.2513810424 0.279101572 6.431891e-01 34 0.0911097701 0.0928617879 0.2627664267 0.123210540 8.142502e-01 35 0.0275765241 0.6088669079 0.7417254893 0.389373654 6.733047e-02 36 0.7061024618 0.8505606914 0.6830506644 0.284471479 3.249732e-01 37 0.6172355278 0.4425195777 0.8666974416 0.469516267 6.445455e-01 38 0.0632016384 0.0005237793 0.7255827556 0.371542407 5.937821e-01 39 0.8622419508 0.7715821874 0.3958992928 0.667401103 2.421245e-01 40 0.7231777157 0.8044306301 0.0778860459 0.172507857 8.447065e-01 41 0.4927164968 0.1889234560 0.4412154641 0.650509367 9.200443e-01 42 0.5703471822 0.8337357312 0.4939526306 0.031746348 8.359302e-01 43 0.8936006089 0.4499214077 0.0217764010 0.729141285 2.380587e-01 44 0.6202150190 0.3614634674 0.9754849086 0.612729790 3.214372e-01 45 0.7912620052 0.1988568865 0.7979321314 0.248550579 4.465604e-01 46 0.5302869780 0.8204511884 0.5899471235 0.236167135 4.993286e-01 47 0.3760423027 0.6197578758 0.9657687983 0.816466525 4.290283e-01 48 0.4486860153 0.7791226325 0.9174016686 0.599676585 8.931907e-01 49 0.4307841142 0.0385082576 0.8245737706 0.897940309 1.788797e-01 50 0.8595965470 0.3248715224 0.7299323552 0.886358950 8.883161e-01 51 0.0252711680 0.6122176033 0.9074188452 0.897394733 4.080858e-01 52 0.1002211126 0.2690452361 0.3296885516 0.345949224 6.038600e-01 53 0.0807856577 0.0998913255 0.8701844758 0.269625058 3.758807e-01 54 0.7170568479 0.4884574136 0.5018407770 0.403440406 8.281473e-01 55 0.8823796129 0.3786438145 0.3983667553 0.804715395 6.636888e-01 56 0.5683260462 0.7532928397 0.6130672316 0.679588470 1.800565e-01 57 0.8688152484 0.2847076976 0.3810614778 0.824492445 1.734603e-01 58 0.6965453394 0.6635359814 0.1475261322 0.804251817 8.262297e-01 59 0.0802273641 0.3858547176 0.6009099570 0.519138621 5.420346e-01 60 0.0140079728 0.1603731690 0.9542593956 0.364582830 4.444947e-01 61 0.4727872738 0.4798752652 0.6213749733 0.598921384 3.226989e-01 62 0.0459575092 0.0832084191 0.6381078002 0.244956968 3.561886e-01 63 0.1385185381 0.6099833576 0.4599676093 0.350077863 6.626158e-01 64 0.8986839550 0.6535483883 0.4232784463 0.657276285 5.436132e-01 65 0.5309889077 0.4783877260 0.0462058538 0.839267493 4.156813e-01 66 0.0409239323 0.2436686188 0.9950620085 0.206467974 5.036194e-01 67 0.7674356611 0.6750660734 0.6059721843 0.020811806 9.566451e-01 68 0.3960749088 0.2688281150 0.0242629193 0.732992485 2.720517e-02 69 0.3985890197 0.7642159278 0.8804058409 0.930405641 3.359340e-01 70 0.3705691348 0.0467719736 0.1671624330 0.061075286 6.948139e-01 71 0.2754363231 0.9146933509 0.5973722835 0.277121296 2.015421e-01 72 0.5407387190 0.9995588632 0.1497331320 0.887567588 9.082803e-01 73 0.5314532970 0.0659897071 0.3875588439 0.617622069 2.811714e-01 74 0.0803362590 0.4144914525 0.2956018788 0.316371846 2.652967e-01 75 0.8250675497 0.7498207777 0.1106022566 0.318800121 5.904470e-01 76 0.1926816499 0.7235343596 0.7979764920 0.609599486 7.587092e-01 77 0.0425402028 0.0603169515 0.7010821693 0.264385223 3.563557e-01 78 0.3964480858 0.9809383829 0.1755605682 0.750374798 1.333250e-01 79 0.4223065777 0.0653125530 0.9426773726 0.399970984 1.586909e-01 80 0.6523972722 0.2083718709 0.4009221988 0.142288364 9.384446e-01 81 0.3475282227 0.1577986199 0.2000566917 0.625475666 9.390198e-01 82 0.5286904851 0.8383829324 0.3611871728 0.550002784 5.167771e-01 83 0.1822423232 0.3047285695 0.2869544714 0.530641184 2.719163e-01 84 0.0047013757 0.3647190086 0.0340154995 0.298535647 5.472704e-01 85 0.3293453683 0.9229505726 0.7833694841 0.632987165 7.515297e-01 86 0.1202686552 0.4016410608 0.3511313878 0.629080631 8.448410e-01 87 0.8493540538 0.6416005327 0.0867994754 0.628401057 2.774768e-01 88 0.7695235745 0.5029771237 0.6142535997 0.687793811 6.338755e-01 89 0.6904120625 0.8910172952 0.5705231265 0.182845271 7.624777e-01 90 0.3335720669 0.2518689649 0.7833944124 0.695882965 5.926175e-01 91 0.7919240277 0.1950826326 0.6733682633 0.899178525 8.392912e-01 92 0.6215966814 0.8693118924 0.6596714961 0.815885935 1.616443e-01 93 0.5720835368 0.9879097429 0.7489858174 0.896938740 7.624196e-01 94 0.4893391300 0.5527650674 0.8465101460 0.806428441 8.202123e-02 95 0.3000833855 0.4928710270 0.7013142661 0.701419483 7.109783e-01 96 0.1436036394 0.3200193204 0.3412105888 0.606645083 1.577007e-01 97 0.5476652577 0.2205543050 0.3517985044 0.930552275 2.001931e-01 98 0.9131340059 0.0837841036 0.7133782210 0.719024671 7.160846e-02 99 0.8401359348 0.0370367223 0.4614702794 0.079629550 8.718041e-03 100 0.4504831121 0.2381348317 0.4102221406 0.520588716 8.591019e-01 101 0.6254940971 0.5222004815 0.8796439485 0.827647213 5.727372e-02 102 0.7448368245 0.6181551786 0.4219239037 0.467653792 6.276878e-02 103 0.9457870251 0.4207581431 0.3550725051 0.006337496 1.791115e-01 104 0.1310959703 0.4963657029 0.1765153394 0.359040588 6.913411e-01 105 0.8484062718 0.9034369027 0.6787415841 0.586619119 5.452538e-01 106 0.4371888463 0.4749349221 0.6336629451 0.044809929 7.567161e-01 107 0.9108372824 0.8643784064 0.8615905407 0.148015360 1.638905e-01 108 0.4796622975 0.0227832580 0.5454826001 0.527876457 2.512274e-01 109 0.1027305629 0.1072980338 0.3083832473 0.525531210 6.378927e-01 110 0.2847951869 0.4515920919 0.3195284132 0.648003328 2.805665e-01 111 0.9316735922 0.0052489392 0.7868197537 0.505368755 4.154278e-01 112 0.0924596551 0.8432675595 0.4399540033 0.789810633 8.377471e-01 113 0.5673171869 0.2813623182 0.6962677925 0.029624238 3.589913e-01 114 0.8735257180 0.5234353847 0.2538753240 0.232593814 9.064808e-02 115 0.4290504728 0.4694526973 0.2661211840 0.907042397 1.552227e-01 116 0.8996819127 0.0621974380 0.3251096997 0.853816796 2.705220e-01 117 0.0579402959 0.0302452876 0.9352964088 0.585931442 9.032564e-01 118 0.1014418246 0.8006376394 0.2328273214 0.820464912 5.355829e-01 119 0.4509296967 0.1611418875 0.2132689748 0.153000990 1.609333e-01 120 0.2546673652 0.5602219575 0.5468407678 0.947769340 4.493483e-01 121 0.7113273235 0.8592565532 0.1178469069 0.498519980 5.536828e-01 122 0.7692845000 0.0566702003 0.9734982299 0.122954023 6.887685e-01 123 0.3481028744 0.4445713721 0.3522821588 0.787033934 7.327817e-02 124 0.9676634180 0.8296126574 0.9950421427 0.103310564 3.178156e-01 125 0.7331326583 0.0971798035 0.2761500876 0.127017952 4.720898e-01 126 0.0599658110 0.7115563331 0.0540546447 0.106573350 8.228742e-01 127 0.1248628269 0.7587779057 0.1176031553 0.934332104 5.604531e-02 128 0.2991520732 0.3776659367 0.5170687952 0.392918030 3.367465e-01 129 0.6645840129 0.7026710063 0.7148548134 0.970120105 8.332619e-01 130 0.1244262301 0.9337320582 0.1367022363 0.913373896 6.899806e-01 131 0.1236656245 0.0614476257 0.9959184171 0.158355799 8.971463e-01 132 0.0696135804 0.3393731464 0.3305444729 0.654447101 9.089449e-01 133 0.3501586101 0.5802877264 0.3816649450 0.935220445 7.389805e-02 134 0.5799933816 0.7677533829 0.1642848232 0.147463179 3.779670e-01 135 0.1702301877 0.9032238321 0.7545808731 0.886644857 7.954767e-01 136 0.9765062209 0.9296272635 0.8273214900 0.506397160 7.001717e-01 137 0.7148984962 0.8073559222 0.2429534406 0.094754952 4.551603e-01 138 0.3888362625 0.3348044397 0.1438325557 0.191665294 2.816520e-01 139 0.0689336834 0.3640246105 0.3012798512 0.644458028 4.497799e-01 140 0.9315936987 0.7868268418 0.6475578381 0.858399672 8.137392e-01 141 0.7053553022 0.0936103060 0.3798077465 0.989375187 9.145746e-01 142 0.7641971211 0.1513883369 0.6868384471 0.037770061 4.302203e-01 143 0.7521351997 0.9857477553 0.5470604110 0.860812797 9.163151e-01 144 0.1143717000 0.5475322953 0.3682726121 0.548567706 5.226119e-01 145 0.1755395108 0.1196474847 0.8436278808 0.067070395 5.931327e-01 146 0.7314520737 0.8785161506 0.3468810113 0.807048478 1.057343e-01 147 0.0840596480 0.0384507929 0.0654803049 0.837991760 3.502395e-01 148 0.6503178864 0.4422026391 0.9759091735 0.641077164 5.667044e-01 149 0.8325164884 0.1573755434 0.4993437151 0.488269411 5.032156e-01 150 0.6056464242 0.9779126472 0.0997745579 0.719270619 4.198256e-01 151 0.1708778741 0.0848242871 0.1683927856 0.489719765 1.845283e-01 152 0.7192412303 0.4588409185 0.8876504118 0.228980459 8.690613e-01 153 0.7769590721 0.8675205393 0.6314575365 0.143649914 6.875175e-01 154 0.3580576980 0.3434970572 0.9685689912 0.824826804 6.023344e-01 155 0.1536754405 0.7642185981 0.7190012864 0.920236114 5.640764e-01 156 0.8876610205 0.2691004938 0.9682509759 0.009020783 7.899509e-01 157 0.9968752931 0.8581504109 0.4439522170 0.612094169 9.859647e-01 158 0.5289386453 0.6496191269 0.2969512208 0.413060909 3.806414e-01 159 0.6329279244 0.0386093326 0.3381252782 0.360855727 3.545589e-02 160 0.0034896852 0.1933919925 0.7016994103 0.594918283 8.101724e-01 161 0.8976498600 0.1778210327 0.4423170090 0.982059956 4.761609e-01 162 0.2601647917 0.5901049029 0.1208666377 0.791583062 8.117129e-01 163 0.3986346393 0.8277773953 0.4906746778 0.731339336 7.174461e-01 164 0.5764877701 0.6677092968 0.3722350411 0.684600540 4.484358e-01 165 0.7178556903 0.1503093268 0.0881471203 0.700219793 5.278507e-01 166 0.1486660023 0.3068179993 0.6048259446 0.367801031 8.009471e-01 167 0.2934395475 0.3574407734 0.1875770059 0.305140699 1.799086e-01 168 0.8550011665 0.9941004280 0.6293192946 0.323219864 9.496474e-01 169 0.9138944144 0.1918540595 0.7362092163 0.129870965 7.500945e-01 170 0.5692845336 0.0576184834 0.9621483448 0.946476570 3.975674e-01 171 0.2180541193 0.5792422167 0.7194545367 0.424145223 2.506493e-01 172 0.8578979487 0.7784323918 0.1277054867 0.572289248 2.549385e-01 173 0.0568097783 0.8187201719 0.5725159331 0.619513634 2.115072e-01 174 0.9992063609 0.8830820697 0.0208103533 0.271749542 2.407214e-01 175 0.4691610557 0.1709448709 0.5638727772 0.205742766 7.858254e-01 176 0.9072538142 0.8136773086 0.7826462083 0.661408056 9.535864e-01 177 0.1765437901 0.9630451202 0.0341300871 0.065843770 6.814811e-01 178 0.2081198960 0.5721260966 0.8691763720 0.623829032 7.277410e-01 179 0.9020622084 0.7788336426 0.9740692992 0.897657196 6.079173e-01 180 0.5577519296 0.8870667333 0.0559192663 0.917418429 1.502496e-01 181 0.3174477716 0.5591419074 0.9091531408 0.689324741 3.603843e-01 182 0.3988291954 0.9360842388 0.9412023835 0.151519891 4.725731e-01 183 0.6296822894 0.2734462984 0.5310084445 0.435572824 3.481015e-01 184 0.8532116541 0.3789087695 0.4137729306 0.792026432 6.575724e-01 185 0.0019157159 0.9480523288 0.0558498637 0.690238338 5.564615e-01 186 0.2165099829 0.2713331431 0.7154832364 0.642090820 6.811029e-01 187 0.0528591487 0.4438034375 0.6167128414 0.823401734 5.188929e-01 188 0.2140440524 0.9970479552 0.5048359402 0.773988509 3.399884e-01 189 0.6131354324 0.2978879120 0.5568822010 0.096377717 8.602817e-01 190 0.3969264787 0.1390109258 0.3459203229 0.515168304 2.417488e-01 191 0.8599386800 0.0208728844 0.6721742607 0.940914826 1.805654e-01 192 0.3918516901 0.8487497859 0.5915288015 0.326262000 3.761560e-01 193 0.0001515732 0.9925137931 0.0870158968 0.911938774 1.018230e-01 194 0.9137823463 0.9482441705 0.6661104674 0.009660149 6.747976e-01 195 0.3185021740 0.4742731366 0.6751790117 0.216926428 7.693648e-01 196 0.5787437696 0.0562161675 0.0022941325 0.981683360 9.763876e-01 197 0.7559065737 0.3867585564 0.3821420241 0.401733570 4.982639e-01 198 0.5163620966 0.6031210783 0.5991793235 0.768042127 7.776593e-01 199 0.3772833832 0.2417656146 0.0676334375 0.179017943 5.219626e-01 200 0.8303419396 0.6909590135 0.2075274838 0.982338600 2.271104e-01 201 0.7211326503 0.6640000213 0.9592173509 0.649954116 7.933037e-01 202 0.5639012777 0.2657555621 0.3693294188 0.653942175 9.238735e-01 203 0.2644100867 0.4423922328 0.5288352752 0.791906926 9.936834e-01 204 0.4798865798 0.3394446452 0.4974069681 0.758591116 6.578006e-01 205 0.3535982657 0.3782534536 0.2380272320 0.377593074 7.628336e-01 206 0.7971998851 0.0415869437 0.9998667797 0.292781072 5.019907e-02 207 0.9474380957 0.1888907838 0.6537682402 0.442967691 5.412440e-01 208 0.8931702694 0.6448802592 0.6577227118 0.882041997 9.742949e-01 209 0.3331450461 0.0773383095 0.3559794263 0.825966841 5.472149e-01 210 0.3673691975 0.0164993445 0.3723789565 0.994233836 9.561312e-01 211 0.0161621387 0.4990861469 0.4040669564 0.980775977 9.391028e-01 212 0.6557840866 0.7605664600 0.5519499718 0.770046454 9.594055e-01 213 0.2010356160 0.9608684320 0.2316770859 0.630396901 9.695247e-01 214 0.5701537321 0.2108435808 0.0258280484 0.855769821 3.193294e-01 215 0.2032754768 0.1362372376 0.3567790452 0.898438573 6.869137e-01 216 0.4177006818 0.8037221890 0.7529435763 0.010452493 1.591609e-01 217 0.2006519388 0.8048699212 0.8357756212 0.413250533 4.062866e-01 218 0.1065086264 0.6491837867 0.2524011680 0.955415523 7.185965e-01 219 0.9091413908 0.4442297982 0.6562724640 0.923782989 3.390019e-01 220 0.0242540571 0.9883826412 0.8197069752 0.814794991 5.391943e-01 221 0.5542232401 0.8130007542 0.5624401649 0.064768330 9.854170e-01 222 0.6204521400 0.4963250675 0.2481274225 0.389114767 8.654488e-01 223 0.5394885978 0.2772327969 0.1048048458 0.480904928 6.280108e-01 224 0.0745619617 0.1558587539 0.7687460070 0.786436838 2.117698e-01 225 0.2163315983 0.6801391370 0.8055323400 0.686167828 5.312500e-01 226 0.0676476448 0.8733174466 0.3134679864 0.369815800 9.743214e-01 227 0.5916824974 0.0191610819 0.3581683834 0.237326151 8.822456e-02 228 0.2836414992 0.8274080611 0.8160740463 0.393665259 6.703462e-01 229 0.2384083509 0.3266589346 0.6799719462 0.637329965 6.830281e-01 230 0.8608596043 0.3201513030 0.1370995173 0.974307173 3.404101e-01 231 0.5796163038 0.9357685193 0.7526069139 0.197657710 1.402370e-01 232 0.2412788384 0.1476416474 0.1673243914 0.086085135 7.060136e-01 233 0.3736466474 0.0715341098 0.1843706726 0.281694533 8.314010e-01 234 0.3288877220 0.3422220629 0.3864410103 0.129961584 3.187321e-02 235 0.2161439408 0.8040320897 0.1244801774 0.744883351 3.924180e-01 236 0.6735424299 0.7795171738 0.4039235790 0.011590910 9.798672e-01 237 0.1124215857 0.4681109225 0.7216107910 0.829600236 8.380484e-02 238 0.8839060992 0.7237063833 0.2847857845 0.846192709 5.581730e-01 239 0.8819734182 0.1144621901 0.0116158570 0.725888924 3.569096e-01 240 0.9354603821 0.4831247553 0.5211163305 0.773195356 5.565796e-01 241 0.5767845511 0.2684109176 0.6986416469 0.510211729 6.383459e-01 242 0.5818626797 0.9073878161 0.1752908952 0.758165545 5.812421e-01 243 0.8275333415 0.5267427347 0.9090091116 0.344543359 5.143553e-01 244 0.4781270074 0.4272068595 0.4470525885 0.347394072 9.982698e-01 245 0.7396636771 0.5585044932 0.3863007929 0.517322354 9.656356e-01 246 0.8310291567 0.5190551940 0.0384265762 0.150503708 2.366544e-01 247 0.3675826853 0.3041335819 0.6259368560 0.022825431 5.922777e-01 248 0.1966541104 0.5881767110 0.3664567897 0.618014229 9.890586e-01 249 0.9975247402 0.8564226844 0.0035603254 0.301549508 1.862727e-01 250 0.8214518884 0.7014278157 0.5982679054 0.192868671 8.084818e-01 251 0.5489195844 0.6956572754 0.4245661907 0.657216802 6.434412e-02 252 0.9222708542 0.4495614534 0.4786704215 0.608280219 7.829196e-01 253 0.6170779034 0.1915762192 0.7051625785 0.100203936 7.363104e-01 254 0.7798010246 0.8297415115 0.9719362056 0.293687331 5.095400e-02 255 0.8295236412 0.7012862060 0.7040839291 0.076756692 4.320663e-01 256 0.7921326044 0.6680933705 0.7468525905 0.221332673 8.397830e-01 257 0.9725491845 0.8900939021 0.9264180593 0.579828278 4.414026e-01 258 0.5389117019 0.2145371423 0.7034534132 0.916396324 8.566715e-01 259 0.7039349154 0.5264377154 0.2763565620 0.608822979 4.653691e-01 260 0.9149633690 0.8434623827 0.6857282268 0.921606234 1.788714e-01 261 0.1539534209 0.5374164968 0.4353943137 0.066600877 9.459214e-01 262 0.2444682512 0.9467447461 0.2996765305 0.993174250 4.733773e-01 263 0.3417980904 0.6108143430 0.1764438688 0.070948276 4.806769e-01 264 0.4999335201 0.1733239812 0.0905771561 0.800303387 5.200107e-01 265 0.5066742979 0.2890426961 0.5416354267 0.498188548 2.367029e-01 266 0.5838516832 0.3779571787 0.4468531371 0.298747589 1.695074e-01 267 0.8129630184 0.4197585413 0.2865401660 0.384935115 4.562674e-01 268 0.2090866042 0.0312464098 0.6281993804 0.184038194 3.677554e-01 269 0.6440592636 0.1469305602 0.7661810701 0.838563889 9.731990e-01 270 0.7281985290 0.5473523897 0.6157018538 0.700302164 6.428662e-01 271 0.6646557718 0.2829667975 0.6044241658 0.799832808 3.056976e-01 272 0.9352728250 0.7136875202 0.0289973079 0.965694083 3.239665e-01 273 0.0728821249 0.7892174108 0.1717887009 0.521993847 4.899518e-01 274 0.0346095334 0.2692810481 0.7561781360 0.926916890 6.162302e-01 275 0.9504906929 0.7526574284 0.9379664683 0.068118802 8.603263e-01 276 0.5145728933 0.4559605923 0.9176913910 0.004695723 2.866595e-01 277 0.3976553013 0.3987538943 0.2839024831 0.293565525 7.702066e-01 278 0.7009254443 0.0133173962 0.9493629304 0.233285994 4.994754e-01 279 0.6224461349 0.6137198601 0.7054065103 0.907167699 6.948691e-05 280 0.3599138320 0.9875511052 0.2561543379 0.838496193 1.224435e-01 281 0.6635836901 0.4874325164 0.8553156825 0.733857719 5.639178e-01 282 0.4764982043 0.1185725085 0.6769570536 0.422804818 2.323357e-01 283 0.0649527388 0.3997544402 0.5082662296 0.536742677 8.150507e-01 284 0.5637654911 0.5065516052 0.3812452687 0.325095090 7.836266e-01 285 0.6024410788 0.7558844115 0.9651189120 0.758352714 8.845431e-01 286 0.0865011127 0.6272108338 0.0864172361 0.372212857 3.729183e-01 287 0.7726058052 0.8873297924 0.9845032052 0.926265871 3.398011e-02 288 0.2364514670 0.1204383390 0.6467557393 0.499837749 1.033158e-01 289 0.5903890356 0.2814705893 0.6270863349 0.544416004 7.191814e-01 290 0.0362968044 0.6435941649 0.8072538176 0.628011037 1.350844e-01 291 0.4776646101 0.9162236417 0.0477408909 0.195150346 9.917569e-01 292 0.9850660686 0.8860944102 0.8053446305 0.690133943 1.764347e-01 293 0.5599008847 0.8146942554 0.7810434583 0.804373655 5.683600e-01 294 0.5014732699 0.0134419615 0.0673355807 0.265304377 6.757556e-01 295 0.1202006293 0.8631854309 0.9486900396 0.120854634 5.592875e-01 296 0.2335651731 0.1641146073 0.5350339490 0.555488213 9.398174e-01 297 0.9651232909 0.4984271645 0.2115402070 0.966395828 4.653873e-01 298 0.1128552298 0.1122542853 0.5322054287 0.839569817 5.417432e-01 299 0.8394041124 0.2560082425 0.1669750074 0.328798706 8.281256e-01 300 0.3041334234 0.7385249771 0.7241052580 0.278196611 8.285159e-02 301 0.4291426220 0.8197907398 0.0797945955 0.593694864 2.967888e-01 302 0.4523428532 0.7067194383 0.8770909235 0.743807043 8.027994e-02 303 0.7211188022 0.3782203167 0.6527704687 0.173949218 3.157459e-01 304 0.5325777873 0.2437774444 0.8282919656 0.346972036 8.068654e-01 305 0.2279141766 0.5629842561 0.9597962128 0.896484934 1.616834e-02 306 0.9273555675 0.0055806914 0.6181486198 0.898301309 4.427710e-01 307 0.0714621588 0.1762667750 0.0954093228 0.078771684 3.226275e-01 308 0.0913349085 0.1022600732 0.3445813630 0.739462374 9.278758e-01 309 0.8353813579 0.0021331101 0.6448888553 0.433755042 6.941956e-01 310 0.8717507403 0.6973458694 0.9974252689 0.954311525 3.399146e-01 311 0.3402612589 0.0207138949 0.7791891245 0.138285730 9.553810e-01 312 0.1030242851 0.6788007997 0.0091685629 0.080596239 6.100400e-01 313 0.8796044991 0.7409640790 0.5888034666 0.482930689 9.066809e-01 314 0.4184627694 0.1518161653 0.9470163304 0.640979418 7.321634e-01 315 0.4914691530 0.4347692737 0.8240045996 0.113172951 7.336182e-01 316 0.1891047948 0.9546289963 0.9326387078 0.084773930 7.528037e-01 317 0.4091451312 0.4061187478 0.4935212114 0.063884293 6.897970e-01 318 0.1740348001 0.7072475406 0.7000157465 0.471941573 4.158820e-01 319 0.2931793360 0.2669634703 0.4962130494 0.383236896 9.967678e-01 320 0.2781334203 0.2826841313 0.7971504361 0.431237619 1.332127e-01 321 0.6324098671 0.9961036481 0.7603126075 0.319680428 6.017842e-01 322 0.5705256499 0.3483372750 0.9408938496 0.273614281 7.343426e-01 323 0.1463123327 0.1304147257 0.0196428671 0.466923666 8.377781e-02 324 0.3067270615 0.0682549430 0.0426511776 0.288729959 6.353246e-01 325 0.8781642185 0.3042296777 0.4770269010 0.298304684 6.386353e-01 326 0.0776095102 0.0545013193 0.6625969552 0.251515612 3.353573e-01 327 0.4279278640 0.6352059934 0.2660763490 0.261018408 9.899702e-01 328 0.0869317167 0.5169583762 0.3695479026 0.720178066 3.620540e-01 329 0.8999204701 0.7806742371 0.7687030011 0.375773629 6.745213e-01 330 0.8751550377 0.3289561400 0.4191904066 0.427931608 1.081395e-01 331 0.0761351229 0.5991958415 0.2431351477 0.914929265 5.326279e-01 332 0.9381568427 0.4902186608 0.7542906129 0.672044334 3.062858e-01 333 0.8256581253 0.6891283139 0.8559265318 0.364524166 5.110359e-01 334 0.7706560523 0.2872826036 0.2328567246 0.504470113 7.242145e-01 335 0.6093809877 0.9180088332 0.2086923716 0.571133680 3.943475e-01 336 0.6783998506 0.8340922757 0.5525323339 0.361998960 1.303991e-01 337 0.8349843030 0.8474897714 0.5272933764 0.799968964 2.671729e-01 338 0.6991907230 0.4609518785 0.2405392446 0.923717583 4.763034e-01 339 0.8896550976 0.1483478416 0.9771987102 0.383924895 4.411620e-01 340 0.7373167558 0.2668425364 0.4646588794 0.290820904 5.789469e-01 341 0.7777801065 0.1640932390 0.6075391858 0.002127058 7.864690e-01 342 0.1142391388 0.6719365513 0.4796372068 0.594974790 4.127344e-02 343 0.9957590452 0.6051899521 0.3506976240 0.360922266 5.062548e-01 344 0.8975299238 0.8794793843 0.4141596090 0.132358324 5.256242e-02 345 0.3591199489 0.8844800305 0.0099952731 0.824255990 2.001769e-01 346 0.5356684977 0.0438097841 0.3991168796 0.057423466 3.378591e-01 347 0.8537007736 0.3376973567 0.7265061915 0.990835784 4.065400e-01 348 0.6791082718 0.3562199960 0.9257631865 0.600217237 7.820375e-01 349 0.5602437346 0.2175566691 0.5589008050 0.029843965 2.610109e-01 350 0.6625470396 0.7728067513 0.3640043263 0.444563451 8.506265e-01 351 0.0657600570 0.4441296456 0.8104015475 0.270668493 7.601029e-01 352 0.8657236823 0.7491532350 0.8000338343 0.789722052 2.227382e-01 353 0.8693265212 0.1323963902 0.0203081183 0.422943778 8.704134e-01 354 0.6427183610 0.4065263735 0.2248546493 0.780684578 2.740526e-01 355 0.5759076793 0.6806553851 0.8696183837 0.845955456 9.591285e-01 356 0.6498449650 0.2195812284 0.1431319460 0.681847408 5.779802e-01 357 0.3165733689 0.2931888984 0.3416584737 0.114969182 1.011104e-01 358 0.8982761570 0.9184429229 0.7322741628 0.443741165 4.404219e-01 359 0.4469846345 0.5782766959 0.2848459983 0.706270285 4.893691e-01 360 0.1091156455 0.1869821378 0.5436482152 0.204792706 9.771976e-01 361 0.3657996678 0.5740619248 0.3449009201 0.116321134 7.446984e-01 362 0.1806305090 0.2856565488 0.5067477161 0.821413193 3.083452e-01 363 0.1164003408 0.0459067044 0.2686422307 0.954782017 4.855251e-01 364 0.8020089313 0.7331166179 0.9361567646 0.789247320 3.798315e-01 365 0.1938385800 0.5443917396 0.7268236890 0.714485556 8.691682e-02 366 0.7173356623 0.7768663752 0.9018600665 0.963223044 2.476719e-01 367 0.5296206458 0.5590656023 0.0882430347 0.370041748 6.353881e-01 368 0.0742895349 0.3294574337 0.3487890468 0.698375692 3.642144e-01 369 0.0096567774 0.1633153663 0.2697227211 0.612484920 3.893997e-01 370 0.0007421148 0.3710664017 0.3654992618 0.858424162 9.942708e-02 371 0.0144137791 0.4277930632 0.1251683424 0.439849179 5.469326e-01 372 0.6370710037 0.0225923632 0.9081482203 0.981841748 5.412906e-01 373 0.1093050879 0.2239815341 0.9036308534 0.475337584 7.634477e-01 374 0.0706267995 0.6466684432 0.8195659919 0.827542959 7.745236e-02 375 0.7718457142 0.8100467415 0.1238228502 0.858416016 9.465533e-01 376 0.1993048138 0.7963158851 0.9567167670 0.136876087 7.876701e-01 377 0.8673933318 0.4083252754 0.5028514632 0.270459414 4.284302e-01 378 0.0327129478 0.6679480514 0.6981502497 0.241763491 5.291740e-01 379 0.1309007367 0.8956746466 0.4353174695 0.311975433 2.617224e-02 380 0.6862117657 0.0567090861 0.2643035925 0.947567889 3.024849e-01 381 0.9820180680 0.7590667398 0.4497556726 0.826217088 2.475621e-01 382 0.4473482983 0.3026340993 0.2187455115 0.525228607 8.370121e-01 383 0.7837314866 0.9404827747 0.6447940255 0.692938740 6.625183e-01 384 0.4695104088 0.3393136084 0.3539974955 0.076961612 7.125074e-01 385 0.4953724076 0.3190722512 0.9209394744 0.593576920 6.993314e-01 386 0.7110873724 0.5765278214 0.8290389117 0.975892123 4.599566e-01 387 0.2224491003 0.2692736811 0.2377511768 0.800969388 3.679071e-01 388 0.7823642467 0.8590320838 0.5696742025 0.587688962 7.797665e-01 389 0.0418753801 0.8708307620 0.1299097429 0.686934761 3.754749e-01 390 0.3174569593 0.9145249254 0.3968546884 0.463118966 2.330016e-01 391 0.4599263112 0.9685912782 0.2395614071 0.702558178 9.325322e-01 392 0.3235118547 0.7471794712 0.9446711387 0.471918447 3.287914e-01 393 0.0868029166 0.5084290067 0.2607131617 0.270282100 9.171089e-01 394 0.6068624684 0.9036543421 0.5734851684 0.183628874 2.812521e-01 395 0.8760592777 0.3600250112 0.4945351148 0.963925146 8.247189e-01 396 0.1094516823 0.6648093811 0.0002113678 0.381599879 9.678066e-01 397 0.4957438977 0.3097762633 0.6246782839 0.210444017 5.248227e-01 398 0.8213692890 0.6233094407 0.2159364189 0.406675840 4.955265e-01 399 0.0229937893 0.6928474947 0.7193192698 0.296294143 5.317764e-01 400 0.6782247606 0.7866539420 0.7101236880 0.041422781 2.519602e-01 401 0.5135679946 0.4825126145 0.2595921732 0.038293313 5.709323e-01 402 0.2084596925 0.2265437515 0.3525143140 0.768503838 4.490639e-01 403 0.4135049034 0.4232750146 0.5646171367 0.069003359 9.856659e-01 404 0.5864820946 0.5760116780 0.6705476989 0.292407878 1.860817e-01 405 0.2408297034 0.0869413330 0.7113251509 0.030035041 2.684469e-02 406 0.4996919823 0.2686212256 0.2869927129 0.443136572 5.427097e-01 407 0.6976517173 0.5914358348 0.4772423580 0.645434657 5.177249e-01 408 0.6071811004 0.0431133511 0.2221226352 0.558366798 6.757148e-01 409 0.4892499996 0.5023605570 0.1043147992 0.252860805 7.848952e-01 410 0.1347163925 0.8824594235 0.2700915858 0.396543218 5.882499e-01 411 0.4283737643 0.0430903563 0.1562245784 0.110033886 9.573125e-02 412 0.6296868890 0.8916814390 0.5608932087 0.514378582 4.608000e-01 413 0.7040708861 0.4345444480 0.4374845265 0.656432379 6.506390e-01 414 0.6781820890 0.1922134126 0.9248232390 0.437797368 3.952176e-01 415 0.0526506035 0.4131393754 0.8803833262 0.649732224 6.173803e-01 416 0.7959502731 0.9659142359 0.1647356863 0.043900094 2.414191e-01 417 0.0500476982 0.7080166151 0.8463139313 0.121874181 7.449012e-01 418 0.5258683611 0.7866339283 0.2648841939 0.825389879 6.909416e-01 419 0.1958894294 0.3899555679 0.8682446461 0.996587435 3.139904e-01 420 0.0855531532 0.7530330606 0.7656025854 0.180940423 4.797922e-01 421 0.4479133089 0.7504540395 0.8510300457 0.917122714 8.110157e-01 422 0.5358806332 0.1840613915 0.8404297752 0.695809044 4.860580e-01 423 0.8574897067 0.4740581512 0.9840828453 0.388464857 4.838642e-01 424 0.7508345970 0.6199492190 0.3873363633 0.212975476 6.846711e-01 425 0.3002946340 0.2649874613 0.4472604445 0.670382250 7.239693e-01 426 0.8132292426 0.5420388943 0.4577483626 0.621782153 8.033528e-01 427 0.4620008818 0.2777338503 0.4325356120 0.413525639 9.745306e-01 428 0.3230311878 0.6950940713 0.2689813899 0.850076854 3.209517e-01 429 0.4746141164 0.4966602982 0.8828971023 0.542695990 5.727530e-01 430 0.4600041225 0.9534178204 0.8522334506 0.302213856 8.852952e-01 431 0.8426773683 0.9747501956 0.0451706389 0.388267011 4.896859e-01 432 0.4915075402 0.4375492195 0.5891421444 0.690327780 6.521933e-01 433 0.7539070100 0.1259638229 0.5254330765 0.614419028 3.255130e-01 434 0.1207034974 0.1975698317 0.0973578235 0.084314038 8.940771e-04 435 0.6456332053 0.7307007704 0.2282598969 0.646824725 8.661351e-01 436 0.5734037792 0.5650301496 0.1870769637 0.165129308 5.357737e-01 437 0.0494311822 0.9188226117 0.7185625699 0.432994389 5.269118e-01 438 0.0277321704 0.9803931457 0.2070943683 0.710366176 2.701794e-01 439 0.0178599542 0.6922797281 0.7564428258 0.667626088 1.513596e-01 440 0.4946952646 0.4058102395 0.1119535868 0.220409061 5.919344e-01 441 0.8835002473 0.5677845615 0.1081138004 0.675165835 4.661971e-03 442 0.6331623469 0.7801744484 0.1189808135 0.414237151 1.799904e-01 443 0.5561390182 0.3673749596 0.7417061792 0.821517335 6.608645e-01 444 0.1873597139 0.9747207032 0.8465773889 0.046875914 3.714898e-01 445 0.9465421280 0.0706294128 0.0714444439 0.945401258 7.603055e-01 446 0.7546351640 0.8103325381 0.7578929288 0.073837923 3.450114e-01 447 0.8472598379 0.5312037698 0.7027316322 0.347222402 8.442074e-01 448 0.9271843736 0.4976025869 0.2601484831 0.881693521 8.947889e-01 449 0.6015616008 0.0193471697 0.5047459286 0.771554465 6.620372e-01 450 0.3465780267 0.4549790232 0.5562954093 0.799919833 6.022381e-01 451 0.4260562521 0.7672826869 0.2570241319 0.157866101 9.280257e-01 452 0.3984505145 0.4370649243 0.3836146572 0.346790810 8.788060e-01 453 0.2113784782 0.5443354140 0.3122317644 0.552217386 6.442702e-01 454 0.0376454894 0.0106201086 0.4465854045 0.577008852 9.825328e-01 455 0.4784657105 0.8741496983 0.1614553235 0.541224399 3.360020e-01 456 0.0600217197 0.5047202792 0.0530216661 0.561827486 7.207989e-01 457 0.4019439009 0.7448705870 0.6991459318 0.858834204 2.629050e-01 458 0.5859271495 0.8682614388 0.2194845770 0.433029509 8.678002e-01 459 0.7683075613 0.8662802191 0.0412932397 0.332626628 3.531620e-01 460 0.8506514311 0.7222218062 0.8751874901 0.020963814 7.421935e-02 461 0.2668082241 0.9159252078 0.5414669486 0.993659624 5.213159e-01 462 0.8532688692 0.5136732112 0.6805261644 0.969336346 1.181152e-01 463 0.3914166926 0.1625697238 0.7115389123 0.613171886 5.563475e-02 464 0.9178691409 0.5312489253 0.1411212357 0.921012539 5.166795e-01 465 0.3401386235 0.7289904719 0.1812944608 0.052570121 1.991569e-01 466 0.9952564673 0.4240395194 0.3330458766 0.654532294 4.914733e-01 467 0.2361274951 0.7220603130 0.1801518665 0.226354291 2.317657e-01 468 0.9484631482 0.1171784431 0.3814109282 0.151987331 1.259127e-01 469 0.1971958587 0.5476996582 0.8278281968 0.006280510 1.377210e-02 470 0.4303416610 0.7595192476 0.2089759717 0.666954212 4.647386e-01 471 0.3610162260 0.8105125707 0.5239570760 0.831454685 2.229034e-01 472 0.4729091411 0.9529160575 0.1585315673 0.368948445 2.876274e-01 473 0.4228429645 0.1028229753 0.1130753367 0.712432174 7.977939e-01 474 0.4466349385 0.8416559217 0.2405777008 0.997086962 1.029476e-01 475 0.7607556819 0.4972156934 0.0339827556 0.196671375 5.321757e-01 476 0.2251486822 0.0994993590 0.7663894410 0.888381128 5.743740e-02 477 0.7277364382 0.9162787397 0.1941889157 0.516441212 3.768768e-01 478 0.4330620549 0.7756441284 0.6987062974 0.021771210 8.454757e-01 479 0.2342932397 0.4582609411 0.1460496057 0.370035785 1.053588e-01 480 0.4623888060 0.4177094412 0.3688633731 0.421409196 8.330933e-03 481 0.7287827786 0.5170731510 0.5754181799 0.829369209 4.207426e-01 482 0.2487997608 0.7418034966 0.0145854305 0.140747132 9.087611e-01 483 0.1584943631 0.4967057044 0.1377874245 0.246570810 5.367151e-01 484 0.8771969816 0.3497019310 0.5086943456 0.857494939 1.820041e-01 485 0.3275236553 0.6556553524 0.8936726141 0.695424152 5.797824e-01 486 0.3876958531 0.7141255415 0.7401520242 0.730453796 8.218417e-01 487 0.0859429245 0.3932208200 0.4963257634 0.023122997 7.484599e-01 488 0.0478853509 0.5820773568 0.4347384747 0.495591412 2.514400e-01 489 0.7640398338 0.6219047029 0.9018226827 0.367291192 6.662036e-01 490 0.7738003614 0.7122801496 0.3401500941 0.813406494 6.130019e-01 491 0.2811919176 0.1084264976 0.8185484011 0.524333827 5.448712e-01 492 0.4107365129 0.4592057569 0.4585471388 0.939687159 9.684651e-01 493 0.6586276228 0.2361251698 0.3982894844 0.736938675 7.158374e-01 494 0.8373886999 0.6760577103 0.4539175981 0.140440820 1.205565e-01 495 0.3794597711 0.0379660081 0.8831326116 0.119593804 3.948079e-01 496 0.7899617921 0.3214019027 0.7558209519 0.178459910 4.474716e-01 497 0.6494789242 0.1239918764 0.7879019941 0.566567896 2.093041e-01 498 0.4682361067 0.8054523449 0.0313273268 0.917019749 3.626654e-01 499 0.8111232512 0.4842337884 0.9905550019 0.443862415 7.314548e-01 500 0.7845828277 0.6678469195 0.0099117353 0.938890793 8.898346e-01 501 0.8596741629 0.0578216626 0.1373633842 0.242686614 3.390618e-01 502 0.0336864635 0.9917070670 0.0058358226 0.576859908 7.295833e-01 503 0.0090650320 0.1259467914 0.5226027011 0.221464730 8.944133e-01 504 0.7393250617 0.4164622889 0.2562337969 0.309255973 1.536401e-01 505 0.1968866454 0.6010752646 0.8266404339 0.241861298 8.562246e-01 506 0.5490019484 0.0597066092 0.6746688485 0.988395112 2.257646e-01 507 0.6120238367 0.8842251308 0.0106372475 0.657762127 4.932302e-01 508 0.5809495263 0.6934050722 0.7337883902 0.881573875 2.639341e-01 509 0.5484295946 0.7735354542 0.8462061526 0.510266755 8.889722e-01 510 0.0912326679 0.0196348897 0.8939744835 0.996975462 5.472280e-01 511 0.0193336261 0.3792007137 0.3810819855 0.306738382 6.755923e-01 512 0.9039375873 0.6896352544 0.2266765735 0.024039615 8.621554e-01 513 0.8407693366 0.1688590143 0.8865542472 0.448400312 5.756914e-01 514 0.3070470826 0.9788418170 0.5187856420 0.449613948 2.021171e-01 515 0.7346552406 0.4564591136 0.0886887216 0.213024601 1.288321e-01 516 0.1057571096 0.1084782106 0.8899245381 0.383389150 6.462186e-01 517 0.6396863880 0.7724902784 0.1619776264 0.529304153 4.767796e-01 518 0.5444227399 0.5911405408 0.0523558804 0.344859310 3.515537e-01 519 0.5455354273 0.4614260718 0.7244580921 0.583712463 3.205922e-01 520 0.1358930182 0.7673941576 0.4223320177 0.252359887 4.387520e-01 521 0.4758632802 0.5606147940 0.3586316842 0.686842689 8.451977e-01 522 0.1343210761 0.4462387625 0.1000703084 0.198462720 1.241996e-01 523 0.1480307344 0.2055510634 0.9462425506 0.091771150 7.064920e-01 524 0.2279162470 0.5673055674 0.9074591370 0.592499505 5.908225e-01 525 0.3441012220 0.3722269435 0.7897777304 0.841536568 8.157750e-02 526 0.1684615102 0.4908088224 0.6635147301 0.656268670 1.541303e-01 527 0.3644688802 0.6197951366 0.3999979883 0.961548903 2.104624e-01 528 0.3026080551 0.0421625217 0.9051791250 0.189580926 5.986782e-01 529 0.7649763497 0.8567843242 0.4777257033 0.060635211 4.449084e-01 530 0.5515219311 0.3415239272 0.3919794145 0.821882782 8.111506e-01 531 0.6972977808 0.7972091183 0.3467263887 0.019405104 3.704000e-02 532 0.0387044246 0.2070823517 0.8566835134 0.785161634 7.065999e-01 533 0.9039842428 0.6613530181 0.4024148083 0.620648935 1.516588e-01 534 0.4088809304 0.6672650862 0.5995888044 0.338171331 4.423938e-02 535 0.5378535052 0.9996105607 0.5368517251 0.006045962 3.168881e-02 536 0.3881504608 0.0245178782 0.3426053128 0.328514404 5.110480e-01 537 0.3916057695 0.6706551816 0.8657774576 0.036358000 4.571243e-01 538 0.0028829025 0.4516399151 0.7030717239 0.455910068 4.933923e-01 539 0.4001601264 0.1733659208 0.2999020617 0.905883759 5.860176e-01 540 0.8600010681 0.4321305510 0.5080972612 0.443594804 3.532380e-01 541 0.8715729243 0.9938263360 0.5023601486 0.837488207 8.630757e-01 542 0.9454444251 0.1035464187 0.2287527740 0.099763853 9.503204e-02 543 0.2098055060 0.3725548622 0.5470067055 0.847507765 6.545298e-01 544 0.4703532252 0.9060373341 0.2752195084 0.092471990 5.282526e-01 545 0.2748672708 0.3570935251 0.8742467877 0.607690911 3.772721e-01 546 0.0433487017 0.5896492933 0.9129612078 0.755463495 4.825281e-01 547 0.9404139488 0.0481122537 0.6314002185 0.407415794 5.865745e-01 548 0.6125317849 0.3405352619 0.9933596929 0.618332138 4.827819e-01 549 0.4461862082 0.5074156527 0.0893472889 0.997433731 2.901876e-01 550 0.2108046531 0.4348503361 0.3725137783 0.732523779 9.000062e-02 551 0.4465482270 0.7043836070 0.3406603502 0.058125994 8.740000e-01 552 0.5953917825 0.2501409757 0.4145573166 0.527952925 8.065012e-01 553 0.7835872069 0.9648590535 0.6512023690 0.225035774 4.342820e-01 554 0.5583098917 0.9271848723 0.2747644160 0.034011265 3.326598e-01 555 0.0164725739 0.1276548684 0.3477022895 0.940272366 6.519734e-01 556 0.1401534586 0.9556691791 0.8838821112 0.082095703 1.772069e-01 557 0.6577758456 0.9237059227 0.7349070930 0.675827107 8.381635e-01 558 0.3120215600 0.8630618234 0.3595919712 0.646322087 5.722389e-01 559 0.2710284372 0.3668614225 0.5772195486 0.732232192 9.656908e-01 560 0.3835913802 0.8062818865 0.2430923579 0.391548166 1.499482e-03 561 0.2820639270 0.9741106499 0.0780810083 0.262342765 3.725171e-01 562 0.6476774705 0.6113039821 0.3379194401 0.283364764 2.244343e-02 563 0.8264224837 0.6389387369 0.2133443239 0.304230640 6.614637e-02 564 0.1684982514 0.5108453108 0.8730994116 0.952562007 2.711628e-01 565 0.4681478492 0.7290914275 0.6655910178 0.059033521 2.882402e-01 566 0.9135101677 0.7844511706 0.6940691071 0.521119098 6.174569e-01 567 0.5153865137 0.6280066038 0.7529852346 0.926406451 3.512761e-01 568 0.1224220723 0.1532868505 0.6064663581 0.035015481 9.069524e-02 569 0.3729660993 0.3849294826 0.4162248687 0.170865999 3.499442e-01 570 0.6174877922 0.4830182882 0.8551577611 0.948893960 3.666638e-01 571 0.7798820683 0.8074540063 0.0701762645 0.811303859 6.321102e-01 572 0.8141681084 0.4763889555 0.6402247904 0.118680977 8.215278e-01 573 0.8072200266 0.4033720838 0.5565765412 0.933369923 2.928074e-01 574 0.4192860171 0.5062515556 0.9369511518 0.923694299 6.381643e-02 575 0.3336501552 0.7181739374 0.3037285402 0.663063108 3.577619e-02 576 0.2333190406 0.6384027032 0.1179364619 0.621289150 7.826201e-02 577 0.6390773912 0.0315628725 0.0034079296 0.544522583 6.646238e-01 578 0.9833549089 0.5804860534 0.4200558870 0.108356553 9.020294e-01 579 0.2008158572 0.3473298543 0.3942718485 0.882981715 5.132987e-01 580 0.6873001435 0.5002243656 0.8098510485 0.548252749 7.759410e-01 581 0.5232526087 0.5436942442 0.7394590937 0.986454384 7.662074e-01 582 0.7243103618 0.4520305509 0.1652481027 0.533395649 2.761340e-01 583 0.5373778436 0.6290667017 0.0718271255 0.221661672 5.003754e-01 584 0.2417254543 0.3433562904 0.8498937809 0.864886062 8.246796e-02 585 0.0760574646 0.7703167831 0.6272670222 0.711507613 7.299976e-01 586 0.6699323892 0.2712414272 0.8532035290 0.287744463 9.483736e-01 587 0.5009339042 0.9915771615 0.3771723702 0.246797540 7.356113e-01 588 0.3031111252 0.6791734453 0.1358731689 0.205903129 2.677441e-01 589 0.8653934507 0.3075919577 0.9591189560 0.164791528 3.823351e-02 590 0.9512251944 0.8444743052 0.6169150190 0.151851988 2.594963e-01 591 0.2299444869 0.2699593364 0.0633073712 0.337680844 8.593229e-01 592 0.8090767509 0.3491266835 0.9062857677 0.731222640 1.136799e-01 593 0.8875121858 0.7116214407 0.3816257541 0.229801290 2.956320e-01 594 0.0956568804 0.0888715349 0.8685165911 0.571338476 9.496462e-01 595 0.9680835889 0.6196359964 0.6548132098 0.281034419 4.816263e-01 596 0.2339390421 0.1134912746 0.1338451568 0.632273507 6.795302e-02 597 0.1876290324 0.6431201152 0.6688683571 0.352891829 7.041394e-01 598 0.1103235064 0.7108283620 0.2381389178 0.177926907 5.603788e-01 599 0.9005565967 0.7975010527 0.6413422818 0.317935640 1.315685e-01 600 0.0959906711 0.2517449632 0.6953449131 0.886165070 3.520248e-01 601 0.1814462831 0.2464492123 0.2428896630 0.629397307 3.648666e-01 602 0.7031895248 0.8790988030 0.5745146840 0.192540054 8.839200e-02 603 0.5910929132 0.8323000772 0.2072549914 0.794825767 7.403007e-01 604 0.9162794969 0.4879544913 0.5999331572 0.553021319 6.472658e-02 605 0.9888230127 0.2273209461 0.1320640503 0.583122615 2.292195e-01 606 0.4466462396 0.7951611818 0.7163238716 0.111165432 6.223114e-01 607 0.4109430443 0.4526698415 0.6658871539 0.055975484 5.476906e-01 608 0.1328400464 0.7751484194 0.8014833727 0.026586966 6.009309e-01 609 0.7664368560 0.6650214796 0.8394974649 0.796361954 5.203079e-01 610 0.1854454621 0.1539397850 0.5727987136 0.859107810 9.724846e-01 611 0.7509588073 0.2780542122 0.1968538989 0.138429189 7.375585e-02 612 0.9074370672 0.3251694136 0.1056423029 0.376905116 9.260506e-01 613 0.1178869291 0.1247338115 0.9974537201 0.091709557 3.123266e-01 614 0.0469024812 0.6889863366 0.6949616608 0.587234502 1.082442e-01 615 0.1484412735 0.5968929187 0.1998363563 0.185659013 6.482614e-01 616 0.8210094008 0.0725176199 0.6460069709 0.372610050 3.817773e-01 617 0.5549458107 0.7114344193 0.7727056891 0.842564460 4.825940e-01 618 0.3284571527 0.1717800761 0.9150849401 0.928022968 2.964681e-01 619 0.5617234516 0.4060857871 0.5447506779 0.991730924 4.116851e-01 620 0.2218145537 0.6428353186 0.4462690975 0.180419530 9.495265e-01 621 0.6815757749 0.4869625089 0.7706935983 0.124916652 2.543316e-01 622 0.3976176258 0.4112871403 0.2232044286 0.601970080 5.856749e-01 623 0.9302335104 0.7058372160 0.2738621628 0.357624098 4.464640e-01 624 0.1203466251 0.6778112825 0.8039241112 0.283917557 5.086606e-01 625 0.5158565340 0.8589938758 0.7603616987 0.757245618 5.810929e-01 626 0.5419992688 0.1409501764 0.5972314011 0.378633605 1.498809e-01 627 0.4644107393 0.2047461346 0.4112030938 0.081525275 3.996346e-01 628 0.8369497349 0.7150264331 0.6943341750 0.449338492 4.065572e-01 629 0.1867417134 0.2349077109 0.5118668289 0.008123271 7.465035e-01 630 0.5425374825 0.7320022769 0.7225689276 0.056100357 9.743324e-01 631 0.9241157686 0.4882950098 0.5148334769 0.294380933 2.235191e-01 632 0.7712359671 0.9125933389 0.1133061245 0.017098532 7.013883e-01 633 0.0460593526 0.9790674066 0.5507777117 0.216266027 9.388441e-02 634 0.1461966210 0.3678647901 0.9606390365 0.256912482 9.466607e-01 635 0.8583672526 0.4514849845 0.5924144168 0.275959739 8.218458e-01 636 0.3796334988 0.5228201388 0.8796267982 0.914260088 8.577997e-02 637 0.9610019554 0.2582042993 0.2585091207 0.248356227 9.530522e-01 638 0.2226643402 0.5756969464 0.4010980607 0.370071233 7.605457e-01 639 0.8878771043 0.4456070380 0.1770016337 0.079971947 7.741485e-01 640 0.2048562940 0.9132214689 0.5876504485 0.911009921 4.087940e-01 641 0.1734649974 0.4730995838 0.8521742683 0.283892886 7.160428e-01 642 0.2770436495 0.4960478283 0.6533297694 0.337937349 3.102812e-01 643 0.9079919949 0.3234175995 0.1345435700 0.276223309 3.642992e-01 644 0.1404491214 0.3107680771 0.2690007351 0.785765260 8.150575e-01 645 0.7797426870 0.5068352411 0.9562994044 0.341920928 9.123921e-01 646 0.2014403057 0.3889138778 0.6121131913 0.920091983 7.444889e-01 647 0.6695721492 0.0883990279 0.2482306608 0.965957357 2.299187e-01 648 0.8657230681 0.7457466202 0.6249744541 0.324577653 3.874338e-01 649 0.5862377898 0.1959867699 0.2212410027 0.563064340 6.686219e-01 650 0.4096797309 0.1860773508 0.5328029690 0.314105657 7.770745e-01 651 0.0554621299 0.0790727723 0.3894717460 0.840018364 6.212918e-02 652 0.6932099925 0.3149603689 0.0017989841 0.561205384 5.865121e-01 653 0.0772926114 0.1772280850 0.9093107912 0.192656148 8.233684e-01 654 0.5872192709 0.9547599677 0.2526479778 0.381830367 4.776642e-01 655 0.2018798466 0.0876697104 0.1663716871 0.600859793 9.508464e-01 656 0.5674000036 0.0061985739 0.9192362998 0.882563850 8.127025e-01 657 0.2606946430 0.3112595235 0.6329974558 0.790751529 6.720512e-01 658 0.6728667193 0.1917230377 0.7566862372 0.909346010 4.726861e-01 659 0.6331451386 0.2484863934 0.0995883788 0.343220247 3.827711e-01 660 0.0108745436 0.6954743220 0.4698274387 0.426440760 6.104461e-01 661 0.2674112793 0.0800102965 0.2643972533 0.478243147 5.113606e-01 662 0.8316930223 0.2490380928 0.2804101042 0.903230454 1.032151e-01 663 0.1727495792 0.7230207766 0.5985144922 0.054255187 5.225881e-01 664 0.0849175956 0.5175191101 0.3752188238 0.669995113 5.585850e-01 665 0.0127395198 0.8534449597 0.3917715163 0.141239679 7.482148e-01 666 0.9716106884 0.9878049642 0.5612602045 0.210602174 7.658354e-01 667 0.8348956225 0.8170615705 0.3934594379 0.369965748 6.439354e-01 668 0.6081504258 0.4918447125 0.8814690269 0.436927830 8.248758e-01 669 0.9960278347 0.6785293575 0.2428395639 0.235090057 2.776699e-01 670 0.8508732293 0.2890700537 0.9828776065 0.441170447 6.066821e-01 671 0.0809824888 0.1729177088 0.0553947210 0.410132100 6.038980e-01 672 0.5399313546 0.3313864928 0.6883659216 0.780303753 2.586869e-01 673 0.6266353892 0.1214897714 0.1065400506 0.279638008 9.539392e-01 674 0.5492108434 0.1729518115 0.0980195184 0.091090638 9.093982e-01 675 0.2353585865 0.3663545211 0.2753355789 0.370521680 9.157402e-02 676 0.9186354124 0.1570131038 0.7628696139 0.732964899 9.533825e-01 677 0.7215894382 0.3251391754 0.7349442197 0.390790382 8.812004e-01 678 0.4072342522 0.5741532256 0.2547490085 0.101868185 8.243583e-01 679 0.0320119020 0.6594430758 0.6453604414 0.593536775 8.153255e-01 680 0.1372886975 0.5797297091 0.0693826037 0.581811009 8.454477e-01 681 0.2251470848 0.5863237504 0.2929798686 0.064685504 2.603856e-01 682 0.2438405859 0.7252879883 0.2253684865 0.775818338 3.057060e-01 683 0.0316104067 0.4859786753 0.8827559745 0.927763019 8.996976e-01 684 0.8333529353 0.8842396680 0.4077094197 0.951228141 4.519069e-01 685 0.9327697556 0.0735161284 0.8669178819 0.646814072 2.359719e-01 686 0.8194053553 0.0149510549 0.3264283007 0.027954239 9.877356e-01 687 0.4739673156 0.6126506121 0.2575328399 0.626472661 9.376579e-01 688 0.7064988560 0.4339322769 0.0102567382 0.463410470 1.281101e-01 689 0.3043627338 0.3500925838 0.7521409774 0.416593156 6.536726e-01 690 0.2842362507 0.1287338857 0.2068039351 0.479221793 7.736288e-01 691 0.3959755872 0.1325092788 0.0840746027 0.595622467 6.474641e-01 692 0.4561319870 0.7985444975 0.2368864699 0.953477525 6.616659e-01 693 0.0085370028 0.1395568491 0.1916677223 0.754163359 7.051648e-01 694 0.3519615321 0.7734501422 0.3515878150 0.644991686 7.748586e-01 695 0.1480507238 0.6036012371 0.1661285183 0.666506897 1.195510e-01 696 0.4659092973 0.5883147453 0.9563295639 0.460264088 9.507760e-03 697 0.2261838936 0.2115620363 0.1406688350 0.502033343 4.627967e-01 698 0.0966137387 0.7810345865 0.1228184656 0.399053896 6.973598e-02 699 0.0595416413 0.9259417355 0.4779730642 0.556131563 7.944641e-01 700 0.2826902184 0.2328596772 0.3735294535 0.980496759 4.502118e-01 701 0.6457913155 0.9648015301 0.0866789357 0.340338276 3.152243e-01 702 0.8387226122 0.9889767615 0.5429523443 0.467376266 9.904297e-02 703 0.3480575341 0.5784635388 0.8746951353 0.090592088 3.027450e-01 704 0.0110021131 0.7382813357 0.8855151345 0.097531161 6.932548e-01 705 0.4369894997 0.4602281952 0.1504359660 0.592339120 2.363977e-01 706 0.3866372849 0.9427196847 0.8209371674 0.750375597 1.012006e-02 707 0.2527379994 0.2612617402 0.4743072900 0.701019078 1.618566e-01 708 0.0489493115 0.5330059354 0.2273620877 0.057604318 6.559704e-01 709 0.1701454427 0.5906300214 0.1530095886 0.885532326 9.451794e-02 710 0.0542228029 0.4391671510 0.0454088813 0.130954660 3.661530e-01 711 0.9801739862 0.9737242053 0.3088543373 0.292188345 2.974664e-01 712 0.0955795809 0.7315077246 0.0739612088 0.306935967 8.830078e-01 713 0.4907403311 0.3944720011 0.7725880863 0.986870253 5.451626e-01 714 0.6269930771 0.0671702924 0.6396098298 0.257390226 4.399738e-01 715 0.8188843785 0.8974256085 0.2105948704 0.497911549 8.912714e-01 716 0.1428737247 0.7066940332 0.5905662815 0.013904960 8.051229e-01 717 0.3795925688 0.5371038395 0.6400907186 0.396177840 2.185821e-01 718 0.2576724272 0.2867634567 0.5625600184 0.227866938 6.552381e-01 719 0.0705480152 0.7199489921 0.4384352528 0.213859265 5.562379e-01 720 0.5952966632 0.1331510691 0.3311057268 0.812910554 4.576846e-01 721 0.2078502625 0.2657186741 0.1369717133 0.966520458 6.201260e-01 722 0.5607043237 0.7773463074 0.9060784536 0.136882777 5.718761e-01 723 0.1418856278 0.3193872187 0.0067140581 0.402681939 2.011022e-01 724 0.1668371137 0.6748321333 0.7357757199 0.286822234 6.788829e-01 725 0.6645078519 0.4957872450 0.6253226784 0.045322990 4.713916e-01 726 0.2852513075 0.0419426917 0.4170354577 0.144804272 8.927030e-02 727 0.5211206493 0.6694120474 0.2225864746 0.496572894 8.498413e-01 728 0.1769501129 0.6802692772 0.5119223034 0.810593840 3.094975e-01 729 0.6272501328 0.1008226273 0.0221058365 0.660919987 7.107244e-01 730 0.4807866269 0.4724463990 0.8551723175 0.946134662 5.702034e-01 731 0.0405958679 0.6419971804 0.8001623955 0.059512498 5.834628e-01 732 0.1040727708 0.4574132226 0.8362864284 0.119848038 9.500916e-01 733 0.1034192743 0.5646959795 0.4774509370 0.100162905 4.218027e-01 734 0.9854815747 0.9536970994 0.5731427802 0.445599995 9.751907e-01 735 0.5504544294 0.8856823838 0.5006169302 0.326138788 2.829155e-01 736 0.1668087766 0.0087919568 0.2469461611 0.124296868 9.120047e-01 737 0.6289759041 0.3003166930 0.0746609843 0.277244968 7.745181e-01 738 0.8146107760 0.7891051264 0.7982830841 0.843328346 1.065173e-01 739 0.7342906827 0.5856943072 0.3800514466 0.651758524 6.560058e-01 740 0.1165104364 0.4262742412 0.9380351442 0.720093509 5.285259e-02 741 0.4589043292 0.9510245435 0.1878450911 0.010018110 3.818362e-01 742 0.8540700742 0.4387043286 0.8535325744 0.704237403 1.438948e-01 743 0.9853149375 0.3135367928 0.0273535808 0.401024840 8.124945e-01 744 0.2686594662 0.0067920866 0.4991884257 0.925610454 1.121184e-01 745 0.2199555885 0.4688793917 0.0312401301 0.890368610 3.474478e-01 746 0.6150329802 0.3032249976 0.0330985945 0.160531911 5.085447e-01 747 0.5294507435 0.1677674388 0.2016299928 0.780310904 9.195844e-01 748 0.3173471964 0.3425450716 0.3296101303 0.231268557 1.281241e-01 749 0.7328500336 0.8980467783 0.3283071520 0.323327990 5.388319e-03 750 0.8194361259 0.9905332893 0.5591681972 0.266169388 4.770701e-01 751 0.6656111178 0.2108571122 0.1919852202 0.527454810 3.347841e-01 752 0.0899907222 0.1446414352 0.8986281594 0.806791155 9.988664e-01 753 0.4024253956 0.8001327894 0.4767216074 0.267760365 7.607286e-01 754 0.1910845076 0.3644843656 0.7766964245 0.061330716 9.895875e-01 755 0.5832469633 0.1375580644 0.6565976313 0.690765419 7.303636e-01 756 0.8330274366 0.1368968731 0.7017091217 0.198802563 6.753607e-01 757 0.1212478145 0.8072033660 0.3884129189 0.510344740 9.609224e-01 758 0.6087156688 0.3369593923 0.7562846150 0.510526375 5.928940e-02 759 0.7821644433 0.8510950310 0.9395253323 0.166566865 5.062242e-01 760 0.7225771444 0.0084626777 0.2886167157 0.375914235 1.782048e-01 761 0.4327145740 0.3436067994 0.0624468897 0.655956657 4.799398e-01 762 0.3311250603 0.5018027748 0.3175616367 0.909446071 3.785639e-01 763 0.5488906924 0.9179980736 0.5937463716 0.824942662 4.687987e-01 764 0.1809079153 0.1684568480 0.4394692946 0.091948393 1.571436e-01 765 0.3125010028 0.2808426481 0.4364514167 0.871625515 5.357669e-01 766 0.2322662107 0.4296523267 0.2301211185 0.984538529 5.416512e-01 767 0.8883634240 0.5374647570 0.0706931108 0.798453904 4.194775e-01 768 0.5146165742 0.2363249704 0.6942034576 0.255742336 3.743201e-01 769 0.3510731300 0.2868859540 0.2493360748 0.406186156 2.858131e-01 770 0.5866833304 0.5382801190 0.3648477907 0.102987665 7.186622e-01 771 0.5168380113 0.2537941472 0.5216529446 0.386116420 3.009853e-01 772 0.0169250246 0.6376643232 0.5035872550 0.192309255 4.536078e-01 773 0.9882035200 0.4582637991 0.6574109502 0.724014118 4.872410e-01 774 0.5653496098 0.4760491254 0.2371778297 0.042768712 9.784826e-01 775 0.5821441454 0.3042122070 0.0609040807 0.390714176 2.524162e-01 776 0.0911039554 0.8299246619 0.1350643106 0.867280200 1.776634e-01 777 0.1199192528 0.5218563776 0.5861633071 0.499435835 7.811131e-02 778 0.1269978655 0.3822219400 0.3625663975 0.180269022 7.917490e-02 779 0.5197857721 0.0951860771 0.5939297057 0.487396815 3.944825e-01 780 0.7007307441 0.3687405463 0.2572553901 0.813576587 4.809790e-01 781 0.3218411119 0.7246845297 0.7287631440 0.905135344 9.021088e-02 782 0.1232672483 0.4581358170 0.7231167776 0.155507295 4.767607e-01 783 0.8657550549 0.7917757533 0.2241408930 0.840674793 2.868034e-01 784 0.5233116525 0.3896034497 0.7217376744 0.472973718 1.765475e-01 785 0.0661836988 0.1935713969 0.0214482821 0.087035662 9.176547e-01 786 0.0389345712 0.2243677888 0.5614796390 0.309720548 2.056354e-02 787 0.0492720462 0.1595348492 0.8530885784 0.178120089 8.737463e-01 788 0.3390803235 0.2362157856 0.4276224766 0.455062811 7.755917e-02 789 0.3281026052 0.4601694574 0.0599775666 0.718546729 3.753534e-01 790 0.6873349790 0.4976751781 0.8280542898 0.994703966 2.440879e-01 791 0.5504687894 0.2988064871 0.7713529249 0.641040120 5.693950e-02 792 0.5503732006 0.5960831509 0.8664672766 0.476776771 4.819354e-01 793 0.3711241737 0.6343850996 0.8537406514 0.109487688 6.908167e-01 794 0.5504067924 0.1058719386 0.6400646684 0.081498371 8.541506e-01 795 0.6950991168 0.7604977151 0.5514039169 0.081682724 4.830404e-01 796 0.3357875820 0.1431916794 0.0349251456 0.263697140 5.391837e-01 797 0.2259819009 0.7031412621 0.2148530942 0.511179863 9.868655e-01 798 0.6837921543 0.2173088472 0.6941937325 0.415529779 2.726136e-01 799 0.0327904762 0.2515361288 0.3132338880 0.383568716 6.489722e-01 800 0.4708686613 0.9216939155 0.2693657901 0.127065991 2.182584e-01 801 0.6667835233 0.6971665192 0.1054696967 0.693743361 1.188034e-01 802 0.8530509556 0.9432144242 0.0041820426 0.169260422 3.253670e-01 803 0.3427545812 0.1718738738 0.7625045318 0.362998036 7.145535e-01 804 0.7191447199 0.8004429014 0.9849620434 0.050722711 4.945911e-01 805 0.9965520247 0.4413624234 0.0623347035 0.432994378 2.793668e-01 806 0.9527486961 0.5245378804 0.8925296222 0.037442494 9.853151e-01 807 0.8085598280 0.7501264329 0.1495426074 0.275200297 7.410981e-01 808 0.6196632015 0.7699690936 0.7003352877 0.750953684 6.796842e-01 809 0.1371374123 0.2860390744 0.4732657552 0.337846551 6.279401e-01 810 0.2705621868 0.1728939763 0.3674579775 0.646730392 2.963514e-01 811 0.9150904114 0.7387685184 0.4425446887 0.717890505 7.044907e-01 812 0.9508713693 0.2058457162 0.3327484170 0.048251316 8.233207e-02 813 0.7664615572 0.2933199112 0.2387260119 0.455275297 9.507067e-01 814 0.7263937038 0.5308477988 0.7668302804 0.151941190 2.872785e-01 815 0.3706638364 0.0815223935 0.6523769449 0.861204863 9.701118e-01 816 0.3731770006 0.1418519248 0.6558721811 0.392759632 8.911583e-01 817 0.8660431546 0.0405283556 0.2191681382 0.671238937 4.604006e-01 818 0.5390274378 0.1355421511 0.7730570342 0.392236311 4.018251e-01 819 0.8056316061 0.4232798924 0.8585525232 0.273011497 2.771170e-01 820 0.4778904547 0.8586307105 0.3877773187 0.301605025 2.120700e-01 821 0.3268022928 0.9401343844 0.7134420641 0.820058862 6.129209e-01 822 0.4336912178 0.6601184118 0.7966466683 0.753500970 8.045000e-01 823 0.4331766844 0.4773011776 0.2554806194 0.506299197 7.484734e-01 824 0.8422790130 0.9728820582 0.5530510051 0.674755441 3.098467e-01 825 0.1702686194 0.5197307284 0.1117506877 0.576723189 6.939165e-01 826 0.8912892947 0.8173142259 0.2254784491 0.990182979 8.775264e-01 827 0.5795565543 0.3450774932 0.6231800388 0.511314409 7.229131e-01 828 0.2747776266 0.3432068359 0.9651196841 0.361325060 9.342489e-01 829 0.2476471073 0.9399656593 0.7509542222 0.040544073 5.951181e-01 830 0.0888829241 0.2625810562 0.9616506596 0.164866655 5.282961e-01 831 0.0618546458 0.2170708573 0.6758510496 0.806005144 5.418574e-01 832 0.2215657665 0.5892768123 0.9687829807 0.970458879 1.469074e-01 833 0.0160984755 0.6008945550 0.2456119955 0.651282727 7.154888e-01 834 0.9552937921 0.4041952416 0.2487599570 0.636312764 4.222105e-01 835 0.6553695966 0.5982882574 0.4690349526 0.850788158 9.853207e-01 836 0.5880596407 0.1025756893 0.3663468168 0.602122332 4.146614e-01 837 0.2446371287 0.3886715963 0.7592353383 0.304655447 4.547841e-01 838 0.2204478385 0.9776127548 0.8441695466 0.342979762 1.793916e-01 839 0.3436731452 0.3158153961 0.8450364436 0.017197104 5.342961e-02 840 0.9654810687 0.6897880316 0.2408264768 0.725365726 6.414571e-01 841 0.7806982426 0.6866874089 0.1406750046 0.415963592 4.494836e-01 842 0.6681353534 0.9749579220 0.5696881823 0.270559045 9.334318e-01 843 0.6673664290 0.8776515799 0.1567756762 0.577515450 6.526554e-02 844 0.8577287272 0.8350734247 0.8539518388 0.599658105 3.774422e-01 845 0.8676475156 0.2449205024 0.7057895265 0.336051099 2.469455e-01 846 0.9010199986 0.7300883937 0.9259289976 0.107742261 7.782273e-01 847 0.7096090924 0.0396126274 0.1711437271 0.912411376 5.841550e-01 848 0.4057656860 0.9635527530 0.4667839229 0.717135843 5.471613e-01 849 0.7586153776 0.8188142958 0.7115409470 0.265018949 4.173710e-01 850 0.1514678800 0.4931847306 0.1066040788 0.420480186 2.265498e-01 851 0.8852276236 0.2273079150 0.6126720684 0.124443571 5.439975e-01 852 0.5744567150 0.2276047985 0.6053443069 0.305235157 1.352699e-01 853 0.7365684998 0.7416466051 0.1987565625 0.522348852 1.889310e-01 854 0.4682052871 0.3656761451 0.5374135601 0.834061932 4.192377e-01 855 0.6883175839 0.1932561044 0.1337969434 0.655146906 6.233718e-01 856 0.7630665472 0.1755269326 0.8618537828 0.065738074 4.840831e-01 857 0.9105806397 0.0796877367 0.0654560952 0.970875686 6.588444e-01 858 0.1453003632 0.8365933388 0.8908922726 0.261465481 1.285208e-01 859 0.3542107171 0.6549203501 0.8706293327 0.595211849 6.201236e-01 860 0.8858120129 0.6786385714 0.0019265474 0.633881345 6.488960e-01 861 0.3534604481 0.4462263004 0.3821332473 0.116964221 4.485467e-01 862 0.9199138735 0.0237773440 0.5560805255 0.789658329 7.332141e-01 863 0.9699242383 0.5838220422 0.0731530387 0.854077701 1.952940e-01 864 0.7197193392 0.7666043034 0.6545700596 0.109955078 9.903314e-03 865 0.0818659512 0.6813366392 0.4428950842 0.709426838 7.134775e-01 866 0.6979543704 0.4409124681 0.2596999193 0.435666425 7.401312e-01 867 0.7083249867 0.4796856840 0.3304600932 0.663771980 9.346799e-01 868 0.4236047838 0.7879075778 0.0797441986 0.362275834 1.116769e-01 869 0.9025010683 0.0370089954 0.0607495804 0.455845351 7.383802e-01 870 0.8385839090 0.8592968574 0.3819316330 0.732232572 6.211616e-02 871 0.1577102917 0.4951161523 0.2630529224 0.717813409 3.060167e-01 872 0.9411812874 0.2867955500 0.2457986970 0.908677576 3.361926e-01 873 0.3959288662 0.2671015316 0.0694725793 0.657614531 2.711497e-01 874 0.1749171666 0.1481003091 0.5796920711 0.745290652 5.726620e-01 875 0.9484079392 0.7067262111 0.3593763858 0.696661414 6.557866e-01 876 0.6690065281 0.8686357243 0.9274752026 0.429747088 9.805910e-01 877 0.9932561866 0.6987060872 0.7170277392 0.572124985 7.858925e-01 878 0.4792634952 0.2627820971 0.7591925235 0.129092884 3.584519e-01 879 0.3339846164 0.4879570440 0.5219738027 0.981418908 4.780838e-01 880 0.1384664588 0.6071906351 0.5598796897 0.989587379 6.047264e-01 881 0.7710127225 0.6588871563 0.7000522728 0.787040020 8.951891e-01 882 0.5183066048 0.6633623212 0.8556959080 0.823626497 9.873870e-01 883 0.2342772360 0.6825878853 0.9765392048 0.422077550 8.276469e-01 884 0.3819961718 0.0703841767 0.4545353968 0.190806217 4.993768e-01 885 0.9671158306 0.7996307290 0.2528239412 0.821535721 4.520912e-02 886 0.0136003532 0.6550489797 0.0971307924 0.569252931 4.415581e-01 887 0.6918485614 0.2938269705 0.3830597901 0.642045519 9.181501e-01 888 0.3883587052 0.5746545340 0.2149518812 0.544876507 9.811128e-01 889 0.9782044799 0.0871506219 0.1721066297 0.638855571 2.163524e-01 890 0.3884682686 0.8530711008 0.1081484584 0.618824126 1.981257e-02 891 0.1955370163 0.7554930146 0.4383497564 0.132186846 6.693339e-01 892 0.2558058237 0.8910853162 0.6054714422 0.106588792 8.727763e-01 893 0.8466363044 0.8384201061 0.7106585323 0.261846789 3.145544e-01 894 0.9117230456 0.7739696172 0.4066036423 0.836243367 9.512813e-01 895 0.6891804636 0.3827441628 0.1441087264 0.428895871 5.805177e-01 896 0.8616765880 0.7499236313 0.2040350866 0.042978100 2.002000e-01 897 0.9397967570 0.0891257052 0.7237532646 0.214750154 9.383528e-02 898 0.2836855208 0.5146681564 0.9748851419 0.309161969 5.718429e-01 899 0.7020970217 0.3102047946 0.6902426740 0.594432420 1.961699e-01 900 0.7543787274 0.9368917353 0.2379066127 0.744932082 6.898677e-01 901 0.7100764371 0.6819450008 0.5354397860 0.666972563 2.406382e-01 902 0.0359942482 0.0883137148 0.1095670776 0.368549353 8.317809e-01 903 0.7027321772 0.3958795723 0.8308744195 0.667122809 2.450692e-01 904 0.1076769722 0.0905030516 0.4746082048 0.579430669 3.238268e-01 905 0.5423400379 0.2575905705 0.7975925563 0.839304272 3.700970e-01 906 0.0559626976 0.7763351945 0.5180712640 0.131845527 3.509410e-01 907 0.7400617898 0.5512401585 0.2532550723 0.822768779 8.241569e-01 908 0.4324181397 0.7704832107 0.2057194079 0.622663645 8.665828e-01 909 0.6370878578 0.7114341015 0.0513802432 0.275731774 3.926777e-01 910 0.5504355514 0.8300711268 0.7932873622 0.971701424 4.464620e-01 911 0.7207374696 0.9681204916 0.1152818713 0.205870625 2.824269e-01 912 0.5694926230 0.1999330653 0.9196312737 0.321336865 1.276558e-01 913 0.4082449640 0.1262337428 0.7942077753 0.763436640 5.887029e-01 914 0.4719120276 0.9897765913 0.5567615679 0.235130594 4.128340e-01 915 0.7132404083 0.5091732969 0.2665097436 0.872430083 9.947829e-02 916 0.1123925871 0.9880523295 0.7234942676 0.710679841 9.196550e-01 917 0.2404889199 0.3670683077 0.8855743406 0.572891420 9.907376e-01 918 0.1648135930 0.7147001112 0.2808103329 0.826797771 8.433905e-01 919 0.8316822357 0.3342582213 0.9897506847 0.129931385 7.228052e-01 920 0.1540134929 0.4937861396 0.6946977631 0.642777493 9.363438e-01 921 0.4543502852 0.8231381234 0.5084731737 0.392673834 9.929688e-01 922 0.3393653259 0.3207413722 0.2487738428 0.036524708 1.525176e-01 923 0.3792336930 0.4274902237 0.3213149323 0.111199606 9.098137e-01 924 0.5646767376 0.5653868199 0.1198367970 0.422641320 3.241015e-01 925 0.1275393234 0.9610971808 0.8526588532 0.547694317 1.969815e-01 926 0.8284457396 0.0512343606 0.0969893662 0.565572405 6.216574e-01 927 0.7716819895 0.4303607254 0.1500063352 0.585179174 4.304587e-01 928 0.3144028510 0.9650541637 0.2011498599 0.832636225 7.382756e-01 929 0.1932181078 0.6512502013 0.2314884884 0.927042429 4.021827e-02 930 0.7715724281 0.4867733018 0.1722336062 0.592240821 4.878809e-01 931 0.7515096315 0.5237351716 0.9337820550 0.175378338 5.437635e-01 932 0.8508526227 0.0862903530 0.7518694196 0.392386721 5.070725e-01 933 0.1897171729 0.1816355581 0.4939800212 0.508380143 2.517736e-01 934 0.5888304978 0.4325272697 0.0950893415 0.437182602 7.073556e-01 935 0.2581488835 0.4775416015 0.1880199569 0.091574369 9.642912e-01 936 0.6145524746 0.8077542640 0.8791012922 0.404682925 2.885075e-01 937 0.5502511666 0.2100446864 0.5053161618 0.570401419 7.775891e-01 938 0.1422360775 0.3714883709 0.1705700466 0.292427428 5.072701e-01 939 0.8544583651 0.3779014812 0.7818198670 0.209379298 4.591110e-01 940 0.2279679456 0.1332134781 0.7430715247 0.263717869 9.951537e-01 941 0.4878764427 0.0266515280 0.0557566388 0.827095761 6.739393e-01 942 0.8901376666 0.2336050419 0.1721143238 0.955066324 7.380528e-01 943 0.1214600294 0.4506204431 0.7488469749 0.722901567 9.061563e-01 944 0.7492403670 0.2964121955 0.4126732238 0.134044858 4.084631e-01 945 0.6883822854 0.3348744807 0.7348977642 0.435535827 3.775312e-01 946 0.9731870238 0.6234922898 0.6055679652 0.011814762 8.236566e-01 947 0.9803779942 0.0189021132 0.1514285270 0.779741396 7.189808e-02 948 0.5742041636 0.4015159667 0.2583783229 0.319161324 3.810088e-02 949 0.9520442467 0.3840122933 0.3215753071 0.721180730 8.162514e-01 950 0.4607166387 0.5502329234 0.0424744408 0.470151842 4.324335e-01 951 0.7038704425 0.7253101626 0.1991410654 0.663179845 7.651094e-01 952 0.2755301245 0.1031086305 0.9857126495 0.406166487 7.041206e-01 953 0.6193735383 0.6403544305 0.4304613241 0.992695211 5.010757e-01 954 0.4249681248 0.5705294732 0.2173429558 0.736243507 2.958308e-01 955 0.3728593271 0.9821952912 0.7585317735 0.592364626 4.824808e-01 956 0.7996276352 0.7557521961 0.6896903608 0.032121529 9.333821e-01 957 0.2530086664 0.7840213208 0.9472654869 0.663430023 7.808786e-03 958 0.0272058961 0.2785600286 0.3803765716 0.835774167 8.285646e-01 959 0.2937615011 0.0414462413 0.0293130069 0.170471911 5.053126e-01 960 0.7656114011 0.0089509301 0.2224720593 0.078633697 2.267945e-01 961 0.3921685980 0.2418958903 0.9058356252 0.499363041 4.301341e-01 962 0.3203731477 0.4558730088 0.9356109097 0.637428897 4.638048e-01 963 0.5701269405 0.6541033043 0.4553435929 0.659193589 7.637788e-01 964 0.0837780091 0.9852263576 0.2699921329 0.889402745 7.838773e-01 965 0.7902619520 0.1120501796 0.0643302598 0.015220200 7.573140e-01 966 0.7188133628 0.7126189214 0.5146951526 0.265695591 2.805009e-01 967 0.8039738089 0.0866877660 0.3811699026 0.007797114 9.017973e-01 968 0.6556652877 0.4472971286 0.0009898420 0.250218173 8.946227e-01 969 0.6478033902 0.0806381418 0.8741640016 0.686927262 3.710893e-01 970 0.6908786618 0.6214244557 0.5556211434 0.115327901 3.217848e-01 971 0.6140362916 0.7585889928 0.4772790568 0.943758306 5.007378e-01 972 0.9190024359 0.9279428197 0.9869498806 0.354942342 7.683242e-01 973 0.1376092818 0.5861151936 0.9461021069 0.957316337 1.416249e-01 974 0.4968745462 0.1127922863 0.0896480761 0.140535582 3.406880e-01 975 0.5084140948 0.6140513800 0.0141397875 0.618412951 2.553456e-01 976 0.2373713481 0.5146937706 0.3362690192 0.534919100 2.279837e-01 977 0.5096209319 0.5837802538 0.4960927456 0.120788795 3.330714e-02 978 0.1317708287 0.6946347281 0.8838458196 0.551703196 8.626118e-01 979 0.4485086391 0.4274698249 0.3073356152 0.731657210 1.683407e-01 980 0.6195011809 0.9784419455 0.3550067239 0.086869737 8.103572e-01 981 0.4437196946 0.3406247022 0.4528926273 0.325831012 9.969143e-01 982 0.0834825563 0.0264296336 0.8081067060 0.268322631 2.651312e-01 983 0.7067420189 0.3201212729 0.8177060552 0.896760443 6.064301e-01 984 0.3496549178 0.1091470283 0.1311323550 0.889285605 8.667603e-01 985 0.9456903765 0.3948074104 0.0551960305 0.270541456 1.841105e-01 986 0.9264556284 0.0210456059 0.7544529487 0.329619684 1.666222e-01 987 0.5862394304 0.6943669352 0.3462126998 0.527938245 4.539635e-01 988 0.9264138332 0.9935972001 0.4456170811 0.276350456 1.458880e-01 989 0.7399739220 0.6072702506 0.5072083352 0.379948666 2.127185e-01 990 0.0597490836 0.7014014195 0.8693522580 0.502023081 1.920523e-01 991 0.2525925478 0.4642360706 0.5797812503 0.206372628 2.255902e-01 992 0.5294484263 0.5832591089 0.9141128107 0.763919299 4.386716e-01 993 0.1451663070 0.9450887532 0.3267974902 0.163087269 4.304320e-01 994 0.5973799566 0.6611484769 0.2045684843 0.530934318 9.091520e-01 995 0.4801052473 0.1244018658 0.6161254332 0.985765266 4.358157e-01 996 0.8676580756 0.6908842688 0.2463870917 0.558830278 8.897796e-01 997 0.8354824937 0.7563201888 0.4218094908 0.991251539 1.910084e-01 998 0.5086493925 0.5916594807 0.7986441688 0.352129334 9.052197e-01 999 0.7984968482 0.6484692944 0.1080854419 0.114980506 4.598162e-01 1000 0.3461663174 0.5929790989 0.4973262595 0.372802859 6.241228e-01 i j k 1 0.9087631686 0.5128349718 0.433474417 2 0.5681274838 0.7094579476 0.868427460 3 0.3827259571 0.5897734014 0.180360822 4 0.6609854468 0.4958711024 0.009055904 5 0.8411586101 0.3029566994 0.585858709 6 0.6222287491 0.1019961545 0.630805569 7 0.5166449379 0.2911676636 0.261239435 11 0.5606055886 0.5415306007 0.117883356 12 0.1668717416 0.3277200800 0.320070869 13 0.9821718861 0.3913873483 0.735874106 14 0.1250124935 0.1662794249 0.617329021 15 0.4052526173 0.9588967944 0.125180201 16 0.3963778000 0.4164709412 0.982822154 17 0.7747443833 0.2370111805 0.867924212 18 0.4656205408 0.9236596262 0.231345516 19 0.4267182045 0.0398125688 0.276681161 20 0.4886503392 0.1524191860 0.040110499 21 0.7435912108 0.9295226706 0.971871639 22 0.3373999882 0.3702988313 0.919676143 23 0.5863423934 0.8232238216 0.723063651 24 0.4206146235 0.6212165076 0.282144273 25 0.1873300616 0.4141433262 0.553026780 26 0.8554507741 0.1028212600 0.855329942 27 0.6119108447 0.8649808620 0.325192982 28 0.2997493446 0.3446830176 0.702674859 29 0.6681878306 0.4829978379 0.869019716 30 0.5278012829 0.6876936750 0.849721408 31 0.7944821278 0.2671868550 0.872588813 32 0.3656178131 0.3114310633 0.124459583 33 0.9967463582 0.1866228166 0.459630596 34 0.3590255990 0.9903327457 0.988608231 35 0.2418361048 0.8444164961 0.473984825 36 0.5806110287 0.4397265683 0.489710954 37 0.6584511327 0.6120709099 0.122655185 38 0.9218367229 0.5005969931 0.271223680 39 0.8944407739 0.8447717724 0.088994969 40 0.7279412807 0.1609291413 0.455329117 41 0.2609252520 0.8845530343 0.210940112 42 0.1416404401 0.3299012582 0.467406417 43 0.2746811195 0.7520817884 0.792780091 44 0.0553447758 0.3767659578 0.026498032 45 0.5352210163 0.0118170890 0.846525728 46 0.0120629487 0.0339999525 0.166745803 47 0.4314778787 0.3114104606 0.714891932 48 0.2245725456 0.1469827355 0.018858860 49 0.7272778642 0.5995649623 0.449367810 50 0.6059436901 0.7773033865 0.684086278 51 0.4036247542 0.7489980566 0.366653045 52 0.5969315961 0.5205937014 0.886234621 53 0.0006523626 0.4913336923 0.175098365 54 0.2632054156 0.9477295359 0.419257508 55 0.3361853021 0.3503014820 0.565365737 56 0.3226710237 0.5014365071 0.358642312 57 0.4974331714 0.7017149401 0.191931669 58 0.1178657564 0.4157309311 0.556759141 59 0.3016790133 0.2646031538 0.755432571 60 0.0308654625 0.2068238084 0.846652366 61 0.4426214227 0.3930100391 0.313803358 62 0.7519367803 0.1991266685 0.373874883 63 0.0827221849 0.4457892301 0.021788683 64 0.8904469314 0.6041067331 0.965939397 65 0.2234720613 0.1910297391 0.286601910 66 0.8156521297 0.6457851853 0.387838364 67 0.4547833218 0.5390125816 0.146064876 68 0.2293814677 0.0510656524 0.234307113 69 0.4554125951 0.0007678478 0.579237812 70 0.9778966792 0.6642413577 0.981427724 71 0.4937450592 0.5499402732 0.159018812 72 0.1111480240 0.0701609459 0.231798086 73 0.4515543673 0.0222580845 0.169742819 74 0.3860736378 0.5143827731 0.339853824 75 0.6689951743 0.8895948618 0.331853422 76 0.5848921356 0.2110785230 0.208752149 77 0.8040093554 0.5435449358 0.155490631 78 0.8703865418 0.0060005118 0.535063164 79 0.6247375568 0.6896314544 0.220268407 80 0.5502351066 0.9226864551 0.188979229 81 0.8907975026 0.1401077805 0.012584140 82 0.0914727584 0.8452462007 0.567303002 83 0.1601004598 0.7238995091 0.347789467 84 0.3930475307 0.8140243778 0.828480477 85 0.2362872674 0.5648667500 0.536917268 86 0.0663146342 0.7406738328 0.967070757 87 0.6229225849 0.2372979226 0.681185639 88 0.5117010218 0.3952996521 0.237138671 89 0.8064635289 0.3335397751 0.565703884 90 0.3967762347 0.2208842041 0.354194236 91 0.6337079392 0.4096777418 0.079051379 92 0.1203049868 0.9191357759 0.658497361 93 0.9676421033 0.2858615983 0.453540877 94 0.2220023738 0.6622591268 0.935944299 95 0.0554578195 0.1503409792 0.458454961 96 0.0830594508 0.2259703281 0.460871697 97 0.8682666586 0.6073553285 0.618568527 98 0.6663306805 0.8752778720 0.774079820 99 0.4043017544 0.8350057611 0.125036883 100 0.1059333733 0.4167972640 0.458615416 101 0.6536240370 0.2890427141 0.524334089 102 0.6261212248 0.5107074548 0.048700516 103 0.2947792842 0.0883161519 0.316675600 104 0.6815966177 0.0293583267 0.242670482 105 0.9335999803 0.1404198715 0.179384844 106 0.6278110058 0.7190329526 0.200458986 107 0.5040061027 0.5368715730 0.488505151 108 0.5013903244 0.8250459556 0.930052144 109 0.1733192850 0.1501131626 0.766793029 110 0.3508784429 0.2671870328 0.727526051 111 0.1484719319 0.1510998618 0.631773586 112 0.0966769739 0.4766372084 0.924724548 113 0.7525902183 0.5807615130 0.878133470 114 0.8149411348 0.5931538984 0.529054842 115 0.9948693211 0.5770180277 0.497331441 116 0.8509829661 0.8519616781 0.331109564 117 0.1651958560 0.0783651082 0.222836789 118 0.8542163239 0.9436419141 0.642416314 119 0.2957376034 0.5372309394 0.328969282 120 0.1680100688 0.3125867224 0.536918208 121 0.7444190066 0.4906409730 0.539057107 122 0.9736629406 0.3209659122 0.420134468 123 0.4210970441 0.0146621745 0.281792870 124 0.4054784656 0.5823995634 0.021190652 125 0.1213328205 0.1566025014 0.499803931 126 0.3509171081 0.2642908753 0.828844704 127 0.8019870594 0.3684905737 0.597643980 128 0.3392660415 0.5528952766 0.362731453 129 0.2755800916 0.4141004644 0.263369496 130 0.0099175237 0.5354448098 0.867901461 131 0.5877318792 0.7913212397 0.356186446 132 0.7294213187 0.9341020375 0.426399379 133 0.3597076745 0.8540280554 0.690510602 134 0.1923128793 0.8539461282 0.470327384 135 0.4146025018 0.3285077966 0.174154247 136 0.2408502251 0.7539579268 0.974151978 137 0.6713664781 0.7490618394 0.018064682 138 0.5027397233 0.0004517024 0.371284921 139 0.1293624879 0.4354867004 0.719204444 140 0.1697100580 0.2979691955 0.825164750 141 0.1851382013 0.0322515192 0.209856055 142 0.9614997103 0.3880613439 0.210859042 143 0.2341886915 0.9113518861 0.569904638 144 0.3903274005 0.0529591336 0.977473462 145 0.5128328698 0.8339039281 0.460075917 146 0.6121268612 0.1546220388 0.120337063 147 0.5986534422 0.2321277794 0.255277535 148 0.2247179085 0.7172596608 0.901589938 149 0.7513462589 0.4561471040 0.682734193 150 0.4696188967 0.7050004448 0.276803780 151 0.9800768141 0.1836125506 0.087107321 152 0.9729995320 0.1371118471 0.786187609 153 0.0847310543 0.6312713376 0.981879490 154 0.5851558545 0.6476236130 0.915172418 155 0.0922475369 0.4266638111 0.594584180 156 0.5917444066 0.7326848062 0.536128583 157 0.8340488868 0.7819989014 0.743682370 158 0.6174082272 0.5634941428 0.103459783 159 0.1671749738 0.6828045091 0.075492039 160 0.6154925302 0.9422851873 0.305467341 161 0.6246360233 0.6056861058 0.008500006 162 0.6745371576 0.6684385163 0.103568461 163 0.1869240811 0.0430012536 0.847895277 164 0.6141133632 0.3480020654 0.071074953 165 0.7515403559 0.4299957363 0.545414267 166 0.4849006436 0.8814194989 0.211329428 167 0.2106576399 0.7893810470 0.213353900 168 0.5196030815 0.7781596312 0.544735801 169 0.1838931392 0.5161872935 0.615889838 170 0.8333251288 0.7455936016 0.366780687 171 0.0395859268 0.8005671275 0.916855233 172 0.4805583220 0.9664080332 0.414999548 173 0.5490431390 0.1338877140 0.664842228 174 0.0264012944 0.1551172663 0.822628885 175 0.0048946154 0.9222773779 0.955932898 176 0.9043120472 0.1706953272 0.072777700 177 0.8809468760 0.6098237170 0.719657137 178 0.8611115997 0.3326158209 0.364571405 179 0.4787534853 0.9453399039 0.835941072 180 0.2313803730 0.2082363386 0.499824949 181 0.6148180722 0.6784512543 0.777743101 182 0.0572387041 0.7870746215 0.120125221 183 0.8088287492 0.8559843907 0.259347438 184 0.9785300081 0.8136936259 0.709260552 185 0.3395234370 0.9604597690 0.022556264 186 0.2212379947 0.5665022719 0.951045089 187 0.3356777702 0.3005251158 0.084501502 188 0.3548264690 0.9781491724 0.799717161 189 0.7405867146 0.8685159674 0.821510137 190 0.0839215950 0.2648300638 0.943535396 191 0.8840797534 0.4699844518 0.236652090 192 0.8545890038 0.1840025745 0.977806848 193 0.5177294672 0.6102390429 0.245517909 194 0.8757470779 0.6909048136 0.651942725 195 0.6882468495 0.0161601556 0.784394057 196 0.1693982442 0.7199928972 0.756506318 197 0.3532229802 0.4630767726 0.897241289 198 0.3698309476 0.2420675876 0.738088635 199 0.1395692111 0.9148447975 0.002973847 200 0.5528753118 0.9722143693 0.530145853 201 0.0911104653 0.3148403978 0.858897036 202 0.1397922952 0.1736123802 0.794187781 203 0.9248953597 0.9052255033 0.426479524 204 0.3468633790 0.9629188555 0.392073041 205 0.2623246550 0.5600378469 0.582281488 206 0.5293257805 0.3108575726 0.160060341 207 0.5093579234 0.5014171167 0.736309079 208 0.4495433341 0.4862429700 0.392430910 209 0.9123518646 0.3051007891 0.203720998 210 0.1214102015 0.7437134380 0.704168370 211 0.7131768700 0.4649773457 0.543138901 212 0.9961626902 0.9921279163 0.345299343 213 0.1603799369 0.6949981558 0.597703475 214 0.6117657335 0.1799172421 0.127134755 215 0.2350276292 0.0418957674 0.210518366 216 0.3027018649 0.2641500751 0.995105395 217 0.7299060032 0.6030209041 0.274804636 218 0.6489603042 0.5311673591 0.184956109 219 0.2516879274 0.9647897545 0.341185981 220 0.2851197992 0.1207269041 0.697508165 221 0.5125428722 0.9168800218 0.117690779 222 0.0346664493 0.1519337283 0.779520737 223 0.7842136300 0.0845155257 0.978813627 224 0.3757599506 0.8362768008 0.939809005 225 0.5080087527 0.5440474791 0.235630636 226 0.7891359089 0.2309179760 0.190111594 227 0.6982576286 0.2912786996 0.414994985 228 0.4591538832 0.1998717184 0.226559677 229 0.0087905687 0.1122070975 0.679795615 230 0.2509475215 0.6780429292 0.506995059 231 0.0223060360 0.8224451512 0.976316434 232 0.1185056434 0.3756171942 0.999038306 233 0.0395731088 0.6464639278 0.744022369 234 0.5190228012 0.1079823810 0.281804436 235 0.8141576792 0.0351354424 0.302080039 236 0.2354405455 0.3621697398 0.462507161 237 0.1280833837 0.0429123768 0.452132628 238 0.9522720049 0.5174656950 0.667757805 239 0.8626499793 0.6394905939 0.116929316 240 0.2211448003 0.4862877068 0.187651631 241 0.2826683724 0.7685764139 0.427660304 242 0.2217451246 0.3901322596 0.527077607 243 0.9123899231 0.3484480807 0.940976249 244 0.7760570277 0.0196964843 0.994308030 245 0.7245896910 0.8625496353 0.669495775 246 0.3545963238 0.4262103755 0.925164024 247 0.8029720054 0.3310154788 0.190491454 248 0.5781933405 0.9248862842 0.819874573 249 0.7588987695 0.3347283669 0.723955127 250 0.4717798149 0.6719663588 0.033997452 251 0.5023258904 0.3928283551 0.863187823 252 0.8355372436 0.0686451662 0.909715006 253 0.8593941671 0.0926849283 0.817367764 254 0.5077188036 0.3954995340 0.768992276 255 0.2380844436 0.0159355055 0.582561954 256 0.5875904860 0.2722851331 0.175310384 257 0.4994263807 0.5650870153 0.936873940 258 0.0801673585 0.1591223918 0.240441793 259 0.5156380439 0.7538698432 0.836027631 260 0.2213389149 0.7571562580 0.967936541 261 0.5441098993 0.4593958042 0.259123230 262 0.8928442860 0.9421351061 0.866291614 263 0.6189724577 0.5877450497 0.485781986 264 0.7953684707 0.0485105908 0.582787094 265 0.7586312888 0.0050816601 0.654969776 266 0.9054281176 0.5962432623 0.478027690 267 0.9187670490 0.3565554691 0.993413433 268 0.2827955044 0.6572014024 0.191259727 269 0.7256454087 0.0734363843 0.191539427 270 0.7012636382 0.3691736532 0.951903309 271 0.7524833072 0.7902583296 0.406234259 272 0.1661315192 0.1737899601 0.991651452 273 0.8801912477 0.6872708595 0.278342104 274 0.1365374811 0.7761638940 0.887483506 275 0.7663580186 0.1873531982 0.729194304 276 0.6235552917 0.8355714106 0.314209814 277 0.5843322033 0.5417277587 0.167233856 278 0.8195801824 0.0575507390 0.816330421 279 0.4727422609 0.7954525829 0.935344556 280 0.4483542065 0.0807209269 0.061240495 281 0.3400470598 0.2865212108 0.406699197 282 0.9732844774 0.0074201201 0.504856665 283 0.3633071035 0.0794652763 0.697210894 284 0.8816565587 0.5742267463 0.926123858 285 0.4987751339 0.8233126488 0.320464924 286 0.3310508910 0.7643538795 0.709534760 287 0.2243376386 0.9196181563 0.627416339 288 0.7893124865 0.4731599311 0.791820529 289 0.3402598179 0.5928448008 0.836453283 290 0.2806643799 0.9530142890 0.190024565 291 0.8866525306 0.6742309285 0.305205277 292 0.6935368618 0.0542066889 0.255308195 293 0.9925473691 0.3263957959 0.660519107 294 0.7840081144 0.7939104903 0.554709732 295 0.8879083896 0.9838657875 0.047163502 296 0.9471069211 0.8180908239 0.667315026 297 0.7652483105 0.5011755559 0.672978379 298 0.1034767593 0.4843148482 0.765021885 299 0.5173658554 0.0807071435 0.149461499 300 0.2892686401 0.4461832349 0.774942254 301 0.4666807710 0.0640776940 0.937765861 302 0.1666596923 0.7189921502 0.176432163 303 0.6778312610 0.1221541967 0.505464738 304 0.7125912458 0.8826425248 0.330806704 305 0.2068997175 0.8807366712 0.525438205 306 0.8083262281 0.8745090137 0.359391946 307 0.0551950403 0.6054255778 0.010514143 308 0.1101146743 0.2348861066 0.428129612 309 0.3846570568 0.2119303760 0.407141645 310 0.9082448669 0.6626014228 0.642785348 311 0.5171715580 0.4968071438 0.974462328 312 0.4125887151 0.7784681309 0.425039892 313 0.5129293425 0.2333928708 0.442837440 314 0.3842024251 0.2738116076 0.910525002 315 0.8241676399 0.2300581113 0.871515020 316 0.7714029867 0.2671443268 0.820216401 317 0.1911138035 0.9620553681 0.130298762 318 0.9307477702 0.0430600131 0.650889409 319 0.8699085701 0.5043442317 0.645747940 320 0.1354798172 0.6577726228 0.614540099 321 0.9219136874 0.9839676502 0.527190544 322 0.2234225660 0.1896401674 0.277660568 323 0.7306426747 0.3842383027 0.147647479 324 0.1554096898 0.5335272748 0.714302043 325 0.7123912408 0.0143899969 0.362525354 326 0.4453180153 0.3131151772 0.545730561 327 0.6089746670 0.0491327595 0.556184235 328 0.7024399380 0.4329056886 0.196519810 329 0.7986177169 0.9928155858 0.675117668 330 0.9083080657 0.4011237149 0.939470744 331 0.2855855885 0.9352167114 0.064763133 332 0.0581693540 0.3703997759 0.489970324 333 0.2188624633 0.1252656823 0.082836872 334 0.3849162261 0.0405746803 0.599802457 335 0.5753972360 0.5074641234 0.025409097 336 0.8585140959 0.9870252733 0.960869779 337 0.3733973808 0.6797194134 0.456802760 338 0.4922717544 0.2692998312 0.738611541 339 0.7769749579 0.4674859173 0.649227513 340 0.9744688380 0.7548743489 0.444152255 341 0.5815969589 0.1518399711 0.602368516 342 0.7353012746 0.9282031991 0.711713375 343 0.9093837244 0.2169220045 0.387050143 344 0.2522708455 0.4072291572 0.123953464 345 0.1319291440 0.6047342650 0.586918667 346 0.4362283603 0.6418692195 0.057993301 347 0.7414943012 0.5013274332 0.708908894 348 0.4571445726 0.3489230359 0.690105600 349 0.1595911114 0.6860206951 0.762169352 350 0.4302436933 0.3642897324 0.242127506 351 0.5977981715 0.0813042349 0.103939542 352 0.5270542719 0.6627145151 0.735818434 353 0.6737627659 0.0368648355 0.026491203 354 0.9854462768 0.7834856187 0.373826430 355 0.4471681069 0.1585723683 0.202335776 356 0.4414274348 0.3658822568 0.603922377 357 0.3329481015 0.8038419769 0.171805945 358 0.3547660108 0.5783449381 0.172469585 359 0.5894069702 0.7482325078 0.388472585 360 0.0325384494 0.1603930925 0.764646099 361 0.4140505295 0.9774947141 0.973752263 362 0.3705947967 0.7234468276 0.418361759 363 0.1936284041 0.7779606243 0.166575791 364 0.2225097425 0.1565632462 0.553050242 365 0.2961740468 0.9200274399 0.913567509 366 0.0282904631 0.3159458183 0.394937945 367 0.9362333464 0.9701456351 0.449891258 368 0.5500495192 0.9458807199 0.152785856 369 0.9320686467 0.7501517776 0.102089221 370 0.3137839800 0.7753677391 0.062030176 371 0.5316903319 0.2782962408 0.946551740 372 0.9503071038 0.1564524630 0.373653485 373 0.8605535319 0.7530576503 0.381815319 374 0.4187593728 0.0342201130 0.807256441 375 0.7662927075 0.5564019093 0.099438679 376 0.4496172934 0.5887439523 0.059386802 377 0.4658499782 0.8791806730 0.505105258 378 0.1408295704 0.3202534819 0.866802158 379 0.6859266665 0.0451234847 0.183402799 380 0.4025848354 0.1113692916 0.956379740 381 0.5544540114 0.0858751207 0.142778292 382 0.5648256443 0.5336159950 0.924890156 383 0.6727542018 0.8860401020 0.915325739 384 0.6970705483 0.2500414085 0.768281287 385 0.3993773803 0.4700438085 0.316371213 386 0.7216110707 0.4414328204 0.099874506 387 0.7341371970 0.5050938698 0.653269146 388 0.1684000157 0.3985682614 0.863572443 389 0.6248012667 0.2710192350 0.276954715 390 0.8690601182 0.9936061928 0.115726013 391 0.8773299251 0.6416055518 0.339723316 392 0.8473763240 0.3966204205 0.818845225 393 0.6296141907 0.3974838224 0.466163284 394 0.0357694868 0.0644413177 0.624232494 395 0.4002664983 0.4900650519 0.125103871 396 0.5582493504 0.9668820433 0.078048980 397 0.5576146126 0.0716046500 0.190499994 398 0.3291113013 0.7323917581 0.997338800 399 0.5653475181 0.7782082327 0.569697561 400 0.5139552359 0.8691298212 0.528138879 401 0.1029199765 0.6415922479 0.738865932 402 0.9017956555 0.3681989533 0.014788399 403 0.1300081748 0.3727634479 0.084816476 404 0.5028760482 0.2834281570 0.753392662 405 0.6167644931 0.7392662682 0.451063136 406 0.7917854108 0.7611131743 0.813065219 407 0.1439542745 0.0858153393 0.649235810 408 0.0336355709 0.5852627060 0.615088174 409 0.6107383363 0.7312218356 0.835827948 410 0.2447151740 0.9148567214 0.042306389 411 0.5208800221 0.4018730107 0.781439138 412 0.3595253055 0.4949001716 0.004967534 413 0.6461015765 0.8702182316 0.973611509 414 0.1529908949 0.2444074138 0.765582877 415 0.1566277174 0.7693172845 0.068892283 416 0.5521451884 0.5519532149 0.177417176 417 0.6843436547 0.3240076548 0.900702647 418 0.1856415893 0.3318564647 0.096866918 419 0.0975673480 0.4760786693 0.471986917 420 0.7290326329 0.6597836316 0.955772876 421 0.5366336612 0.5549164847 0.553102183 422 0.6114070823 0.6587186693 0.007953998 423 0.0937127022 0.7113661575 0.876898227 424 0.0803781056 0.1035210942 0.598406682 425 0.3068068121 0.6794941770 0.188838216 426 0.4531581693 0.5950453898 0.409847462 427 0.3094340521 0.5425589513 0.650776462 428 0.2318717774 0.2191956949 0.648946756 429 0.5163603048 0.6094980745 0.447946401 430 0.5681897087 0.2336364791 0.295214692 431 0.4685455731 0.3028877801 0.068841520 432 0.7069857062 0.0218567387 0.233716408 433 0.3515282888 0.8465444185 0.803573804 434 0.4385973148 0.3473629637 0.617883441 435 0.4749626925 0.0890574595 0.188085022 436 0.3547779708 0.7379974185 0.791526760 437 0.1942676017 0.0079394246 0.997477157 438 0.0256246962 0.3535924666 0.698489423 439 0.8884308005 0.7628535840 0.809369163 440 0.0471414689 0.2817637813 0.904198337 441 0.1474339725 0.2587661603 0.242719773 442 0.7021593908 0.2530528193 0.524919445 443 0.4807156723 0.8349318991 0.070271240 444 0.8432966908 0.8468397048 0.470134102 445 0.3482438952 0.6815490499 0.064031017 446 0.1242048866 0.8927252733 0.522188024 447 0.6574711774 0.6846775450 0.890985590 448 0.7541771296 0.7206762349 0.774996555 449 0.7070043853 0.7600170255 0.267518109 450 0.9933493754 0.8513029092 0.694688216 451 0.5402909513 0.5576848101 0.181010432 452 0.0522799960 0.6944604991 0.640672027 453 0.5086942879 0.9485459770 0.182666690 454 0.2795328584 0.9230681325 0.071007218 455 0.2925957285 0.0272559715 0.396009628 456 0.4208056864 0.2842155597 0.657231304 457 0.9405288654 0.3529494354 0.067483188 458 0.3875212078 0.8732754863 0.913377441 459 0.2597758120 0.3927863508 0.867984843 460 0.2291357683 0.7875319326 0.051149221 461 0.1484227192 0.8090391355 0.705117003 462 0.4866195151 0.0734129762 0.442824473 463 0.7892362867 0.4453539846 0.121954784 464 0.5683366538 0.6619364661 0.186760232 465 0.2412845306 0.1936834501 0.890848876 466 0.1244373966 0.5603391614 0.533789977 467 0.4005386035 0.4215240139 0.446426343 468 0.6987896913 0.6909501497 0.352884758 469 0.8079235938 0.4607041548 0.625612257 470 0.5136809410 0.2786674271 0.611246112 471 0.1271985730 0.4093848013 0.702205135 472 0.4425768794 0.1256974903 0.462740155 473 0.2701393508 0.0287763786 0.963752114 474 0.0852672718 0.3647785818 0.785544906 475 0.8383072678 0.3441293056 0.229997027 476 0.5987685372 0.6069351842 0.916894967 477 0.3016595787 0.8286003179 0.098079569 478 0.9367990731 0.7363867506 0.099831548 479 0.8027157031 0.3406797077 0.764953703 480 0.5650394219 0.7784218604 0.571389669 481 0.8543107586 0.2325923357 0.177473234 482 0.5957680093 0.2863060525 0.621867881 483 0.5386852284 0.4697146376 0.561348902 484 0.9937895050 0.9476125687 0.195175298 485 0.8102212714 0.2234492698 0.496894690 486 0.3026532985 0.2417237794 0.904216400 487 0.8369588512 0.4713993466 0.017311705 488 0.3915421951 0.0331468829 0.286754538 489 0.8201161565 0.3621713065 0.306632450 490 0.0397220741 0.6384723403 0.862288375 491 0.6482997711 0.6928636658 0.498058393 492 0.5576362605 0.2238930671 0.915707878 493 0.6607955222 0.3236755331 0.891231451 494 0.4501192681 0.8065570174 0.833592845 495 0.9771914661 0.2452873208 0.684543002 496 0.7574431095 0.2715495902 0.929892377 497 0.1792897196 0.1909950075 0.430867825 498 0.6345388771 0.6050982235 0.213430690 499 0.7307245585 0.2590056360 0.491157572 500 0.9550017666 0.6125783336 0.430126610 501 0.4132789590 0.5587415013 0.707540820 502 0.1261792867 0.3612853771 0.656284911 503 0.9251572632 0.4820273374 0.021563028 504 0.4478524600 0.6163896762 0.859326350 505 0.7165222999 0.4614907785 0.907702564 506 0.6541780606 0.2412444674 0.490135853 507 0.9442219913 0.6415377590 0.456914689 508 0.9467369465 0.8996300348 0.901641032 509 0.9640912083 0.0410869946 0.980695251 510 0.3732388653 0.4830861366 0.464831451 511 0.1478042577 0.7105469978 0.361243103 512 0.7865168659 0.0258799670 0.610352629 513 0.4026820601 0.1141515123 0.073774567 514 0.9134162779 0.1482030996 0.838317783 515 0.9313118907 0.7333646615 0.186578115 516 0.7229551040 0.0172408349 0.876216590 517 0.1166268843 0.2034378981 0.622242737 518 0.5041656836 0.2613980127 0.729145212 519 0.0489721072 0.5793831972 0.163670554 520 0.9104810045 0.4284867200 0.348988740 521 0.8480586212 0.6895436323 0.741701515 522 0.4400068077 0.9950883300 0.472101074 523 0.5311336038 0.3148409440 0.508500267 524 0.4360992794 0.8477999608 0.074460126 525 0.5760545996 0.0326191103 0.332112870 526 0.6865018832 0.5540374245 0.623823412 527 0.5416533023 0.4976719327 0.197002878 528 0.4076520230 0.2285872947 0.757847161 529 0.2378068483 0.2019681430 0.869347882 530 0.6370063778 0.0563453555 0.534364452 531 0.6947946933 0.7111502644 0.327195446 532 0.8854648860 0.9748951907 0.583619741 533 0.5297113250 0.1012475549 0.639655538 534 0.5360497350 0.7042040324 0.703935319 535 0.8615456868 0.9087736702 0.259641427 536 0.3270071556 0.9687156640 0.510379517 537 0.5498819028 0.7116793904 0.800633240 538 0.3981204438 0.1055317647 0.332887672 539 0.1884881954 0.8991657628 0.651935739 540 0.9430633688 0.8470065235 0.985933232 541 0.2957795497 0.4845798714 0.620495155 542 0.5941085669 0.7551016384 0.626696358 543 0.6616673837 0.4585884756 0.332763716 544 0.2252898337 0.8735535287 0.250021933 545 0.8492385307 0.6050229876 0.976643395 546 0.3284978110 0.0208509073 0.552758015 547 0.5258779998 0.0941185125 0.712287780 548 0.0221860658 0.9694864992 0.365406210 549 0.9999005587 0.8640711615 0.799345604 550 0.2249033919 0.5597511516 0.985683206 551 0.2581388494 0.5881800281 0.297886255 552 0.0824325080 0.6931518228 0.818499598 553 0.2511851306 0.1219530550 0.073947041 554 0.5003025474 0.9877631674 0.345837254 555 0.1924816635 0.2932026354 0.722906101 556 0.9450151417 0.1164777910 0.899795833 557 0.0556947265 0.3653142145 0.833011540 558 0.7865477495 0.5493993000 0.922149335 559 0.3554171724 0.9726744783 0.872534773 560 0.3390859307 0.9219060326 0.820741070 561 0.7379814328 0.2120381724 0.693108915 562 0.0683563130 0.5029085143 0.827244471 563 0.1917957552 0.1268493799 0.267469959 564 0.9083188039 0.0586665533 0.942383993 565 0.9460777743 0.8651171569 0.837674263 566 0.5658098406 0.4958966884 0.791546705 567 0.3632471245 0.2307066286 0.804219844 568 0.8588721370 0.8229775466 0.228302267 569 0.5019012033 0.8366414213 0.455467683 570 0.2970695554 0.8778691809 0.141149248 571 0.3248096840 0.5907368434 0.104615112 572 0.5410586749 0.4280349151 0.594926988 573 0.2025833130 0.4288823237 0.677922638 574 0.9951313485 0.8449413823 0.481459997 575 0.5276287389 0.0723635927 0.321710046 576 0.1055847048 0.7462469260 0.912850004 577 0.7024908115 0.3999357389 0.186478262 578 0.7943248851 0.5395993334 0.920239061 579 0.0276398500 0.3101629145 0.118177905 580 0.1592246417 0.6026619161 0.576358764 581 0.7662116871 0.5950969979 0.813566438 582 0.4213486142 0.3764504716 0.827479247 583 0.7426504176 0.8308128808 0.507187529 584 0.6318207965 0.6347201027 0.125925017 585 0.7360877271 0.9745964138 0.068576889 586 0.3384028485 0.9400807733 0.005205019 587 0.4784741311 0.2316760146 0.342613811 588 0.4661606846 0.5499884067 0.588007716 589 0.2016149708 0.2244607336 0.990291373 590 0.1429585121 0.3208649538 0.698472717 591 0.5382393540 0.0899150085 0.088298887 592 0.6516490525 0.5870290762 0.460073830 593 0.7825980638 0.6463206944 0.834109541 594 0.0360894774 0.9555895720 0.374218001 595 0.0227680141 0.4798172456 0.230312304 596 0.6223328020 0.8188983048 0.259263913 597 0.0481542398 0.8776977926 0.928196097 598 0.4031829375 0.8861788490 0.386161405 599 0.3800641324 0.8961149775 0.744411892 600 0.9373641352 0.2962837527 0.228169761 601 0.1529641864 0.4246568638 0.561339771 602 0.3132382028 0.7459880235 0.701694527 603 0.6395901495 0.5864937957 0.118726775 604 0.3812313871 0.9908500304 0.687834089 605 0.6585895927 0.6807643520 0.807358899 606 0.0407362625 0.3835575725 0.941637933 607 0.6343256603 0.9410840643 0.201974260 608 0.7297780693 0.7785401796 0.685937374 609 0.8169210239 0.0184312759 0.995562205 610 0.6954687983 0.6902421599 0.107519840 611 0.3587358908 0.8174600382 0.887226315 612 0.4950156456 0.9988616975 0.460948817 613 0.5124422519 0.9000427283 0.311366444 614 0.4193161500 0.2330806397 0.331944056 615 0.1372738420 0.8558078955 0.634889112 616 0.6487600016 0.4483145340 0.200628157 617 0.6930406457 0.1405613457 0.946231753 618 0.5379465765 0.7251331760 0.693917649 619 0.3548438435 0.0162725917 0.684812097 620 0.5906515468 0.7329806052 0.904670520 621 0.6603484906 0.6076894940 0.091763721 622 0.3636276750 0.0564669154 0.066468826 623 0.8929951529 0.0173468851 0.913456712 624 0.5193466372 0.4707073863 0.934256916 625 0.6822469798 0.3953355562 0.289309436 626 0.0824959101 0.2921910062 0.044693490 627 0.7019017276 0.9668182791 0.750223392 628 0.4275053341 0.8367497751 0.332937845 629 0.0922464526 0.4102489238 0.812326842 630 0.4956276130 0.8354235855 0.744443205 631 0.5282284787 0.3624600177 0.437094010 632 0.4597527271 0.8304499635 0.176761237 633 0.0696412656 0.6655701364 0.389712420 634 0.3637543309 0.0200155203 0.947725208 635 0.1510170416 0.2281706810 0.351725206 636 0.6223586730 0.2150866047 0.854345126 637 0.7047801397 0.0133177778 0.107109227 638 0.0828919602 0.5865104373 0.861315191 639 0.3410899027 0.5765409295 0.923982420 640 0.1366543320 0.7885096699 0.751887410 641 0.2085322624 0.8079397904 0.664778188 642 0.4698440309 0.3802732481 0.143456051 643 0.3877236466 0.6920087750 0.195913500 644 0.5385239455 0.7067448399 0.208434227 645 0.7802318172 0.4299515062 0.629565495 646 0.0606531394 0.1196082544 0.712476384 647 0.7366263098 0.1238772105 0.722834192 648 0.4990216279 0.9745745591 0.335687502 649 0.1208609531 0.7079302194 0.138499342 650 0.5504307044 0.3144544242 0.634861803 651 0.5357888581 0.1576599039 0.895698367 652 0.8372551946 0.6539439561 0.121514479 653 0.8663264213 0.9303858271 0.101986314 654 0.8151593243 0.7399452543 0.564897440 655 0.5766503746 0.7543237149 0.178147843 656 0.7617674056 0.3053654318 0.211049212 657 0.5318685232 0.5440830449 0.438073355 658 0.3935762031 0.1543934196 0.792410720 659 0.6021584028 0.1720456653 0.780616953 660 0.4456974030 0.2089627727 0.670902713 661 0.1230438619 0.8262171666 0.825892488 662 0.1998506242 0.1914696826 0.688846716 663 0.8685076090 0.0552638469 0.592013019 664 0.1232273690 0.4114682395 0.111715549 665 0.5658827955 0.9385932428 0.630991100 666 0.0928694389 0.3512963140 0.551601721 667 0.6724121408 0.6340143762 0.619171673 668 0.7189048890 0.4819496642 0.394437478 669 0.7450684533 0.6334214911 0.120663491 670 0.4686049288 0.3850550903 0.225975410 671 0.3261675073 0.5564902842 0.439655563 672 0.7405954481 0.1667849601 0.778386813 673 0.5571954157 0.3600399296 0.341081738 674 0.3966088423 0.4169549660 0.984560798 675 0.8554791166 0.3829855390 0.851567330 676 0.2645492698 0.3424455107 0.810737652 677 0.9462965208 0.7442224536 0.081737134 678 0.2153703177 0.2868734032 0.846269100 679 0.3989983632 0.6966406952 0.934992647 680 0.2243203325 0.8841968733 0.506934221 681 0.7125027745 0.7271546698 0.810377810 682 0.0787354475 0.2923811390 0.245231815 683 0.2284422005 0.7890823116 0.217577334 684 0.2295348563 0.3330317193 0.373440096 685 0.7260223969 0.7212731063 0.153669810 686 0.2057637249 0.2594939065 0.245352808 687 0.5401250874 0.8030458153 0.469113623 688 0.4596121595 0.3054662987 0.468811055 689 0.3042413592 0.1917874399 0.576269256 690 0.2890700931 0.9998134219 0.942503915 691 0.8854543604 0.1116818776 0.383314030 692 0.6776874792 0.5922678534 0.693309897 693 0.5526406986 0.1233293717 0.087034084 694 0.1912126953 0.0772855950 0.887516976 695 0.7959685409 0.3028606996 0.898052296 696 0.3024255738 0.5259479519 0.319401479 697 0.6322551961 0.9411505456 0.893078855 698 0.0522503981 0.2155010924 0.074356175 699 0.4422330016 0.0192078033 0.237554085 700 0.2619074290 0.6082642223 0.982594245 701 0.9869507232 0.8930197172 0.273582796 702 0.4603780659 0.4993943570 0.096071468 703 0.2949062802 0.8040989162 0.918891617 704 0.4938740865 0.5205094554 0.076841577 705 0.0972780201 0.2060259327 0.607541317 706 0.8583569909 0.7992311558 0.047517068 707 0.7276338739 0.5565277084 0.241794257 708 0.4677256306 0.1712370019 0.228214926 709 0.0024954216 0.2747133188 0.322586709 710 0.3169592826 0.9069585775 0.423306790 711 0.7098196479 0.9432440118 0.358426370 712 0.2738314560 0.0971547274 0.470391238 713 0.3008469422 0.0383837584 0.365313627 714 0.7889818386 0.9164815550 0.875330874 715 0.2325974195 0.1971113200 0.561480183 716 0.5846996335 0.2200600684 0.825061466 717 0.5494406375 0.6702300981 0.063295416 718 0.1967900842 0.6804504094 0.817557294 719 0.9293358885 0.9159706607 0.612163059 720 0.4464485829 0.1391113657 0.016358729 721 0.1984538860 0.9922945008 0.260804367 722 0.6011682674 0.0078241592 0.994232268 723 0.4368371693 0.6018165236 0.271021605 724 0.0359789054 0.0309819141 0.291022451 725 0.4358850084 0.3571438494 0.672495972 726 0.0843085411 0.7200475056 0.543844765 727 0.9935347575 0.2231217008 0.162310042 728 0.8694156781 0.7412591046 0.116366554 729 0.3230546513 0.6869672353 0.355232729 730 0.2692568868 0.7453184524 0.318895462 731 0.9205692166 0.0721968049 0.747437760 732 0.9830821056 0.9566366056 0.735812146 733 0.3031027087 0.1807147223 0.515523697 734 0.8074569593 0.7621163037 0.277175065 735 0.1328059128 0.2716216373 0.386344013 736 0.3731001110 0.4226538686 0.893595606 737 0.8101217598 0.1615734680 0.545682621 738 0.9005346450 0.9483917356 0.162716153 739 0.5641075023 0.9057268400 0.315164645 740 0.7819188943 0.0401506533 0.443097079 741 0.7546357475 0.8867348845 0.545873337 742 0.9840175221 0.1725108344 0.646062576 743 0.1318724579 0.0028678142 0.238467700 744 0.2567289958 0.1308603347 0.707534412 745 0.6086509489 0.3696970518 0.405707285 746 0.7872655632 0.5497150687 0.446210140 747 0.6726261911 0.8797215447 0.694525995 748 0.2360209036 0.1031810008 0.939940774 749 0.7058318090 0.8746639634 0.876881678 750 0.5646322125 0.5175348681 0.899540994 751 0.3558960068 0.0915856815 0.071928061 752 0.7108148087 0.9289045038 0.409138034 753 0.3208266883 0.6941444234 0.420427548 754 0.7587849370 0.8223717990 0.617787307 755 0.8554418513 0.6104312195 0.685158405 756 0.5451220218 0.1108894546 0.754115731 757 0.7196569699 0.8765047307 0.742762204 758 0.0215774111 0.7833220700 0.358652892 759 0.7936053767 0.1422403960 0.228015036 760 0.5894972838 0.6867280926 0.727455930 761 0.5965793543 0.2810900006 0.049796629 762 0.6823342303 0.5309740093 0.709166650 763 0.4851749572 0.8965564165 0.270272462 764 0.2186638459 0.4845173971 0.223458015 765 0.5240909741 0.1192635912 0.836980513 766 0.7615321467 0.2461064239 0.305156313 767 0.5604892469 0.1504576304 0.778905628 768 0.5576477910 0.1195758597 0.729724572 769 0.0298766780 0.3231485880 0.810902923 770 0.6399024175 0.1802191990 0.031875964 771 0.4483339144 0.8136381572 0.266554726 772 0.1456385537 0.6185407676 0.424736338 773 0.9980573370 0.4370639510 0.906303392 774 0.1771693868 0.6572026338 0.648354213 775 0.3075491870 0.2273569726 0.480896802 776 0.3877885581 0.1247133319 0.641370056 777 0.3255822414 0.4710750806 0.601308974 778 0.9333021557 0.2480874057 0.644261836 779 0.6875130839 0.1116264574 0.975304031 780 0.3070184211 0.5074775650 0.232165708 781 0.5424951171 0.2500219222 0.605900436 782 0.9170924036 0.3036775775 0.130279193 783 0.3457037862 0.1089829586 0.030755766 784 0.2805052300 0.3548592539 0.194080905 785 0.1462095752 0.0438470747 0.631534894 786 0.7462217780 0.2378483214 0.231059715 787 0.6584212827 0.7611358352 0.918849279 788 0.2268953726 0.0877234547 0.932185473 789 0.5527597035 0.3166887127 0.353739069 790 0.2126589350 0.4487409566 0.044028442 791 0.6011075461 0.7731962018 0.346391862 792 0.4955664894 0.4370704067 0.739043013 793 0.8423375196 0.1270604522 0.317217213 794 0.1852961355 0.2097269609 0.088107983 795 0.9665051368 0.2191235535 0.096969624 796 0.7390349819 0.7743024451 0.050907627 797 0.3564518855 0.6329019496 0.299431361 798 0.1001586283 0.9697424432 0.119666547 799 0.2597023719 0.6517637181 0.153376347 800 0.6991052001 0.9141410457 0.265971137 801 0.4336216610 0.6204367809 0.287241898 802 0.9722799619 0.9041905978 0.949226398 803 0.3676507603 0.7783370393 0.605557245 804 0.9472060276 0.1846980839 0.961454876 805 0.7027460397 0.0863679377 0.350598877 806 0.1309114473 0.5038668043 0.978715864 807 0.2550966237 0.5919990689 0.726682640 808 0.2965044272 0.7501525732 0.205615580 809 0.2213688432 0.3952954053 0.998190792 810 0.4808703209 0.4208385709 0.995545863 811 0.3624091498 0.4595744461 0.799096379 812 0.8317626240 0.2396454047 0.548187687 813 0.6742728841 0.1104490117 0.598536491 814 0.1261447764 0.1552675441 0.272370511 815 0.9580946744 0.4285296819 0.968113240 816 0.6149920234 0.8321298480 0.698046484 817 0.5011624463 0.6944097814 0.278748058 818 0.9852243848 0.2406683902 0.018022690 819 0.0772609098 0.7450529367 0.039625611 820 0.5415186076 0.4747119257 0.896825491 821 0.3348767199 0.6098611136 0.577773161 822 0.4783724891 0.3886900644 0.677212795 823 0.3442536590 0.7314892649 0.086071891 824 0.5835320374 0.4907233738 0.177435393 825 0.9339889446 0.3731712026 0.393874652 826 0.9251223728 0.6926013883 0.662840765 827 0.3417600116 0.5710211955 0.394355623 828 0.4704917639 0.5288532262 0.204753422 829 0.6319271945 0.8954059565 0.101903225 830 0.7197887672 0.7918050652 0.871796291 831 0.6301961564 0.2209867758 0.095405005 832 0.7079224242 0.9972060299 0.683585294 833 0.5663749434 0.4601809371 0.591159837 834 0.8835416073 0.9517572247 0.021346556 835 0.3805341318 0.7682048625 0.607100267 836 0.1149117290 0.3352859647 0.221796387 837 0.6165271252 0.1312675015 0.518161186 838 0.1176810323 0.2363307266 0.026905061 839 0.3870377922 0.6967954086 0.984365234 840 0.5219549667 0.0975140380 0.633635873 841 0.7895589520 0.0749162380 0.677225510 842 0.7397307083 0.4174277787 0.146708182 843 0.3478056667 0.4072185773 0.446643704 844 0.1604789551 0.2810919166 0.376148333 845 0.9251088433 0.8910745184 0.184053494 846 0.0136304810 0.8324750320 0.012508075 847 0.8104454086 0.9549242419 0.174654766 848 0.2838601123 0.4352603666 0.662441208 849 0.9989845387 0.4433427551 0.085998558 850 0.3344213252 0.8014390990 0.625002468 851 0.0715592853 0.2017857837 0.922036019 852 0.5103901837 0.6150838642 0.096749347 853 0.0633041605 0.1500133087 0.950916851 854 0.3187373579 0.5897086798 0.941266669 855 0.2308404257 0.6086097441 0.500319101 856 0.0412874152 0.1480539243 0.938956048 857 0.7764879493 0.2771860566 0.941295495 858 0.6102122869 0.8902521830 0.208078722 859 0.6847100409 0.1657704103 0.559949947 860 0.4759049457 0.2603934291 0.265524188 861 0.7726181280 0.9765032728 0.764477738 862 0.1444186131 0.4385247866 0.596427928 863 0.8826502324 0.5756433317 0.878524726 864 0.9136634860 0.1600622404 0.461303858 865 0.0380516683 0.7695483940 0.921434690 866 0.8698804867 0.4721365480 0.359045910 867 0.9890793655 0.4400611159 0.450260557 868 0.2904401754 0.1828926203 0.895283058 869 0.8492796938 0.0380649534 0.518706080 870 0.7597751443 0.3915797304 0.304194722 871 0.5994729039 0.2184883629 0.986610676 872 0.2150855558 0.0174954759 0.904102191 873 0.8826147132 0.5637028904 0.361423424 874 0.9353977251 0.9109342301 0.867015991 875 0.5928153037 0.4265120404 0.147806567 876 0.2736188730 0.3341847470 0.252160472 877 0.4527158521 0.2142571628 0.992705174 878 0.3849735709 0.0373925450 0.334103196 879 0.7877801368 0.9842010399 0.195866342 880 0.1202234565 0.1930400908 0.652330256 881 0.2110027948 0.5344378769 0.642685626 882 0.7905226327 0.3779459668 0.635173605 883 0.2934547048 0.3679792546 0.593422699 884 0.4573139457 0.1582164683 0.023487672 885 0.8029606193 0.7010082020 0.284318620 886 0.9665408591 0.6073600804 0.660012832 887 0.2198708144 0.1453944338 0.889665270 888 0.2914561022 0.7143639419 0.116632195 889 0.9410663755 0.6670784273 0.292615006 890 0.7086967225 0.8629533281 0.860167607 891 0.0894650980 0.1249317278 0.458329211 892 0.4917318639 0.5387740363 0.287814197 893 0.9280869819 0.7142212233 0.937446594 894 0.4264453109 0.0457125187 0.646521847 895 0.7130430103 0.2010276795 0.914412531 896 0.7420520796 0.7263595280 0.231711044 897 0.7444685814 0.4017101536 0.065026434 898 0.8719011499 0.9411909385 0.356957232 899 0.3430910874 0.3079510119 0.265917442 900 0.5024578348 0.2550702172 0.718842493 901 0.4036464305 0.0388136716 0.539094977 902 0.7733309979 0.7575183806 0.438675189 903 0.7544153144 0.0066090750 0.055222656 904 0.6703264886 0.6471884502 0.448854770 905 0.2629647448 0.9012367099 0.881672440 906 0.1803582727 0.3752786340 0.852202812 907 0.8431271741 0.5248582852 0.310761757 908 0.2284120221 0.1554611062 0.271397468 909 0.9815748923 0.3106818793 0.503561401 910 0.5820819039 0.8009110682 0.313130387 911 0.3962558024 0.5859679591 0.967218323 912 0.2827562944 0.0372282511 0.217953848 913 0.8501226422 0.3453022940 0.648826417 914 0.6198121691 0.5673128264 0.431295119 915 0.8348960320 0.0189236847 0.810088007 916 0.9729937878 0.1587446914 0.135784560 917 0.6216949238 0.5137279432 0.302157789 918 0.3172746876 0.7171976375 0.318524733 919 0.2923211546 0.7407354964 0.835473312 920 0.9858742347 0.5725584505 0.127078771 921 0.1445829689 0.2193578896 0.297499642 922 0.1202723817 0.9384786098 0.926139504 923 0.5611545546 0.5124028493 0.830471531 924 0.6683973325 0.0950436117 0.228388001 925 0.3101932399 0.1064313874 0.696718747 926 0.0065761928 0.5799916589 0.728362826 927 0.2705754482 0.6334289019 0.213996048 928 0.6163870748 0.9408771771 0.305505149 929 0.5825870181 0.7055658610 0.854551745 930 0.0560807060 0.2184536899 0.973126440 931 0.0305702714 0.2604252915 0.294440166 932 0.1147877721 0.2299285294 0.343987265 933 0.6380620983 0.7959253674 0.404231852 934 0.5519847185 0.9795302413 0.858354224 935 0.9288181656 0.8940306874 0.888728004 936 0.3502963928 0.8894755677 0.944722258 937 0.0143268388 0.0346940127 0.235338558 938 0.0341333786 0.7793426653 0.963776887 939 0.7106323622 0.5434363140 0.751127602 940 0.1490545860 0.4038637085 0.716740757 941 0.4734449964 0.4437388908 0.475056946 942 0.7208320657 0.9866875494 0.543951573 943 0.2622331488 0.2832836569 0.035830080 944 0.7833650876 0.7233259575 0.428406089 945 0.5218209329 0.1005121029 0.218219518 946 0.7760500603 0.6374043820 0.052455828 947 0.6739972050 0.0909351802 0.904614732 948 0.8914999729 0.1385136431 0.479137238 949 0.1951620504 0.8737525195 0.972547704 950 0.4718771121 0.2480492322 0.041709913 951 0.2666831575 0.6943463988 0.988687727 952 0.7738986546 0.1257526183 0.893773123 953 0.1496802690 0.3521917525 0.754022287 954 0.4720940117 0.6001135744 0.338170915 955 0.8082588040 0.8172353797 0.916966529 956 0.5283849533 0.7120828282 0.081214865 957 0.8441957091 0.4700241142 0.493242827 958 0.0938648465 0.6583943067 0.981820053 959 0.3471312686 0.0675094635 0.037047612 960 0.6650372180 0.3251894296 0.116741180 961 0.7140586991 0.0734348153 0.631955886 962 0.6655903978 0.7986309046 0.094634300 963 0.1438620712 0.3221085093 0.243953107 964 0.7278595115 0.2289379491 0.362771598 965 0.1802145545 0.8742005217 0.723245273 966 0.7823882208 0.3580379246 0.054962467 967 0.6995504668 0.0912349010 0.632198407 968 0.7469125430 0.3847571795 0.911859033 969 0.1873785281 0.8334370614 0.705362193 970 0.2881696546 0.5870317714 0.125191815 971 0.4128162214 0.0516679150 0.382608344 972 0.4137606274 0.0732873864 0.657100869 973 0.7626614401 0.3218054264 0.278459829 974 0.6579748753 0.9010156130 0.481620893 975 0.7438406860 0.4247153068 0.448899247 976 0.0268771388 0.9448438413 0.121263162 977 0.4899677269 0.7269265945 0.664452199 978 0.8597049168 0.5871146580 0.460823134 979 0.2966905709 0.5181974168 0.803420998 980 0.3388050934 0.8238629079 0.973437019 981 0.4403209696 0.8365467857 0.604767612 982 0.3581107967 0.7897584133 0.748093019 983 0.9181199386 0.7010477460 0.974371480 984 0.4855272081 0.5026488579 0.734248891 985 0.7215003686 0.7091594590 0.313281513 986 0.6219019089 0.2435479793 0.284571647 987 0.5285257522 0.3192963838 0.505222295 988 0.9388092165 0.8667535833 0.026928221 989 0.2814255413 0.7957225109 0.526132444 990 0.5068233504 0.2621329527 0.250766840 991 0.2548627399 0.4510898469 0.885296197 992 0.8310871634 0.1405788586 0.569291269 993 0.4782470900 0.0357218925 0.887661504 994 0.1346597481 0.9022380263 0.849145710 995 0.1659126263 0.7283752465 0.633044650 996 0.1651590609 0.6786643637 0.996066710 997 0.6562744505 0.5517038938 0.438446892 998 0.3805340903 0.9445086999 0.743781529 999 0.8062844079 0.7442193504 0.075846629 1000 0.9749252927 0.7738602485 0.271969781 # select all of the columns from rows 5, 11, and 12 of the dataframe data [ c ( 5 , 11 , 12 ),] x y z a m i j 5 0.4706880 0.2900230 0.8256190 0.04703202 0.8461648 0.8411586 0.3029567 11 0.7780771 0.4888598 0.4218451 0.54172635 0.3588103 0.5606056 0.5415306 12 0.8549661 0.6202295 0.1322223 0.37803832 0.6000676 0.1668717 0.3277201 k 5 0.5858587 11 0.1178834 12 0.3200709 # use a logical test (675 > 54) to select all columns from certain rows data [ 675 > 54 ,] x y z a m 1 0.9370949692 0.3904391259 0.2530045395 0.577547544 3.311816e-01 2 0.8169912936 0.7717700286 0.1949706976 0.431474575 4.468994e-01 3 0.6313947993 0.5873462579 0.4276207259 0.449063063 2.161926e-01 4 0.0716134447 0.4721571105 0.7047920560 0.530008280 8.032733e-01 5 0.4706879971 0.2900230386 0.8256189562 0.047032020 8.461648e-01 6 0.3116779991 0.4097562127 0.2170954866 0.047558205 8.109110e-01 7 0.7180650725 0.8154219799 0.2912252382 0.941795656 8.794287e-01 8 0.1410273889 0.2734181802 0.0728253024 0.958947799 7.707356e-01 9 0.1039375949 0.6066154244 0.7298178351 0.237605995 9.278795e-01 10 0.7831988228 0.3816812821 0.0445121613 0.878714589 8.033606e-01 11 0.7780771402 0.4888597687 0.4218451059 0.541726347 3.588103e-01 12 0.8549661185 0.6202294999 0.1322223113 0.378038323 6.000676e-01 13 0.7936905599 0.8205492713 0.5068847497 0.344106114 5.298757e-01 14 0.0497731785 0.4382137246 0.3183242229 0.206805342 4.169467e-01 15 0.1041691788 0.2583737297 0.8669875974 0.409303598 8.821301e-01 16 0.5839785282 0.9458635810 0.1451297677 0.124021082 4.282974e-01 17 0.6301283748 0.8938101421 0.4633655346 0.863932969 8.371921e-01 18 0.0538908630 0.7993367051 0.1662894906 0.337508279 3.171606e-01 19 0.4699743150 0.2179973929 0.9039500949 0.542787326 7.940503e-01 20 0.8096281542 0.4553503187 0.5965530800 0.498133432 6.258840e-01 21 0.6269215296 0.7560675466 0.7957581254 0.840931672 5.378133e-01 22 0.4254209346 0.6859814830 0.8225671803 0.186610433 5.037850e-01 23 0.1013888507 0.2460164321 0.7510759791 0.237333820 7.254038e-01 24 0.9101979593 0.4163650593 0.5020412898 0.633828752 2.637897e-02 25 0.7577146925 0.1546449522 0.4458510922 0.467762029 2.085573e-01 26 0.8808603676 0.8900883312 0.6414468170 0.336437304 1.080850e-01 27 0.2358829738 0.6140667766 0.6801568472 0.451031481 2.824390e-01 28 0.3214730755 0.8646407460 0.2427712576 0.178794772 1.326834e-01 29 0.7767506840 0.9355752321 0.4893625444 0.524841033 5.627676e-01 30 0.2756272962 0.7027894342 0.4434864377 0.173538475 7.091635e-01 31 0.7999626761 0.1894985719 0.6973984847 0.389671007 1.434607e-01 32 0.4605515839 0.1737330768 0.2664361666 0.525079773 3.887599e-01 33 0.7878976951 0.9669068004 0.2513810424 0.279101572 6.431891e-01 34 0.0911097701 0.0928617879 0.2627664267 0.123210540 8.142502e-01 35 0.0275765241 0.6088669079 0.7417254893 0.389373654 6.733047e-02 36 0.7061024618 0.8505606914 0.6830506644 0.284471479 3.249732e-01 37 0.6172355278 0.4425195777 0.8666974416 0.469516267 6.445455e-01 38 0.0632016384 0.0005237793 0.7255827556 0.371542407 5.937821e-01 39 0.8622419508 0.7715821874 0.3958992928 0.667401103 2.421245e-01 40 0.7231777157 0.8044306301 0.0778860459 0.172507857 8.447065e-01 41 0.4927164968 0.1889234560 0.4412154641 0.650509367 9.200443e-01 42 0.5703471822 0.8337357312 0.4939526306 0.031746348 8.359302e-01 43 0.8936006089 0.4499214077 0.0217764010 0.729141285 2.380587e-01 44 0.6202150190 0.3614634674 0.9754849086 0.612729790 3.214372e-01 45 0.7912620052 0.1988568865 0.7979321314 0.248550579 4.465604e-01 46 0.5302869780 0.8204511884 0.5899471235 0.236167135 4.993286e-01 47 0.3760423027 0.6197578758 0.9657687983 0.816466525 4.290283e-01 48 0.4486860153 0.7791226325 0.9174016686 0.599676585 8.931907e-01 49 0.4307841142 0.0385082576 0.8245737706 0.897940309 1.788797e-01 50 0.8595965470 0.3248715224 0.7299323552 0.886358950 8.883161e-01 51 0.0252711680 0.6122176033 0.9074188452 0.897394733 4.080858e-01 52 0.1002211126 0.2690452361 0.3296885516 0.345949224 6.038600e-01 53 0.0807856577 0.0998913255 0.8701844758 0.269625058 3.758807e-01 54 0.7170568479 0.4884574136 0.5018407770 0.403440406 8.281473e-01 55 0.8823796129 0.3786438145 0.3983667553 0.804715395 6.636888e-01 56 0.5683260462 0.7532928397 0.6130672316 0.679588470 1.800565e-01 57 0.8688152484 0.2847076976 0.3810614778 0.824492445 1.734603e-01 58 0.6965453394 0.6635359814 0.1475261322 0.804251817 8.262297e-01 59 0.0802273641 0.3858547176 0.6009099570 0.519138621 5.420346e-01 60 0.0140079728 0.1603731690 0.9542593956 0.364582830 4.444947e-01 61 0.4727872738 0.4798752652 0.6213749733 0.598921384 3.226989e-01 62 0.0459575092 0.0832084191 0.6381078002 0.244956968 3.561886e-01 63 0.1385185381 0.6099833576 0.4599676093 0.350077863 6.626158e-01 64 0.8986839550 0.6535483883 0.4232784463 0.657276285 5.436132e-01 65 0.5309889077 0.4783877260 0.0462058538 0.839267493 4.156813e-01 66 0.0409239323 0.2436686188 0.9950620085 0.206467974 5.036194e-01 67 0.7674356611 0.6750660734 0.6059721843 0.020811806 9.566451e-01 68 0.3960749088 0.2688281150 0.0242629193 0.732992485 2.720517e-02 69 0.3985890197 0.7642159278 0.8804058409 0.930405641 3.359340e-01 70 0.3705691348 0.0467719736 0.1671624330 0.061075286 6.948139e-01 71 0.2754363231 0.9146933509 0.5973722835 0.277121296 2.015421e-01 72 0.5407387190 0.9995588632 0.1497331320 0.887567588 9.082803e-01 73 0.5314532970 0.0659897071 0.3875588439 0.617622069 2.811714e-01 74 0.0803362590 0.4144914525 0.2956018788 0.316371846 2.652967e-01 75 0.8250675497 0.7498207777 0.1106022566 0.318800121 5.904470e-01 76 0.1926816499 0.7235343596 0.7979764920 0.609599486 7.587092e-01 77 0.0425402028 0.0603169515 0.7010821693 0.264385223 3.563557e-01 78 0.3964480858 0.9809383829 0.1755605682 0.750374798 1.333250e-01 79 0.4223065777 0.0653125530 0.9426773726 0.399970984 1.586909e-01 80 0.6523972722 0.2083718709 0.4009221988 0.142288364 9.384446e-01 81 0.3475282227 0.1577986199 0.2000566917 0.625475666 9.390198e-01 82 0.5286904851 0.8383829324 0.3611871728 0.550002784 5.167771e-01 83 0.1822423232 0.3047285695 0.2869544714 0.530641184 2.719163e-01 84 0.0047013757 0.3647190086 0.0340154995 0.298535647 5.472704e-01 85 0.3293453683 0.9229505726 0.7833694841 0.632987165 7.515297e-01 86 0.1202686552 0.4016410608 0.3511313878 0.629080631 8.448410e-01 87 0.8493540538 0.6416005327 0.0867994754 0.628401057 2.774768e-01 88 0.7695235745 0.5029771237 0.6142535997 0.687793811 6.338755e-01 89 0.6904120625 0.8910172952 0.5705231265 0.182845271 7.624777e-01 90 0.3335720669 0.2518689649 0.7833944124 0.695882965 5.926175e-01 91 0.7919240277 0.1950826326 0.6733682633 0.899178525 8.392912e-01 92 0.6215966814 0.8693118924 0.6596714961 0.815885935 1.616443e-01 93 0.5720835368 0.9879097429 0.7489858174 0.896938740 7.624196e-01 94 0.4893391300 0.5527650674 0.8465101460 0.806428441 8.202123e-02 95 0.3000833855 0.4928710270 0.7013142661 0.701419483 7.109783e-01 96 0.1436036394 0.3200193204 0.3412105888 0.606645083 1.577007e-01 97 0.5476652577 0.2205543050 0.3517985044 0.930552275 2.001931e-01 98 0.9131340059 0.0837841036 0.7133782210 0.719024671 7.160846e-02 99 0.8401359348 0.0370367223 0.4614702794 0.079629550 8.718041e-03 100 0.4504831121 0.2381348317 0.4102221406 0.520588716 8.591019e-01 101 0.6254940971 0.5222004815 0.8796439485 0.827647213 5.727372e-02 102 0.7448368245 0.6181551786 0.4219239037 0.467653792 6.276878e-02 103 0.9457870251 0.4207581431 0.3550725051 0.006337496 1.791115e-01 104 0.1310959703 0.4963657029 0.1765153394 0.359040588 6.913411e-01 105 0.8484062718 0.9034369027 0.6787415841 0.586619119 5.452538e-01 106 0.4371888463 0.4749349221 0.6336629451 0.044809929 7.567161e-01 107 0.9108372824 0.8643784064 0.8615905407 0.148015360 1.638905e-01 108 0.4796622975 0.0227832580 0.5454826001 0.527876457 2.512274e-01 109 0.1027305629 0.1072980338 0.3083832473 0.525531210 6.378927e-01 110 0.2847951869 0.4515920919 0.3195284132 0.648003328 2.805665e-01 111 0.9316735922 0.0052489392 0.7868197537 0.505368755 4.154278e-01 112 0.0924596551 0.8432675595 0.4399540033 0.789810633 8.377471e-01 113 0.5673171869 0.2813623182 0.6962677925 0.029624238 3.589913e-01 114 0.8735257180 0.5234353847 0.2538753240 0.232593814 9.064808e-02 115 0.4290504728 0.4694526973 0.2661211840 0.907042397 1.552227e-01 116 0.8996819127 0.0621974380 0.3251096997 0.853816796 2.705220e-01 117 0.0579402959 0.0302452876 0.9352964088 0.585931442 9.032564e-01 118 0.1014418246 0.8006376394 0.2328273214 0.820464912 5.355829e-01 119 0.4509296967 0.1611418875 0.2132689748 0.153000990 1.609333e-01 120 0.2546673652 0.5602219575 0.5468407678 0.947769340 4.493483e-01 121 0.7113273235 0.8592565532 0.1178469069 0.498519980 5.536828e-01 122 0.7692845000 0.0566702003 0.9734982299 0.122954023 6.887685e-01 123 0.3481028744 0.4445713721 0.3522821588 0.787033934 7.327817e-02 124 0.9676634180 0.8296126574 0.9950421427 0.103310564 3.178156e-01 125 0.7331326583 0.0971798035 0.2761500876 0.127017952 4.720898e-01 126 0.0599658110 0.7115563331 0.0540546447 0.106573350 8.228742e-01 127 0.1248628269 0.7587779057 0.1176031553 0.934332104 5.604531e-02 128 0.2991520732 0.3776659367 0.5170687952 0.392918030 3.367465e-01 129 0.6645840129 0.7026710063 0.7148548134 0.970120105 8.332619e-01 130 0.1244262301 0.9337320582 0.1367022363 0.913373896 6.899806e-01 131 0.1236656245 0.0614476257 0.9959184171 0.158355799 8.971463e-01 132 0.0696135804 0.3393731464 0.3305444729 0.654447101 9.089449e-01 133 0.3501586101 0.5802877264 0.3816649450 0.935220445 7.389805e-02 134 0.5799933816 0.7677533829 0.1642848232 0.147463179 3.779670e-01 135 0.1702301877 0.9032238321 0.7545808731 0.886644857 7.954767e-01 136 0.9765062209 0.9296272635 0.8273214900 0.506397160 7.001717e-01 137 0.7148984962 0.8073559222 0.2429534406 0.094754952 4.551603e-01 138 0.3888362625 0.3348044397 0.1438325557 0.191665294 2.816520e-01 139 0.0689336834 0.3640246105 0.3012798512 0.644458028 4.497799e-01 140 0.9315936987 0.7868268418 0.6475578381 0.858399672 8.137392e-01 141 0.7053553022 0.0936103060 0.3798077465 0.989375187 9.145746e-01 142 0.7641971211 0.1513883369 0.6868384471 0.037770061 4.302203e-01 143 0.7521351997 0.9857477553 0.5470604110 0.860812797 9.163151e-01 144 0.1143717000 0.5475322953 0.3682726121 0.548567706 5.226119e-01 145 0.1755395108 0.1196474847 0.8436278808 0.067070395 5.931327e-01 146 0.7314520737 0.8785161506 0.3468810113 0.807048478 1.057343e-01 147 0.0840596480 0.0384507929 0.0654803049 0.837991760 3.502395e-01 148 0.6503178864 0.4422026391 0.9759091735 0.641077164 5.667044e-01 149 0.8325164884 0.1573755434 0.4993437151 0.488269411 5.032156e-01 150 0.6056464242 0.9779126472 0.0997745579 0.719270619 4.198256e-01 151 0.1708778741 0.0848242871 0.1683927856 0.489719765 1.845283e-01 152 0.7192412303 0.4588409185 0.8876504118 0.228980459 8.690613e-01 153 0.7769590721 0.8675205393 0.6314575365 0.143649914 6.875175e-01 154 0.3580576980 0.3434970572 0.9685689912 0.824826804 6.023344e-01 155 0.1536754405 0.7642185981 0.7190012864 0.920236114 5.640764e-01 156 0.8876610205 0.2691004938 0.9682509759 0.009020783 7.899509e-01 157 0.9968752931 0.8581504109 0.4439522170 0.612094169 9.859647e-01 158 0.5289386453 0.6496191269 0.2969512208 0.413060909 3.806414e-01 159 0.6329279244 0.0386093326 0.3381252782 0.360855727 3.545589e-02 160 0.0034896852 0.1933919925 0.7016994103 0.594918283 8.101724e-01 161 0.8976498600 0.1778210327 0.4423170090 0.982059956 4.761609e-01 162 0.2601647917 0.5901049029 0.1208666377 0.791583062 8.117129e-01 163 0.3986346393 0.8277773953 0.4906746778 0.731339336 7.174461e-01 164 0.5764877701 0.6677092968 0.3722350411 0.684600540 4.484358e-01 165 0.7178556903 0.1503093268 0.0881471203 0.700219793 5.278507e-01 166 0.1486660023 0.3068179993 0.6048259446 0.367801031 8.009471e-01 167 0.2934395475 0.3574407734 0.1875770059 0.305140699 1.799086e-01 168 0.8550011665 0.9941004280 0.6293192946 0.323219864 9.496474e-01 169 0.9138944144 0.1918540595 0.7362092163 0.129870965 7.500945e-01 170 0.5692845336 0.0576184834 0.9621483448 0.946476570 3.975674e-01 171 0.2180541193 0.5792422167 0.7194545367 0.424145223 2.506493e-01 172 0.8578979487 0.7784323918 0.1277054867 0.572289248 2.549385e-01 173 0.0568097783 0.8187201719 0.5725159331 0.619513634 2.115072e-01 174 0.9992063609 0.8830820697 0.0208103533 0.271749542 2.407214e-01 175 0.4691610557 0.1709448709 0.5638727772 0.205742766 7.858254e-01 176 0.9072538142 0.8136773086 0.7826462083 0.661408056 9.535864e-01 177 0.1765437901 0.9630451202 0.0341300871 0.065843770 6.814811e-01 178 0.2081198960 0.5721260966 0.8691763720 0.623829032 7.277410e-01 179 0.9020622084 0.7788336426 0.9740692992 0.897657196 6.079173e-01 180 0.5577519296 0.8870667333 0.0559192663 0.917418429 1.502496e-01 181 0.3174477716 0.5591419074 0.9091531408 0.689324741 3.603843e-01 182 0.3988291954 0.9360842388 0.9412023835 0.151519891 4.725731e-01 183 0.6296822894 0.2734462984 0.5310084445 0.435572824 3.481015e-01 184 0.8532116541 0.3789087695 0.4137729306 0.792026432 6.575724e-01 185 0.0019157159 0.9480523288 0.0558498637 0.690238338 5.564615e-01 186 0.2165099829 0.2713331431 0.7154832364 0.642090820 6.811029e-01 187 0.0528591487 0.4438034375 0.6167128414 0.823401734 5.188929e-01 188 0.2140440524 0.9970479552 0.5048359402 0.773988509 3.399884e-01 189 0.6131354324 0.2978879120 0.5568822010 0.096377717 8.602817e-01 190 0.3969264787 0.1390109258 0.3459203229 0.515168304 2.417488e-01 191 0.8599386800 0.0208728844 0.6721742607 0.940914826 1.805654e-01 192 0.3918516901 0.8487497859 0.5915288015 0.326262000 3.761560e-01 193 0.0001515732 0.9925137931 0.0870158968 0.911938774 1.018230e-01 194 0.9137823463 0.9482441705 0.6661104674 0.009660149 6.747976e-01 195 0.3185021740 0.4742731366 0.6751790117 0.216926428 7.693648e-01 196 0.5787437696 0.0562161675 0.0022941325 0.981683360 9.763876e-01 197 0.7559065737 0.3867585564 0.3821420241 0.401733570 4.982639e-01 198 0.5163620966 0.6031210783 0.5991793235 0.768042127 7.776593e-01 199 0.3772833832 0.2417656146 0.0676334375 0.179017943 5.219626e-01 200 0.8303419396 0.6909590135 0.2075274838 0.982338600 2.271104e-01 201 0.7211326503 0.6640000213 0.9592173509 0.649954116 7.933037e-01 202 0.5639012777 0.2657555621 0.3693294188 0.653942175 9.238735e-01 203 0.2644100867 0.4423922328 0.5288352752 0.791906926 9.936834e-01 204 0.4798865798 0.3394446452 0.4974069681 0.758591116 6.578006e-01 205 0.3535982657 0.3782534536 0.2380272320 0.377593074 7.628336e-01 206 0.7971998851 0.0415869437 0.9998667797 0.292781072 5.019907e-02 207 0.9474380957 0.1888907838 0.6537682402 0.442967691 5.412440e-01 208 0.8931702694 0.6448802592 0.6577227118 0.882041997 9.742949e-01 209 0.3331450461 0.0773383095 0.3559794263 0.825966841 5.472149e-01 210 0.3673691975 0.0164993445 0.3723789565 0.994233836 9.561312e-01 211 0.0161621387 0.4990861469 0.4040669564 0.980775977 9.391028e-01 212 0.6557840866 0.7605664600 0.5519499718 0.770046454 9.594055e-01 213 0.2010356160 0.9608684320 0.2316770859 0.630396901 9.695247e-01 214 0.5701537321 0.2108435808 0.0258280484 0.855769821 3.193294e-01 215 0.2032754768 0.1362372376 0.3567790452 0.898438573 6.869137e-01 216 0.4177006818 0.8037221890 0.7529435763 0.010452493 1.591609e-01 217 0.2006519388 0.8048699212 0.8357756212 0.413250533 4.062866e-01 218 0.1065086264 0.6491837867 0.2524011680 0.955415523 7.185965e-01 219 0.9091413908 0.4442297982 0.6562724640 0.923782989 3.390019e-01 220 0.0242540571 0.9883826412 0.8197069752 0.814794991 5.391943e-01 221 0.5542232401 0.8130007542 0.5624401649 0.064768330 9.854170e-01 222 0.6204521400 0.4963250675 0.2481274225 0.389114767 8.654488e-01 223 0.5394885978 0.2772327969 0.1048048458 0.480904928 6.280108e-01 224 0.0745619617 0.1558587539 0.7687460070 0.786436838 2.117698e-01 225 0.2163315983 0.6801391370 0.8055323400 0.686167828 5.312500e-01 226 0.0676476448 0.8733174466 0.3134679864 0.369815800 9.743214e-01 227 0.5916824974 0.0191610819 0.3581683834 0.237326151 8.822456e-02 228 0.2836414992 0.8274080611 0.8160740463 0.393665259 6.703462e-01 229 0.2384083509 0.3266589346 0.6799719462 0.637329965 6.830281e-01 230 0.8608596043 0.3201513030 0.1370995173 0.974307173 3.404101e-01 231 0.5796163038 0.9357685193 0.7526069139 0.197657710 1.402370e-01 232 0.2412788384 0.1476416474 0.1673243914 0.086085135 7.060136e-01 233 0.3736466474 0.0715341098 0.1843706726 0.281694533 8.314010e-01 234 0.3288877220 0.3422220629 0.3864410103 0.129961584 3.187321e-02 235 0.2161439408 0.8040320897 0.1244801774 0.744883351 3.924180e-01 236 0.6735424299 0.7795171738 0.4039235790 0.011590910 9.798672e-01 237 0.1124215857 0.4681109225 0.7216107910 0.829600236 8.380484e-02 238 0.8839060992 0.7237063833 0.2847857845 0.846192709 5.581730e-01 239 0.8819734182 0.1144621901 0.0116158570 0.725888924 3.569096e-01 240 0.9354603821 0.4831247553 0.5211163305 0.773195356 5.565796e-01 241 0.5767845511 0.2684109176 0.6986416469 0.510211729 6.383459e-01 242 0.5818626797 0.9073878161 0.1752908952 0.758165545 5.812421e-01 243 0.8275333415 0.5267427347 0.9090091116 0.344543359 5.143553e-01 244 0.4781270074 0.4272068595 0.4470525885 0.347394072 9.982698e-01 245 0.7396636771 0.5585044932 0.3863007929 0.517322354 9.656356e-01 246 0.8310291567 0.5190551940 0.0384265762 0.150503708 2.366544e-01 247 0.3675826853 0.3041335819 0.6259368560 0.022825431 5.922777e-01 248 0.1966541104 0.5881767110 0.3664567897 0.618014229 9.890586e-01 249 0.9975247402 0.8564226844 0.0035603254 0.301549508 1.862727e-01 250 0.8214518884 0.7014278157 0.5982679054 0.192868671 8.084818e-01 251 0.5489195844 0.6956572754 0.4245661907 0.657216802 6.434412e-02 252 0.9222708542 0.4495614534 0.4786704215 0.608280219 7.829196e-01 253 0.6170779034 0.1915762192 0.7051625785 0.100203936 7.363104e-01 254 0.7798010246 0.8297415115 0.9719362056 0.293687331 5.095400e-02 255 0.8295236412 0.7012862060 0.7040839291 0.076756692 4.320663e-01 256 0.7921326044 0.6680933705 0.7468525905 0.221332673 8.397830e-01 257 0.9725491845 0.8900939021 0.9264180593 0.579828278 4.414026e-01 258 0.5389117019 0.2145371423 0.7034534132 0.916396324 8.566715e-01 259 0.7039349154 0.5264377154 0.2763565620 0.608822979 4.653691e-01 260 0.9149633690 0.8434623827 0.6857282268 0.921606234 1.788714e-01 261 0.1539534209 0.5374164968 0.4353943137 0.066600877 9.459214e-01 262 0.2444682512 0.9467447461 0.2996765305 0.993174250 4.733773e-01 263 0.3417980904 0.6108143430 0.1764438688 0.070948276 4.806769e-01 264 0.4999335201 0.1733239812 0.0905771561 0.800303387 5.200107e-01 265 0.5066742979 0.2890426961 0.5416354267 0.498188548 2.367029e-01 266 0.5838516832 0.3779571787 0.4468531371 0.298747589 1.695074e-01 267 0.8129630184 0.4197585413 0.2865401660 0.384935115 4.562674e-01 268 0.2090866042 0.0312464098 0.6281993804 0.184038194 3.677554e-01 269 0.6440592636 0.1469305602 0.7661810701 0.838563889 9.731990e-01 270 0.7281985290 0.5473523897 0.6157018538 0.700302164 6.428662e-01 271 0.6646557718 0.2829667975 0.6044241658 0.799832808 3.056976e-01 272 0.9352728250 0.7136875202 0.0289973079 0.965694083 3.239665e-01 273 0.0728821249 0.7892174108 0.1717887009 0.521993847 4.899518e-01 274 0.0346095334 0.2692810481 0.7561781360 0.926916890 6.162302e-01 275 0.9504906929 0.7526574284 0.9379664683 0.068118802 8.603263e-01 276 0.5145728933 0.4559605923 0.9176913910 0.004695723 2.866595e-01 277 0.3976553013 0.3987538943 0.2839024831 0.293565525 7.702066e-01 278 0.7009254443 0.0133173962 0.9493629304 0.233285994 4.994754e-01 279 0.6224461349 0.6137198601 0.7054065103 0.907167699 6.948691e-05 280 0.3599138320 0.9875511052 0.2561543379 0.838496193 1.224435e-01 281 0.6635836901 0.4874325164 0.8553156825 0.733857719 5.639178e-01 282 0.4764982043 0.1185725085 0.6769570536 0.422804818 2.323357e-01 283 0.0649527388 0.3997544402 0.5082662296 0.536742677 8.150507e-01 284 0.5637654911 0.5065516052 0.3812452687 0.325095090 7.836266e-01 285 0.6024410788 0.7558844115 0.9651189120 0.758352714 8.845431e-01 286 0.0865011127 0.6272108338 0.0864172361 0.372212857 3.729183e-01 287 0.7726058052 0.8873297924 0.9845032052 0.926265871 3.398011e-02 288 0.2364514670 0.1204383390 0.6467557393 0.499837749 1.033158e-01 289 0.5903890356 0.2814705893 0.6270863349 0.544416004 7.191814e-01 290 0.0362968044 0.6435941649 0.8072538176 0.628011037 1.350844e-01 291 0.4776646101 0.9162236417 0.0477408909 0.195150346 9.917569e-01 292 0.9850660686 0.8860944102 0.8053446305 0.690133943 1.764347e-01 293 0.5599008847 0.8146942554 0.7810434583 0.804373655 5.683600e-01 294 0.5014732699 0.0134419615 0.0673355807 0.265304377 6.757556e-01 295 0.1202006293 0.8631854309 0.9486900396 0.120854634 5.592875e-01 296 0.2335651731 0.1641146073 0.5350339490 0.555488213 9.398174e-01 297 0.9651232909 0.4984271645 0.2115402070 0.966395828 4.653873e-01 298 0.1128552298 0.1122542853 0.5322054287 0.839569817 5.417432e-01 299 0.8394041124 0.2560082425 0.1669750074 0.328798706 8.281256e-01 300 0.3041334234 0.7385249771 0.7241052580 0.278196611 8.285159e-02 301 0.4291426220 0.8197907398 0.0797945955 0.593694864 2.967888e-01 302 0.4523428532 0.7067194383 0.8770909235 0.743807043 8.027994e-02 303 0.7211188022 0.3782203167 0.6527704687 0.173949218 3.157459e-01 304 0.5325777873 0.2437774444 0.8282919656 0.346972036 8.068654e-01 305 0.2279141766 0.5629842561 0.9597962128 0.896484934 1.616834e-02 306 0.9273555675 0.0055806914 0.6181486198 0.898301309 4.427710e-01 307 0.0714621588 0.1762667750 0.0954093228 0.078771684 3.226275e-01 308 0.0913349085 0.1022600732 0.3445813630 0.739462374 9.278758e-01 309 0.8353813579 0.0021331101 0.6448888553 0.433755042 6.941956e-01 310 0.8717507403 0.6973458694 0.9974252689 0.954311525 3.399146e-01 311 0.3402612589 0.0207138949 0.7791891245 0.138285730 9.553810e-01 312 0.1030242851 0.6788007997 0.0091685629 0.080596239 6.100400e-01 313 0.8796044991 0.7409640790 0.5888034666 0.482930689 9.066809e-01 314 0.4184627694 0.1518161653 0.9470163304 0.640979418 7.321634e-01 315 0.4914691530 0.4347692737 0.8240045996 0.113172951 7.336182e-01 316 0.1891047948 0.9546289963 0.9326387078 0.084773930 7.528037e-01 317 0.4091451312 0.4061187478 0.4935212114 0.063884293 6.897970e-01 318 0.1740348001 0.7072475406 0.7000157465 0.471941573 4.158820e-01 319 0.2931793360 0.2669634703 0.4962130494 0.383236896 9.967678e-01 320 0.2781334203 0.2826841313 0.7971504361 0.431237619 1.332127e-01 321 0.6324098671 0.9961036481 0.7603126075 0.319680428 6.017842e-01 322 0.5705256499 0.3483372750 0.9408938496 0.273614281 7.343426e-01 323 0.1463123327 0.1304147257 0.0196428671 0.466923666 8.377781e-02 324 0.3067270615 0.0682549430 0.0426511776 0.288729959 6.353246e-01 325 0.8781642185 0.3042296777 0.4770269010 0.298304684 6.386353e-01 326 0.0776095102 0.0545013193 0.6625969552 0.251515612 3.353573e-01 327 0.4279278640 0.6352059934 0.2660763490 0.261018408 9.899702e-01 328 0.0869317167 0.5169583762 0.3695479026 0.720178066 3.620540e-01 329 0.8999204701 0.7806742371 0.7687030011 0.375773629 6.745213e-01 330 0.8751550377 0.3289561400 0.4191904066 0.427931608 1.081395e-01 331 0.0761351229 0.5991958415 0.2431351477 0.914929265 5.326279e-01 332 0.9381568427 0.4902186608 0.7542906129 0.672044334 3.062858e-01 333 0.8256581253 0.6891283139 0.8559265318 0.364524166 5.110359e-01 334 0.7706560523 0.2872826036 0.2328567246 0.504470113 7.242145e-01 335 0.6093809877 0.9180088332 0.2086923716 0.571133680 3.943475e-01 336 0.6783998506 0.8340922757 0.5525323339 0.361998960 1.303991e-01 337 0.8349843030 0.8474897714 0.5272933764 0.799968964 2.671729e-01 338 0.6991907230 0.4609518785 0.2405392446 0.923717583 4.763034e-01 339 0.8896550976 0.1483478416 0.9771987102 0.383924895 4.411620e-01 340 0.7373167558 0.2668425364 0.4646588794 0.290820904 5.789469e-01 341 0.7777801065 0.1640932390 0.6075391858 0.002127058 7.864690e-01 342 0.1142391388 0.6719365513 0.4796372068 0.594974790 4.127344e-02 343 0.9957590452 0.6051899521 0.3506976240 0.360922266 5.062548e-01 344 0.8975299238 0.8794793843 0.4141596090 0.132358324 5.256242e-02 345 0.3591199489 0.8844800305 0.0099952731 0.824255990 2.001769e-01 346 0.5356684977 0.0438097841 0.3991168796 0.057423466 3.378591e-01 347 0.8537007736 0.3376973567 0.7265061915 0.990835784 4.065400e-01 348 0.6791082718 0.3562199960 0.9257631865 0.600217237 7.820375e-01 349 0.5602437346 0.2175566691 0.5589008050 0.029843965 2.610109e-01 350 0.6625470396 0.7728067513 0.3640043263 0.444563451 8.506265e-01 351 0.0657600570 0.4441296456 0.8104015475 0.270668493 7.601029e-01 352 0.8657236823 0.7491532350 0.8000338343 0.789722052 2.227382e-01 353 0.8693265212 0.1323963902 0.0203081183 0.422943778 8.704134e-01 354 0.6427183610 0.4065263735 0.2248546493 0.780684578 2.740526e-01 355 0.5759076793 0.6806553851 0.8696183837 0.845955456 9.591285e-01 356 0.6498449650 0.2195812284 0.1431319460 0.681847408 5.779802e-01 357 0.3165733689 0.2931888984 0.3416584737 0.114969182 1.011104e-01 358 0.8982761570 0.9184429229 0.7322741628 0.443741165 4.404219e-01 359 0.4469846345 0.5782766959 0.2848459983 0.706270285 4.893691e-01 360 0.1091156455 0.1869821378 0.5436482152 0.204792706 9.771976e-01 361 0.3657996678 0.5740619248 0.3449009201 0.116321134 7.446984e-01 362 0.1806305090 0.2856565488 0.5067477161 0.821413193 3.083452e-01 363 0.1164003408 0.0459067044 0.2686422307 0.954782017 4.855251e-01 364 0.8020089313 0.7331166179 0.9361567646 0.789247320 3.798315e-01 365 0.1938385800 0.5443917396 0.7268236890 0.714485556 8.691682e-02 366 0.7173356623 0.7768663752 0.9018600665 0.963223044 2.476719e-01 367 0.5296206458 0.5590656023 0.0882430347 0.370041748 6.353881e-01 368 0.0742895349 0.3294574337 0.3487890468 0.698375692 3.642144e-01 369 0.0096567774 0.1633153663 0.2697227211 0.612484920 3.893997e-01 370 0.0007421148 0.3710664017 0.3654992618 0.858424162 9.942708e-02 371 0.0144137791 0.4277930632 0.1251683424 0.439849179 5.469326e-01 372 0.6370710037 0.0225923632 0.9081482203 0.981841748 5.412906e-01 373 0.1093050879 0.2239815341 0.9036308534 0.475337584 7.634477e-01 374 0.0706267995 0.6466684432 0.8195659919 0.827542959 7.745236e-02 375 0.7718457142 0.8100467415 0.1238228502 0.858416016 9.465533e-01 376 0.1993048138 0.7963158851 0.9567167670 0.136876087 7.876701e-01 377 0.8673933318 0.4083252754 0.5028514632 0.270459414 4.284302e-01 378 0.0327129478 0.6679480514 0.6981502497 0.241763491 5.291740e-01 379 0.1309007367 0.8956746466 0.4353174695 0.311975433 2.617224e-02 380 0.6862117657 0.0567090861 0.2643035925 0.947567889 3.024849e-01 381 0.9820180680 0.7590667398 0.4497556726 0.826217088 2.475621e-01 382 0.4473482983 0.3026340993 0.2187455115 0.525228607 8.370121e-01 383 0.7837314866 0.9404827747 0.6447940255 0.692938740 6.625183e-01 384 0.4695104088 0.3393136084 0.3539974955 0.076961612 7.125074e-01 385 0.4953724076 0.3190722512 0.9209394744 0.593576920 6.993314e-01 386 0.7110873724 0.5765278214 0.8290389117 0.975892123 4.599566e-01 387 0.2224491003 0.2692736811 0.2377511768 0.800969388 3.679071e-01 388 0.7823642467 0.8590320838 0.5696742025 0.587688962 7.797665e-01 389 0.0418753801 0.8708307620 0.1299097429 0.686934761 3.754749e-01 390 0.3174569593 0.9145249254 0.3968546884 0.463118966 2.330016e-01 391 0.4599263112 0.9685912782 0.2395614071 0.702558178 9.325322e-01 392 0.3235118547 0.7471794712 0.9446711387 0.471918447 3.287914e-01 393 0.0868029166 0.5084290067 0.2607131617 0.270282100 9.171089e-01 394 0.6068624684 0.9036543421 0.5734851684 0.183628874 2.812521e-01 395 0.8760592777 0.3600250112 0.4945351148 0.963925146 8.247189e-01 396 0.1094516823 0.6648093811 0.0002113678 0.381599879 9.678066e-01 397 0.4957438977 0.3097762633 0.6246782839 0.210444017 5.248227e-01 398 0.8213692890 0.6233094407 0.2159364189 0.406675840 4.955265e-01 399 0.0229937893 0.6928474947 0.7193192698 0.296294143 5.317764e-01 400 0.6782247606 0.7866539420 0.7101236880 0.041422781 2.519602e-01 401 0.5135679946 0.4825126145 0.2595921732 0.038293313 5.709323e-01 402 0.2084596925 0.2265437515 0.3525143140 0.768503838 4.490639e-01 403 0.4135049034 0.4232750146 0.5646171367 0.069003359 9.856659e-01 404 0.5864820946 0.5760116780 0.6705476989 0.292407878 1.860817e-01 405 0.2408297034 0.0869413330 0.7113251509 0.030035041 2.684469e-02 406 0.4996919823 0.2686212256 0.2869927129 0.443136572 5.427097e-01 407 0.6976517173 0.5914358348 0.4772423580 0.645434657 5.177249e-01 408 0.6071811004 0.0431133511 0.2221226352 0.558366798 6.757148e-01 409 0.4892499996 0.5023605570 0.1043147992 0.252860805 7.848952e-01 410 0.1347163925 0.8824594235 0.2700915858 0.396543218 5.882499e-01 411 0.4283737643 0.0430903563 0.1562245784 0.110033886 9.573125e-02 412 0.6296868890 0.8916814390 0.5608932087 0.514378582 4.608000e-01 413 0.7040708861 0.4345444480 0.4374845265 0.656432379 6.506390e-01 414 0.6781820890 0.1922134126 0.9248232390 0.437797368 3.952176e-01 415 0.0526506035 0.4131393754 0.8803833262 0.649732224 6.173803e-01 416 0.7959502731 0.9659142359 0.1647356863 0.043900094 2.414191e-01 417 0.0500476982 0.7080166151 0.8463139313 0.121874181 7.449012e-01 418 0.5258683611 0.7866339283 0.2648841939 0.825389879 6.909416e-01 419 0.1958894294 0.3899555679 0.8682446461 0.996587435 3.139904e-01 420 0.0855531532 0.7530330606 0.7656025854 0.180940423 4.797922e-01 421 0.4479133089 0.7504540395 0.8510300457 0.917122714 8.110157e-01 422 0.5358806332 0.1840613915 0.8404297752 0.695809044 4.860580e-01 423 0.8574897067 0.4740581512 0.9840828453 0.388464857 4.838642e-01 424 0.7508345970 0.6199492190 0.3873363633 0.212975476 6.846711e-01 425 0.3002946340 0.2649874613 0.4472604445 0.670382250 7.239693e-01 426 0.8132292426 0.5420388943 0.4577483626 0.621782153 8.033528e-01 427 0.4620008818 0.2777338503 0.4325356120 0.413525639 9.745306e-01 428 0.3230311878 0.6950940713 0.2689813899 0.850076854 3.209517e-01 429 0.4746141164 0.4966602982 0.8828971023 0.542695990 5.727530e-01 430 0.4600041225 0.9534178204 0.8522334506 0.302213856 8.852952e-01 431 0.8426773683 0.9747501956 0.0451706389 0.388267011 4.896859e-01 432 0.4915075402 0.4375492195 0.5891421444 0.690327780 6.521933e-01 433 0.7539070100 0.1259638229 0.5254330765 0.614419028 3.255130e-01 434 0.1207034974 0.1975698317 0.0973578235 0.084314038 8.940771e-04 435 0.6456332053 0.7307007704 0.2282598969 0.646824725 8.661351e-01 436 0.5734037792 0.5650301496 0.1870769637 0.165129308 5.357737e-01 437 0.0494311822 0.9188226117 0.7185625699 0.432994389 5.269118e-01 438 0.0277321704 0.9803931457 0.2070943683 0.710366176 2.701794e-01 439 0.0178599542 0.6922797281 0.7564428258 0.667626088 1.513596e-01 440 0.4946952646 0.4058102395 0.1119535868 0.220409061 5.919344e-01 441 0.8835002473 0.5677845615 0.1081138004 0.675165835 4.661971e-03 442 0.6331623469 0.7801744484 0.1189808135 0.414237151 1.799904e-01 443 0.5561390182 0.3673749596 0.7417061792 0.821517335 6.608645e-01 444 0.1873597139 0.9747207032 0.8465773889 0.046875914 3.714898e-01 445 0.9465421280 0.0706294128 0.0714444439 0.945401258 7.603055e-01 446 0.7546351640 0.8103325381 0.7578929288 0.073837923 3.450114e-01 447 0.8472598379 0.5312037698 0.7027316322 0.347222402 8.442074e-01 448 0.9271843736 0.4976025869 0.2601484831 0.881693521 8.947889e-01 449 0.6015616008 0.0193471697 0.5047459286 0.771554465 6.620372e-01 450 0.3465780267 0.4549790232 0.5562954093 0.799919833 6.022381e-01 451 0.4260562521 0.7672826869 0.2570241319 0.157866101 9.280257e-01 452 0.3984505145 0.4370649243 0.3836146572 0.346790810 8.788060e-01 453 0.2113784782 0.5443354140 0.3122317644 0.552217386 6.442702e-01 454 0.0376454894 0.0106201086 0.4465854045 0.577008852 9.825328e-01 455 0.4784657105 0.8741496983 0.1614553235 0.541224399 3.360020e-01 456 0.0600217197 0.5047202792 0.0530216661 0.561827486 7.207989e-01 457 0.4019439009 0.7448705870 0.6991459318 0.858834204 2.629050e-01 458 0.5859271495 0.8682614388 0.2194845770 0.433029509 8.678002e-01 459 0.7683075613 0.8662802191 0.0412932397 0.332626628 3.531620e-01 460 0.8506514311 0.7222218062 0.8751874901 0.020963814 7.421935e-02 461 0.2668082241 0.9159252078 0.5414669486 0.993659624 5.213159e-01 462 0.8532688692 0.5136732112 0.6805261644 0.969336346 1.181152e-01 463 0.3914166926 0.1625697238 0.7115389123 0.613171886 5.563475e-02 464 0.9178691409 0.5312489253 0.1411212357 0.921012539 5.166795e-01 465 0.3401386235 0.7289904719 0.1812944608 0.052570121 1.991569e-01 466 0.9952564673 0.4240395194 0.3330458766 0.654532294 4.914733e-01 467 0.2361274951 0.7220603130 0.1801518665 0.226354291 2.317657e-01 468 0.9484631482 0.1171784431 0.3814109282 0.151987331 1.259127e-01 469 0.1971958587 0.5476996582 0.8278281968 0.006280510 1.377210e-02 470 0.4303416610 0.7595192476 0.2089759717 0.666954212 4.647386e-01 471 0.3610162260 0.8105125707 0.5239570760 0.831454685 2.229034e-01 472 0.4729091411 0.9529160575 0.1585315673 0.368948445 2.876274e-01 473 0.4228429645 0.1028229753 0.1130753367 0.712432174 7.977939e-01 474 0.4466349385 0.8416559217 0.2405777008 0.997086962 1.029476e-01 475 0.7607556819 0.4972156934 0.0339827556 0.196671375 5.321757e-01 476 0.2251486822 0.0994993590 0.7663894410 0.888381128 5.743740e-02 477 0.7277364382 0.9162787397 0.1941889157 0.516441212 3.768768e-01 478 0.4330620549 0.7756441284 0.6987062974 0.021771210 8.454757e-01 479 0.2342932397 0.4582609411 0.1460496057 0.370035785 1.053588e-01 480 0.4623888060 0.4177094412 0.3688633731 0.421409196 8.330933e-03 481 0.7287827786 0.5170731510 0.5754181799 0.829369209 4.207426e-01 482 0.2487997608 0.7418034966 0.0145854305 0.140747132 9.087611e-01 483 0.1584943631 0.4967057044 0.1377874245 0.246570810 5.367151e-01 484 0.8771969816 0.3497019310 0.5086943456 0.857494939 1.820041e-01 485 0.3275236553 0.6556553524 0.8936726141 0.695424152 5.797824e-01 486 0.3876958531 0.7141255415 0.7401520242 0.730453796 8.218417e-01 487 0.0859429245 0.3932208200 0.4963257634 0.023122997 7.484599e-01 488 0.0478853509 0.5820773568 0.4347384747 0.495591412 2.514400e-01 489 0.7640398338 0.6219047029 0.9018226827 0.367291192 6.662036e-01 490 0.7738003614 0.7122801496 0.3401500941 0.813406494 6.130019e-01 491 0.2811919176 0.1084264976 0.8185484011 0.524333827 5.448712e-01 492 0.4107365129 0.4592057569 0.4585471388 0.939687159 9.684651e-01 493 0.6586276228 0.2361251698 0.3982894844 0.736938675 7.158374e-01 494 0.8373886999 0.6760577103 0.4539175981 0.140440820 1.205565e-01 495 0.3794597711 0.0379660081 0.8831326116 0.119593804 3.948079e-01 496 0.7899617921 0.3214019027 0.7558209519 0.178459910 4.474716e-01 497 0.6494789242 0.1239918764 0.7879019941 0.566567896 2.093041e-01 498 0.4682361067 0.8054523449 0.0313273268 0.917019749 3.626654e-01 499 0.8111232512 0.4842337884 0.9905550019 0.443862415 7.314548e-01 500 0.7845828277 0.6678469195 0.0099117353 0.938890793 8.898346e-01 501 0.8596741629 0.0578216626 0.1373633842 0.242686614 3.390618e-01 502 0.0336864635 0.9917070670 0.0058358226 0.576859908 7.295833e-01 503 0.0090650320 0.1259467914 0.5226027011 0.221464730 8.944133e-01 504 0.7393250617 0.4164622889 0.2562337969 0.309255973 1.536401e-01 505 0.1968866454 0.6010752646 0.8266404339 0.241861298 8.562246e-01 506 0.5490019484 0.0597066092 0.6746688485 0.988395112 2.257646e-01 507 0.6120238367 0.8842251308 0.0106372475 0.657762127 4.932302e-01 508 0.5809495263 0.6934050722 0.7337883902 0.881573875 2.639341e-01 509 0.5484295946 0.7735354542 0.8462061526 0.510266755 8.889722e-01 510 0.0912326679 0.0196348897 0.8939744835 0.996975462 5.472280e-01 511 0.0193336261 0.3792007137 0.3810819855 0.306738382 6.755923e-01 512 0.9039375873 0.6896352544 0.2266765735 0.024039615 8.621554e-01 513 0.8407693366 0.1688590143 0.8865542472 0.448400312 5.756914e-01 514 0.3070470826 0.9788418170 0.5187856420 0.449613948 2.021171e-01 515 0.7346552406 0.4564591136 0.0886887216 0.213024601 1.288321e-01 516 0.1057571096 0.1084782106 0.8899245381 0.383389150 6.462186e-01 517 0.6396863880 0.7724902784 0.1619776264 0.529304153 4.767796e-01 518 0.5444227399 0.5911405408 0.0523558804 0.344859310 3.515537e-01 519 0.5455354273 0.4614260718 0.7244580921 0.583712463 3.205922e-01 520 0.1358930182 0.7673941576 0.4223320177 0.252359887 4.387520e-01 521 0.4758632802 0.5606147940 0.3586316842 0.686842689 8.451977e-01 522 0.1343210761 0.4462387625 0.1000703084 0.198462720 1.241996e-01 523 0.1480307344 0.2055510634 0.9462425506 0.091771150 7.064920e-01 524 0.2279162470 0.5673055674 0.9074591370 0.592499505 5.908225e-01 525 0.3441012220 0.3722269435 0.7897777304 0.841536568 8.157750e-02 526 0.1684615102 0.4908088224 0.6635147301 0.656268670 1.541303e-01 527 0.3644688802 0.6197951366 0.3999979883 0.961548903 2.104624e-01 528 0.3026080551 0.0421625217 0.9051791250 0.189580926 5.986782e-01 529 0.7649763497 0.8567843242 0.4777257033 0.060635211 4.449084e-01 530 0.5515219311 0.3415239272 0.3919794145 0.821882782 8.111506e-01 531 0.6972977808 0.7972091183 0.3467263887 0.019405104 3.704000e-02 532 0.0387044246 0.2070823517 0.8566835134 0.785161634 7.065999e-01 533 0.9039842428 0.6613530181 0.4024148083 0.620648935 1.516588e-01 534 0.4088809304 0.6672650862 0.5995888044 0.338171331 4.423938e-02 535 0.5378535052 0.9996105607 0.5368517251 0.006045962 3.168881e-02 536 0.3881504608 0.0245178782 0.3426053128 0.328514404 5.110480e-01 537 0.3916057695 0.6706551816 0.8657774576 0.036358000 4.571243e-01 538 0.0028829025 0.4516399151 0.7030717239 0.455910068 4.933923e-01 539 0.4001601264 0.1733659208 0.2999020617 0.905883759 5.860176e-01 540 0.8600010681 0.4321305510 0.5080972612 0.443594804 3.532380e-01 541 0.8715729243 0.9938263360 0.5023601486 0.837488207 8.630757e-01 542 0.9454444251 0.1035464187 0.2287527740 0.099763853 9.503204e-02 543 0.2098055060 0.3725548622 0.5470067055 0.847507765 6.545298e-01 544 0.4703532252 0.9060373341 0.2752195084 0.092471990 5.282526e-01 545 0.2748672708 0.3570935251 0.8742467877 0.607690911 3.772721e-01 546 0.0433487017 0.5896492933 0.9129612078 0.755463495 4.825281e-01 547 0.9404139488 0.0481122537 0.6314002185 0.407415794 5.865745e-01 548 0.6125317849 0.3405352619 0.9933596929 0.618332138 4.827819e-01 549 0.4461862082 0.5074156527 0.0893472889 0.997433731 2.901876e-01 550 0.2108046531 0.4348503361 0.3725137783 0.732523779 9.000062e-02 551 0.4465482270 0.7043836070 0.3406603502 0.058125994 8.740000e-01 552 0.5953917825 0.2501409757 0.4145573166 0.527952925 8.065012e-01 553 0.7835872069 0.9648590535 0.6512023690 0.225035774 4.342820e-01 554 0.5583098917 0.9271848723 0.2747644160 0.034011265 3.326598e-01 555 0.0164725739 0.1276548684 0.3477022895 0.940272366 6.519734e-01 556 0.1401534586 0.9556691791 0.8838821112 0.082095703 1.772069e-01 557 0.6577758456 0.9237059227 0.7349070930 0.675827107 8.381635e-01 558 0.3120215600 0.8630618234 0.3595919712 0.646322087 5.722389e-01 559 0.2710284372 0.3668614225 0.5772195486 0.732232192 9.656908e-01 560 0.3835913802 0.8062818865 0.2430923579 0.391548166 1.499482e-03 561 0.2820639270 0.9741106499 0.0780810083 0.262342765 3.725171e-01 562 0.6476774705 0.6113039821 0.3379194401 0.283364764 2.244343e-02 563 0.8264224837 0.6389387369 0.2133443239 0.304230640 6.614637e-02 564 0.1684982514 0.5108453108 0.8730994116 0.952562007 2.711628e-01 565 0.4681478492 0.7290914275 0.6655910178 0.059033521 2.882402e-01 566 0.9135101677 0.7844511706 0.6940691071 0.521119098 6.174569e-01 567 0.5153865137 0.6280066038 0.7529852346 0.926406451 3.512761e-01 568 0.1224220723 0.1532868505 0.6064663581 0.035015481 9.069524e-02 569 0.3729660993 0.3849294826 0.4162248687 0.170865999 3.499442e-01 570 0.6174877922 0.4830182882 0.8551577611 0.948893960 3.666638e-01 571 0.7798820683 0.8074540063 0.0701762645 0.811303859 6.321102e-01 572 0.8141681084 0.4763889555 0.6402247904 0.118680977 8.215278e-01 573 0.8072200266 0.4033720838 0.5565765412 0.933369923 2.928074e-01 574 0.4192860171 0.5062515556 0.9369511518 0.923694299 6.381643e-02 575 0.3336501552 0.7181739374 0.3037285402 0.663063108 3.577619e-02 576 0.2333190406 0.6384027032 0.1179364619 0.621289150 7.826201e-02 577 0.6390773912 0.0315628725 0.0034079296 0.544522583 6.646238e-01 578 0.9833549089 0.5804860534 0.4200558870 0.108356553 9.020294e-01 579 0.2008158572 0.3473298543 0.3942718485 0.882981715 5.132987e-01 580 0.6873001435 0.5002243656 0.8098510485 0.548252749 7.759410e-01 581 0.5232526087 0.5436942442 0.7394590937 0.986454384 7.662074e-01 582 0.7243103618 0.4520305509 0.1652481027 0.533395649 2.761340e-01 583 0.5373778436 0.6290667017 0.0718271255 0.221661672 5.003754e-01 584 0.2417254543 0.3433562904 0.8498937809 0.864886062 8.246796e-02 585 0.0760574646 0.7703167831 0.6272670222 0.711507613 7.299976e-01 586 0.6699323892 0.2712414272 0.8532035290 0.287744463 9.483736e-01 587 0.5009339042 0.9915771615 0.3771723702 0.246797540 7.356113e-01 588 0.3031111252 0.6791734453 0.1358731689 0.205903129 2.677441e-01 589 0.8653934507 0.3075919577 0.9591189560 0.164791528 3.823351e-02 590 0.9512251944 0.8444743052 0.6169150190 0.151851988 2.594963e-01 591 0.2299444869 0.2699593364 0.0633073712 0.337680844 8.593229e-01 592 0.8090767509 0.3491266835 0.9062857677 0.731222640 1.136799e-01 593 0.8875121858 0.7116214407 0.3816257541 0.229801290 2.956320e-01 594 0.0956568804 0.0888715349 0.8685165911 0.571338476 9.496462e-01 595 0.9680835889 0.6196359964 0.6548132098 0.281034419 4.816263e-01 596 0.2339390421 0.1134912746 0.1338451568 0.632273507 6.795302e-02 597 0.1876290324 0.6431201152 0.6688683571 0.352891829 7.041394e-01 598 0.1103235064 0.7108283620 0.2381389178 0.177926907 5.603788e-01 599 0.9005565967 0.7975010527 0.6413422818 0.317935640 1.315685e-01 600 0.0959906711 0.2517449632 0.6953449131 0.886165070 3.520248e-01 601 0.1814462831 0.2464492123 0.2428896630 0.629397307 3.648666e-01 602 0.7031895248 0.8790988030 0.5745146840 0.192540054 8.839200e-02 603 0.5910929132 0.8323000772 0.2072549914 0.794825767 7.403007e-01 604 0.9162794969 0.4879544913 0.5999331572 0.553021319 6.472658e-02 605 0.9888230127 0.2273209461 0.1320640503 0.583122615 2.292195e-01 606 0.4466462396 0.7951611818 0.7163238716 0.111165432 6.223114e-01 607 0.4109430443 0.4526698415 0.6658871539 0.055975484 5.476906e-01 608 0.1328400464 0.7751484194 0.8014833727 0.026586966 6.009309e-01 609 0.7664368560 0.6650214796 0.8394974649 0.796361954 5.203079e-01 610 0.1854454621 0.1539397850 0.5727987136 0.859107810 9.724846e-01 611 0.7509588073 0.2780542122 0.1968538989 0.138429189 7.375585e-02 612 0.9074370672 0.3251694136 0.1056423029 0.376905116 9.260506e-01 613 0.1178869291 0.1247338115 0.9974537201 0.091709557 3.123266e-01 614 0.0469024812 0.6889863366 0.6949616608 0.587234502 1.082442e-01 615 0.1484412735 0.5968929187 0.1998363563 0.185659013 6.482614e-01 616 0.8210094008 0.0725176199 0.6460069709 0.372610050 3.817773e-01 617 0.5549458107 0.7114344193 0.7727056891 0.842564460 4.825940e-01 618 0.3284571527 0.1717800761 0.9150849401 0.928022968 2.964681e-01 619 0.5617234516 0.4060857871 0.5447506779 0.991730924 4.116851e-01 620 0.2218145537 0.6428353186 0.4462690975 0.180419530 9.495265e-01 621 0.6815757749 0.4869625089 0.7706935983 0.124916652 2.543316e-01 622 0.3976176258 0.4112871403 0.2232044286 0.601970080 5.856749e-01 623 0.9302335104 0.7058372160 0.2738621628 0.357624098 4.464640e-01 624 0.1203466251 0.6778112825 0.8039241112 0.283917557 5.086606e-01 625 0.5158565340 0.8589938758 0.7603616987 0.757245618 5.810929e-01 626 0.5419992688 0.1409501764 0.5972314011 0.378633605 1.498809e-01 627 0.4644107393 0.2047461346 0.4112030938 0.081525275 3.996346e-01 628 0.8369497349 0.7150264331 0.6943341750 0.449338492 4.065572e-01 629 0.1867417134 0.2349077109 0.5118668289 0.008123271 7.465035e-01 630 0.5425374825 0.7320022769 0.7225689276 0.056100357 9.743324e-01 631 0.9241157686 0.4882950098 0.5148334769 0.294380933 2.235191e-01 632 0.7712359671 0.9125933389 0.1133061245 0.017098532 7.013883e-01 633 0.0460593526 0.9790674066 0.5507777117 0.216266027 9.388441e-02 634 0.1461966210 0.3678647901 0.9606390365 0.256912482 9.466607e-01 635 0.8583672526 0.4514849845 0.5924144168 0.275959739 8.218458e-01 636 0.3796334988 0.5228201388 0.8796267982 0.914260088 8.577997e-02 637 0.9610019554 0.2582042993 0.2585091207 0.248356227 9.530522e-01 638 0.2226643402 0.5756969464 0.4010980607 0.370071233 7.605457e-01 639 0.8878771043 0.4456070380 0.1770016337 0.079971947 7.741485e-01 640 0.2048562940 0.9132214689 0.5876504485 0.911009921 4.087940e-01 641 0.1734649974 0.4730995838 0.8521742683 0.283892886 7.160428e-01 642 0.2770436495 0.4960478283 0.6533297694 0.337937349 3.102812e-01 643 0.9079919949 0.3234175995 0.1345435700 0.276223309 3.642992e-01 644 0.1404491214 0.3107680771 0.2690007351 0.785765260 8.150575e-01 645 0.7797426870 0.5068352411 0.9562994044 0.341920928 9.123921e-01 646 0.2014403057 0.3889138778 0.6121131913 0.920091983 7.444889e-01 647 0.6695721492 0.0883990279 0.2482306608 0.965957357 2.299187e-01 648 0.8657230681 0.7457466202 0.6249744541 0.324577653 3.874338e-01 649 0.5862377898 0.1959867699 0.2212410027 0.563064340 6.686219e-01 650 0.4096797309 0.1860773508 0.5328029690 0.314105657 7.770745e-01 651 0.0554621299 0.0790727723 0.3894717460 0.840018364 6.212918e-02 652 0.6932099925 0.3149603689 0.0017989841 0.561205384 5.865121e-01 653 0.0772926114 0.1772280850 0.9093107912 0.192656148 8.233684e-01 654 0.5872192709 0.9547599677 0.2526479778 0.381830367 4.776642e-01 655 0.2018798466 0.0876697104 0.1663716871 0.600859793 9.508464e-01 656 0.5674000036 0.0061985739 0.9192362998 0.882563850 8.127025e-01 657 0.2606946430 0.3112595235 0.6329974558 0.790751529 6.720512e-01 658 0.6728667193 0.1917230377 0.7566862372 0.909346010 4.726861e-01 659 0.6331451386 0.2484863934 0.0995883788 0.343220247 3.827711e-01 660 0.0108745436 0.6954743220 0.4698274387 0.426440760 6.104461e-01 661 0.2674112793 0.0800102965 0.2643972533 0.478243147 5.113606e-01 662 0.8316930223 0.2490380928 0.2804101042 0.903230454 1.032151e-01 663 0.1727495792 0.7230207766 0.5985144922 0.054255187 5.225881e-01 664 0.0849175956 0.5175191101 0.3752188238 0.669995113 5.585850e-01 665 0.0127395198 0.8534449597 0.3917715163 0.141239679 7.482148e-01 666 0.9716106884 0.9878049642 0.5612602045 0.210602174 7.658354e-01 667 0.8348956225 0.8170615705 0.3934594379 0.369965748 6.439354e-01 668 0.6081504258 0.4918447125 0.8814690269 0.436927830 8.248758e-01 669 0.9960278347 0.6785293575 0.2428395639 0.235090057 2.776699e-01 670 0.8508732293 0.2890700537 0.9828776065 0.441170447 6.066821e-01 671 0.0809824888 0.1729177088 0.0553947210 0.410132100 6.038980e-01 672 0.5399313546 0.3313864928 0.6883659216 0.780303753 2.586869e-01 673 0.6266353892 0.1214897714 0.1065400506 0.279638008 9.539392e-01 674 0.5492108434 0.1729518115 0.0980195184 0.091090638 9.093982e-01 675 0.2353585865 0.3663545211 0.2753355789 0.370521680 9.157402e-02 676 0.9186354124 0.1570131038 0.7628696139 0.732964899 9.533825e-01 677 0.7215894382 0.3251391754 0.7349442197 0.390790382 8.812004e-01 678 0.4072342522 0.5741532256 0.2547490085 0.101868185 8.243583e-01 679 0.0320119020 0.6594430758 0.6453604414 0.593536775 8.153255e-01 680 0.1372886975 0.5797297091 0.0693826037 0.581811009 8.454477e-01 681 0.2251470848 0.5863237504 0.2929798686 0.064685504 2.603856e-01 682 0.2438405859 0.7252879883 0.2253684865 0.775818338 3.057060e-01 683 0.0316104067 0.4859786753 0.8827559745 0.927763019 8.996976e-01 684 0.8333529353 0.8842396680 0.4077094197 0.951228141 4.519069e-01 685 0.9327697556 0.0735161284 0.8669178819 0.646814072 2.359719e-01 686 0.8194053553 0.0149510549 0.3264283007 0.027954239 9.877356e-01 687 0.4739673156 0.6126506121 0.2575328399 0.626472661 9.376579e-01 688 0.7064988560 0.4339322769 0.0102567382 0.463410470 1.281101e-01 689 0.3043627338 0.3500925838 0.7521409774 0.416593156 6.536726e-01 690 0.2842362507 0.1287338857 0.2068039351 0.479221793 7.736288e-01 691 0.3959755872 0.1325092788 0.0840746027 0.595622467 6.474641e-01 692 0.4561319870 0.7985444975 0.2368864699 0.953477525 6.616659e-01 693 0.0085370028 0.1395568491 0.1916677223 0.754163359 7.051648e-01 694 0.3519615321 0.7734501422 0.3515878150 0.644991686 7.748586e-01 695 0.1480507238 0.6036012371 0.1661285183 0.666506897 1.195510e-01 696 0.4659092973 0.5883147453 0.9563295639 0.460264088 9.507760e-03 697 0.2261838936 0.2115620363 0.1406688350 0.502033343 4.627967e-01 698 0.0966137387 0.7810345865 0.1228184656 0.399053896 6.973598e-02 699 0.0595416413 0.9259417355 0.4779730642 0.556131563 7.944641e-01 700 0.2826902184 0.2328596772 0.3735294535 0.980496759 4.502118e-01 701 0.6457913155 0.9648015301 0.0866789357 0.340338276 3.152243e-01 702 0.8387226122 0.9889767615 0.5429523443 0.467376266 9.904297e-02 703 0.3480575341 0.5784635388 0.8746951353 0.090592088 3.027450e-01 704 0.0110021131 0.7382813357 0.8855151345 0.097531161 6.932548e-01 705 0.4369894997 0.4602281952 0.1504359660 0.592339120 2.363977e-01 706 0.3866372849 0.9427196847 0.8209371674 0.750375597 1.012006e-02 707 0.2527379994 0.2612617402 0.4743072900 0.701019078 1.618566e-01 708 0.0489493115 0.5330059354 0.2273620877 0.057604318 6.559704e-01 709 0.1701454427 0.5906300214 0.1530095886 0.885532326 9.451794e-02 710 0.0542228029 0.4391671510 0.0454088813 0.130954660 3.661530e-01 711 0.9801739862 0.9737242053 0.3088543373 0.292188345 2.974664e-01 712 0.0955795809 0.7315077246 0.0739612088 0.306935967 8.830078e-01 713 0.4907403311 0.3944720011 0.7725880863 0.986870253 5.451626e-01 714 0.6269930771 0.0671702924 0.6396098298 0.257390226 4.399738e-01 715 0.8188843785 0.8974256085 0.2105948704 0.497911549 8.912714e-01 716 0.1428737247 0.7066940332 0.5905662815 0.013904960 8.051229e-01 717 0.3795925688 0.5371038395 0.6400907186 0.396177840 2.185821e-01 718 0.2576724272 0.2867634567 0.5625600184 0.227866938 6.552381e-01 719 0.0705480152 0.7199489921 0.4384352528 0.213859265 5.562379e-01 720 0.5952966632 0.1331510691 0.3311057268 0.812910554 4.576846e-01 721 0.2078502625 0.2657186741 0.1369717133 0.966520458 6.201260e-01 722 0.5607043237 0.7773463074 0.9060784536 0.136882777 5.718761e-01 723 0.1418856278 0.3193872187 0.0067140581 0.402681939 2.011022e-01 724 0.1668371137 0.6748321333 0.7357757199 0.286822234 6.788829e-01 725 0.6645078519 0.4957872450 0.6253226784 0.045322990 4.713916e-01 726 0.2852513075 0.0419426917 0.4170354577 0.144804272 8.927030e-02 727 0.5211206493 0.6694120474 0.2225864746 0.496572894 8.498413e-01 728 0.1769501129 0.6802692772 0.5119223034 0.810593840 3.094975e-01 729 0.6272501328 0.1008226273 0.0221058365 0.660919987 7.107244e-01 730 0.4807866269 0.4724463990 0.8551723175 0.946134662 5.702034e-01 731 0.0405958679 0.6419971804 0.8001623955 0.059512498 5.834628e-01 732 0.1040727708 0.4574132226 0.8362864284 0.119848038 9.500916e-01 733 0.1034192743 0.5646959795 0.4774509370 0.100162905 4.218027e-01 734 0.9854815747 0.9536970994 0.5731427802 0.445599995 9.751907e-01 735 0.5504544294 0.8856823838 0.5006169302 0.326138788 2.829155e-01 736 0.1668087766 0.0087919568 0.2469461611 0.124296868 9.120047e-01 737 0.6289759041 0.3003166930 0.0746609843 0.277244968 7.745181e-01 738 0.8146107760 0.7891051264 0.7982830841 0.843328346 1.065173e-01 739 0.7342906827 0.5856943072 0.3800514466 0.651758524 6.560058e-01 740 0.1165104364 0.4262742412 0.9380351442 0.720093509 5.285259e-02 741 0.4589043292 0.9510245435 0.1878450911 0.010018110 3.818362e-01 742 0.8540700742 0.4387043286 0.8535325744 0.704237403 1.438948e-01 743 0.9853149375 0.3135367928 0.0273535808 0.401024840 8.124945e-01 744 0.2686594662 0.0067920866 0.4991884257 0.925610454 1.121184e-01 745 0.2199555885 0.4688793917 0.0312401301 0.890368610 3.474478e-01 746 0.6150329802 0.3032249976 0.0330985945 0.160531911 5.085447e-01 747 0.5294507435 0.1677674388 0.2016299928 0.780310904 9.195844e-01 748 0.3173471964 0.3425450716 0.3296101303 0.231268557 1.281241e-01 749 0.7328500336 0.8980467783 0.3283071520 0.323327990 5.388319e-03 750 0.8194361259 0.9905332893 0.5591681972 0.266169388 4.770701e-01 751 0.6656111178 0.2108571122 0.1919852202 0.527454810 3.347841e-01 752 0.0899907222 0.1446414352 0.8986281594 0.806791155 9.988664e-01 753 0.4024253956 0.8001327894 0.4767216074 0.267760365 7.607286e-01 754 0.1910845076 0.3644843656 0.7766964245 0.061330716 9.895875e-01 755 0.5832469633 0.1375580644 0.6565976313 0.690765419 7.303636e-01 756 0.8330274366 0.1368968731 0.7017091217 0.198802563 6.753607e-01 757 0.1212478145 0.8072033660 0.3884129189 0.510344740 9.609224e-01 758 0.6087156688 0.3369593923 0.7562846150 0.510526375 5.928940e-02 759 0.7821644433 0.8510950310 0.9395253323 0.166566865 5.062242e-01 760 0.7225771444 0.0084626777 0.2886167157 0.375914235 1.782048e-01 761 0.4327145740 0.3436067994 0.0624468897 0.655956657 4.799398e-01 762 0.3311250603 0.5018027748 0.3175616367 0.909446071 3.785639e-01 763 0.5488906924 0.9179980736 0.5937463716 0.824942662 4.687987e-01 764 0.1809079153 0.1684568480 0.4394692946 0.091948393 1.571436e-01 765 0.3125010028 0.2808426481 0.4364514167 0.871625515 5.357669e-01 766 0.2322662107 0.4296523267 0.2301211185 0.984538529 5.416512e-01 767 0.8883634240 0.5374647570 0.0706931108 0.798453904 4.194775e-01 768 0.5146165742 0.2363249704 0.6942034576 0.255742336 3.743201e-01 769 0.3510731300 0.2868859540 0.2493360748 0.406186156 2.858131e-01 770 0.5866833304 0.5382801190 0.3648477907 0.102987665 7.186622e-01 771 0.5168380113 0.2537941472 0.5216529446 0.386116420 3.009853e-01 772 0.0169250246 0.6376643232 0.5035872550 0.192309255 4.536078e-01 773 0.9882035200 0.4582637991 0.6574109502 0.724014118 4.872410e-01 774 0.5653496098 0.4760491254 0.2371778297 0.042768712 9.784826e-01 775 0.5821441454 0.3042122070 0.0609040807 0.390714176 2.524162e-01 776 0.0911039554 0.8299246619 0.1350643106 0.867280200 1.776634e-01 777 0.1199192528 0.5218563776 0.5861633071 0.499435835 7.811131e-02 778 0.1269978655 0.3822219400 0.3625663975 0.180269022 7.917490e-02 779 0.5197857721 0.0951860771 0.5939297057 0.487396815 3.944825e-01 780 0.7007307441 0.3687405463 0.2572553901 0.813576587 4.809790e-01 781 0.3218411119 0.7246845297 0.7287631440 0.905135344 9.021088e-02 782 0.1232672483 0.4581358170 0.7231167776 0.155507295 4.767607e-01 783 0.8657550549 0.7917757533 0.2241408930 0.840674793 2.868034e-01 784 0.5233116525 0.3896034497 0.7217376744 0.472973718 1.765475e-01 785 0.0661836988 0.1935713969 0.0214482821 0.087035662 9.176547e-01 786 0.0389345712 0.2243677888 0.5614796390 0.309720548 2.056354e-02 787 0.0492720462 0.1595348492 0.8530885784 0.178120089 8.737463e-01 788 0.3390803235 0.2362157856 0.4276224766 0.455062811 7.755917e-02 789 0.3281026052 0.4601694574 0.0599775666 0.718546729 3.753534e-01 790 0.6873349790 0.4976751781 0.8280542898 0.994703966 2.440879e-01 791 0.5504687894 0.2988064871 0.7713529249 0.641040120 5.693950e-02 792 0.5503732006 0.5960831509 0.8664672766 0.476776771 4.819354e-01 793 0.3711241737 0.6343850996 0.8537406514 0.109487688 6.908167e-01 794 0.5504067924 0.1058719386 0.6400646684 0.081498371 8.541506e-01 795 0.6950991168 0.7604977151 0.5514039169 0.081682724 4.830404e-01 796 0.3357875820 0.1431916794 0.0349251456 0.263697140 5.391837e-01 797 0.2259819009 0.7031412621 0.2148530942 0.511179863 9.868655e-01 798 0.6837921543 0.2173088472 0.6941937325 0.415529779 2.726136e-01 799 0.0327904762 0.2515361288 0.3132338880 0.383568716 6.489722e-01 800 0.4708686613 0.9216939155 0.2693657901 0.127065991 2.182584e-01 801 0.6667835233 0.6971665192 0.1054696967 0.693743361 1.188034e-01 802 0.8530509556 0.9432144242 0.0041820426 0.169260422 3.253670e-01 803 0.3427545812 0.1718738738 0.7625045318 0.362998036 7.145535e-01 804 0.7191447199 0.8004429014 0.9849620434 0.050722711 4.945911e-01 805 0.9965520247 0.4413624234 0.0623347035 0.432994378 2.793668e-01 806 0.9527486961 0.5245378804 0.8925296222 0.037442494 9.853151e-01 807 0.8085598280 0.7501264329 0.1495426074 0.275200297 7.410981e-01 808 0.6196632015 0.7699690936 0.7003352877 0.750953684 6.796842e-01 809 0.1371374123 0.2860390744 0.4732657552 0.337846551 6.279401e-01 810 0.2705621868 0.1728939763 0.3674579775 0.646730392 2.963514e-01 811 0.9150904114 0.7387685184 0.4425446887 0.717890505 7.044907e-01 812 0.9508713693 0.2058457162 0.3327484170 0.048251316 8.233207e-02 813 0.7664615572 0.2933199112 0.2387260119 0.455275297 9.507067e-01 814 0.7263937038 0.5308477988 0.7668302804 0.151941190 2.872785e-01 815 0.3706638364 0.0815223935 0.6523769449 0.861204863 9.701118e-01 816 0.3731770006 0.1418519248 0.6558721811 0.392759632 8.911583e-01 817 0.8660431546 0.0405283556 0.2191681382 0.671238937 4.604006e-01 818 0.5390274378 0.1355421511 0.7730570342 0.392236311 4.018251e-01 819 0.8056316061 0.4232798924 0.8585525232 0.273011497 2.771170e-01 820 0.4778904547 0.8586307105 0.3877773187 0.301605025 2.120700e-01 821 0.3268022928 0.9401343844 0.7134420641 0.820058862 6.129209e-01 822 0.4336912178 0.6601184118 0.7966466683 0.753500970 8.045000e-01 823 0.4331766844 0.4773011776 0.2554806194 0.506299197 7.484734e-01 824 0.8422790130 0.9728820582 0.5530510051 0.674755441 3.098467e-01 825 0.1702686194 0.5197307284 0.1117506877 0.576723189 6.939165e-01 826 0.8912892947 0.8173142259 0.2254784491 0.990182979 8.775264e-01 827 0.5795565543 0.3450774932 0.6231800388 0.511314409 7.229131e-01 828 0.2747776266 0.3432068359 0.9651196841 0.361325060 9.342489e-01 829 0.2476471073 0.9399656593 0.7509542222 0.040544073 5.951181e-01 830 0.0888829241 0.2625810562 0.9616506596 0.164866655 5.282961e-01 831 0.0618546458 0.2170708573 0.6758510496 0.806005144 5.418574e-01 832 0.2215657665 0.5892768123 0.9687829807 0.970458879 1.469074e-01 833 0.0160984755 0.6008945550 0.2456119955 0.651282727 7.154888e-01 834 0.9552937921 0.4041952416 0.2487599570 0.636312764 4.222105e-01 835 0.6553695966 0.5982882574 0.4690349526 0.850788158 9.853207e-01 836 0.5880596407 0.1025756893 0.3663468168 0.602122332 4.146614e-01 837 0.2446371287 0.3886715963 0.7592353383 0.304655447 4.547841e-01 838 0.2204478385 0.9776127548 0.8441695466 0.342979762 1.793916e-01 839 0.3436731452 0.3158153961 0.8450364436 0.017197104 5.342961e-02 840 0.9654810687 0.6897880316 0.2408264768 0.725365726 6.414571e-01 841 0.7806982426 0.6866874089 0.1406750046 0.415963592 4.494836e-01 842 0.6681353534 0.9749579220 0.5696881823 0.270559045 9.334318e-01 843 0.6673664290 0.8776515799 0.1567756762 0.577515450 6.526554e-02 844 0.8577287272 0.8350734247 0.8539518388 0.599658105 3.774422e-01 845 0.8676475156 0.2449205024 0.7057895265 0.336051099 2.469455e-01 846 0.9010199986 0.7300883937 0.9259289976 0.107742261 7.782273e-01 847 0.7096090924 0.0396126274 0.1711437271 0.912411376 5.841550e-01 848 0.4057656860 0.9635527530 0.4667839229 0.717135843 5.471613e-01 849 0.7586153776 0.8188142958 0.7115409470 0.265018949 4.173710e-01 850 0.1514678800 0.4931847306 0.1066040788 0.420480186 2.265498e-01 851 0.8852276236 0.2273079150 0.6126720684 0.124443571 5.439975e-01 852 0.5744567150 0.2276047985 0.6053443069 0.305235157 1.352699e-01 853 0.7365684998 0.7416466051 0.1987565625 0.522348852 1.889310e-01 854 0.4682052871 0.3656761451 0.5374135601 0.834061932 4.192377e-01 855 0.6883175839 0.1932561044 0.1337969434 0.655146906 6.233718e-01 856 0.7630665472 0.1755269326 0.8618537828 0.065738074 4.840831e-01 857 0.9105806397 0.0796877367 0.0654560952 0.970875686 6.588444e-01 858 0.1453003632 0.8365933388 0.8908922726 0.261465481 1.285208e-01 859 0.3542107171 0.6549203501 0.8706293327 0.595211849 6.201236e-01 860 0.8858120129 0.6786385714 0.0019265474 0.633881345 6.488960e-01 861 0.3534604481 0.4462263004 0.3821332473 0.116964221 4.485467e-01 862 0.9199138735 0.0237773440 0.5560805255 0.789658329 7.332141e-01 863 0.9699242383 0.5838220422 0.0731530387 0.854077701 1.952940e-01 864 0.7197193392 0.7666043034 0.6545700596 0.109955078 9.903314e-03 865 0.0818659512 0.6813366392 0.4428950842 0.709426838 7.134775e-01 866 0.6979543704 0.4409124681 0.2596999193 0.435666425 7.401312e-01 867 0.7083249867 0.4796856840 0.3304600932 0.663771980 9.346799e-01 868 0.4236047838 0.7879075778 0.0797441986 0.362275834 1.116769e-01 869 0.9025010683 0.0370089954 0.0607495804 0.455845351 7.383802e-01 870 0.8385839090 0.8592968574 0.3819316330 0.732232572 6.211616e-02 871 0.1577102917 0.4951161523 0.2630529224 0.717813409 3.060167e-01 872 0.9411812874 0.2867955500 0.2457986970 0.908677576 3.361926e-01 873 0.3959288662 0.2671015316 0.0694725793 0.657614531 2.711497e-01 874 0.1749171666 0.1481003091 0.5796920711 0.745290652 5.726620e-01 875 0.9484079392 0.7067262111 0.3593763858 0.696661414 6.557866e-01 876 0.6690065281 0.8686357243 0.9274752026 0.429747088 9.805910e-01 877 0.9932561866 0.6987060872 0.7170277392 0.572124985 7.858925e-01 878 0.4792634952 0.2627820971 0.7591925235 0.129092884 3.584519e-01 879 0.3339846164 0.4879570440 0.5219738027 0.981418908 4.780838e-01 880 0.1384664588 0.6071906351 0.5598796897 0.989587379 6.047264e-01 881 0.7710127225 0.6588871563 0.7000522728 0.787040020 8.951891e-01 882 0.5183066048 0.6633623212 0.8556959080 0.823626497 9.873870e-01 883 0.2342772360 0.6825878853 0.9765392048 0.422077550 8.276469e-01 884 0.3819961718 0.0703841767 0.4545353968 0.190806217 4.993768e-01 885 0.9671158306 0.7996307290 0.2528239412 0.821535721 4.520912e-02 886 0.0136003532 0.6550489797 0.0971307924 0.569252931 4.415581e-01 887 0.6918485614 0.2938269705 0.3830597901 0.642045519 9.181501e-01 888 0.3883587052 0.5746545340 0.2149518812 0.544876507 9.811128e-01 889 0.9782044799 0.0871506219 0.1721066297 0.638855571 2.163524e-01 890 0.3884682686 0.8530711008 0.1081484584 0.618824126 1.981257e-02 891 0.1955370163 0.7554930146 0.4383497564 0.132186846 6.693339e-01 892 0.2558058237 0.8910853162 0.6054714422 0.106588792 8.727763e-01 893 0.8466363044 0.8384201061 0.7106585323 0.261846789 3.145544e-01 894 0.9117230456 0.7739696172 0.4066036423 0.836243367 9.512813e-01 895 0.6891804636 0.3827441628 0.1441087264 0.428895871 5.805177e-01 896 0.8616765880 0.7499236313 0.2040350866 0.042978100 2.002000e-01 897 0.9397967570 0.0891257052 0.7237532646 0.214750154 9.383528e-02 898 0.2836855208 0.5146681564 0.9748851419 0.309161969 5.718429e-01 899 0.7020970217 0.3102047946 0.6902426740 0.594432420 1.961699e-01 900 0.7543787274 0.9368917353 0.2379066127 0.744932082 6.898677e-01 901 0.7100764371 0.6819450008 0.5354397860 0.666972563 2.406382e-01 902 0.0359942482 0.0883137148 0.1095670776 0.368549353 8.317809e-01 903 0.7027321772 0.3958795723 0.8308744195 0.667122809 2.450692e-01 904 0.1076769722 0.0905030516 0.4746082048 0.579430669 3.238268e-01 905 0.5423400379 0.2575905705 0.7975925563 0.839304272 3.700970e-01 906 0.0559626976 0.7763351945 0.5180712640 0.131845527 3.509410e-01 907 0.7400617898 0.5512401585 0.2532550723 0.822768779 8.241569e-01 908 0.4324181397 0.7704832107 0.2057194079 0.622663645 8.665828e-01 909 0.6370878578 0.7114341015 0.0513802432 0.275731774 3.926777e-01 910 0.5504355514 0.8300711268 0.7932873622 0.971701424 4.464620e-01 911 0.7207374696 0.9681204916 0.1152818713 0.205870625 2.824269e-01 912 0.5694926230 0.1999330653 0.9196312737 0.321336865 1.276558e-01 913 0.4082449640 0.1262337428 0.7942077753 0.763436640 5.887029e-01 914 0.4719120276 0.9897765913 0.5567615679 0.235130594 4.128340e-01 915 0.7132404083 0.5091732969 0.2665097436 0.872430083 9.947829e-02 916 0.1123925871 0.9880523295 0.7234942676 0.710679841 9.196550e-01 917 0.2404889199 0.3670683077 0.8855743406 0.572891420 9.907376e-01 918 0.1648135930 0.7147001112 0.2808103329 0.826797771 8.433905e-01 919 0.8316822357 0.3342582213 0.9897506847 0.129931385 7.228052e-01 920 0.1540134929 0.4937861396 0.6946977631 0.642777493 9.363438e-01 921 0.4543502852 0.8231381234 0.5084731737 0.392673834 9.929688e-01 922 0.3393653259 0.3207413722 0.2487738428 0.036524708 1.525176e-01 923 0.3792336930 0.4274902237 0.3213149323 0.111199606 9.098137e-01 924 0.5646767376 0.5653868199 0.1198367970 0.422641320 3.241015e-01 925 0.1275393234 0.9610971808 0.8526588532 0.547694317 1.969815e-01 926 0.8284457396 0.0512343606 0.0969893662 0.565572405 6.216574e-01 927 0.7716819895 0.4303607254 0.1500063352 0.585179174 4.304587e-01 928 0.3144028510 0.9650541637 0.2011498599 0.832636225 7.382756e-01 929 0.1932181078 0.6512502013 0.2314884884 0.927042429 4.021827e-02 930 0.7715724281 0.4867733018 0.1722336062 0.592240821 4.878809e-01 931 0.7515096315 0.5237351716 0.9337820550 0.175378338 5.437635e-01 932 0.8508526227 0.0862903530 0.7518694196 0.392386721 5.070725e-01 933 0.1897171729 0.1816355581 0.4939800212 0.508380143 2.517736e-01 934 0.5888304978 0.4325272697 0.0950893415 0.437182602 7.073556e-01 935 0.2581488835 0.4775416015 0.1880199569 0.091574369 9.642912e-01 936 0.6145524746 0.8077542640 0.8791012922 0.404682925 2.885075e-01 937 0.5502511666 0.2100446864 0.5053161618 0.570401419 7.775891e-01 938 0.1422360775 0.3714883709 0.1705700466 0.292427428 5.072701e-01 939 0.8544583651 0.3779014812 0.7818198670 0.209379298 4.591110e-01 940 0.2279679456 0.1332134781 0.7430715247 0.263717869 9.951537e-01 941 0.4878764427 0.0266515280 0.0557566388 0.827095761 6.739393e-01 942 0.8901376666 0.2336050419 0.1721143238 0.955066324 7.380528e-01 943 0.1214600294 0.4506204431 0.7488469749 0.722901567 9.061563e-01 944 0.7492403670 0.2964121955 0.4126732238 0.134044858 4.084631e-01 945 0.6883822854 0.3348744807 0.7348977642 0.435535827 3.775312e-01 946 0.9731870238 0.6234922898 0.6055679652 0.011814762 8.236566e-01 947 0.9803779942 0.0189021132 0.1514285270 0.779741396 7.189808e-02 948 0.5742041636 0.4015159667 0.2583783229 0.319161324 3.810088e-02 949 0.9520442467 0.3840122933 0.3215753071 0.721180730 8.162514e-01 950 0.4607166387 0.5502329234 0.0424744408 0.470151842 4.324335e-01 951 0.7038704425 0.7253101626 0.1991410654 0.663179845 7.651094e-01 952 0.2755301245 0.1031086305 0.9857126495 0.406166487 7.041206e-01 953 0.6193735383 0.6403544305 0.4304613241 0.992695211 5.010757e-01 954 0.4249681248 0.5705294732 0.2173429558 0.736243507 2.958308e-01 955 0.3728593271 0.9821952912 0.7585317735 0.592364626 4.824808e-01 956 0.7996276352 0.7557521961 0.6896903608 0.032121529 9.333821e-01 957 0.2530086664 0.7840213208 0.9472654869 0.663430023 7.808786e-03 958 0.0272058961 0.2785600286 0.3803765716 0.835774167 8.285646e-01 959 0.2937615011 0.0414462413 0.0293130069 0.170471911 5.053126e-01 960 0.7656114011 0.0089509301 0.2224720593 0.078633697 2.267945e-01 961 0.3921685980 0.2418958903 0.9058356252 0.499363041 4.301341e-01 962 0.3203731477 0.4558730088 0.9356109097 0.637428897 4.638048e-01 963 0.5701269405 0.6541033043 0.4553435929 0.659193589 7.637788e-01 964 0.0837780091 0.9852263576 0.2699921329 0.889402745 7.838773e-01 965 0.7902619520 0.1120501796 0.0643302598 0.015220200 7.573140e-01 966 0.7188133628 0.7126189214 0.5146951526 0.265695591 2.805009e-01 967 0.8039738089 0.0866877660 0.3811699026 0.007797114 9.017973e-01 968 0.6556652877 0.4472971286 0.0009898420 0.250218173 8.946227e-01 969 0.6478033902 0.0806381418 0.8741640016 0.686927262 3.710893e-01 970 0.6908786618 0.6214244557 0.5556211434 0.115327901 3.217848e-01 971 0.6140362916 0.7585889928 0.4772790568 0.943758306 5.007378e-01 972 0.9190024359 0.9279428197 0.9869498806 0.354942342 7.683242e-01 973 0.1376092818 0.5861151936 0.9461021069 0.957316337 1.416249e-01 974 0.4968745462 0.1127922863 0.0896480761 0.140535582 3.406880e-01 975 0.5084140948 0.6140513800 0.0141397875 0.618412951 2.553456e-01 976 0.2373713481 0.5146937706 0.3362690192 0.534919100 2.279837e-01 977 0.5096209319 0.5837802538 0.4960927456 0.120788795 3.330714e-02 978 0.1317708287 0.6946347281 0.8838458196 0.551703196 8.626118e-01 979 0.4485086391 0.4274698249 0.3073356152 0.731657210 1.683407e-01 980 0.6195011809 0.9784419455 0.3550067239 0.086869737 8.103572e-01 981 0.4437196946 0.3406247022 0.4528926273 0.325831012 9.969143e-01 982 0.0834825563 0.0264296336 0.8081067060 0.268322631 2.651312e-01 983 0.7067420189 0.3201212729 0.8177060552 0.896760443 6.064301e-01 984 0.3496549178 0.1091470283 0.1311323550 0.889285605 8.667603e-01 985 0.9456903765 0.3948074104 0.0551960305 0.270541456 1.841105e-01 986 0.9264556284 0.0210456059 0.7544529487 0.329619684 1.666222e-01 987 0.5862394304 0.6943669352 0.3462126998 0.527938245 4.539635e-01 988 0.9264138332 0.9935972001 0.4456170811 0.276350456 1.458880e-01 989 0.7399739220 0.6072702506 0.5072083352 0.379948666 2.127185e-01 990 0.0597490836 0.7014014195 0.8693522580 0.502023081 1.920523e-01 991 0.2525925478 0.4642360706 0.5797812503 0.206372628 2.255902e-01 992 0.5294484263 0.5832591089 0.9141128107 0.763919299 4.386716e-01 993 0.1451663070 0.9450887532 0.3267974902 0.163087269 4.304320e-01 994 0.5973799566 0.6611484769 0.2045684843 0.530934318 9.091520e-01 995 0.4801052473 0.1244018658 0.6161254332 0.985765266 4.358157e-01 996 0.8676580756 0.6908842688 0.2463870917 0.558830278 8.897796e-01 997 0.8354824937 0.7563201888 0.4218094908 0.991251539 1.910084e-01 998 0.5086493925 0.5916594807 0.7986441688 0.352129334 9.052197e-01 999 0.7984968482 0.6484692944 0.1080854419 0.114980506 4.598162e-01 1000 0.3461663174 0.5929790989 0.4973262595 0.372802859 6.241228e-01 i j k 1 0.9087631686 0.5128349718 0.433474417 2 0.5681274838 0.7094579476 0.868427460 3 0.3827259571 0.5897734014 0.180360822 4 0.6609854468 0.4958711024 0.009055904 5 0.8411586101 0.3029566994 0.585858709 6 0.6222287491 0.1019961545 0.630805569 7 0.5166449379 0.2911676636 0.261239435 8 0.5022442560 0.6165078580 0.410738634 9 0.3282486817 0.3663124072 0.802557074 10 0.8184540244 0.3216555105 0.994237013 11 0.5606055886 0.5415306007 0.117883356 12 0.1668717416 0.3277200800 0.320070869 13 0.9821718861 0.3913873483 0.735874106 14 0.1250124935 0.1662794249 0.617329021 15 0.4052526173 0.9588967944 0.125180201 16 0.3963778000 0.4164709412 0.982822154 17 0.7747443833 0.2370111805 0.867924212 18 0.4656205408 0.9236596262 0.231345516 19 0.4267182045 0.0398125688 0.276681161 20 0.4886503392 0.1524191860 0.040110499 21 0.7435912108 0.9295226706 0.971871639 22 0.3373999882 0.3702988313 0.919676143 23 0.5863423934 0.8232238216 0.723063651 24 0.4206146235 0.6212165076 0.282144273 25 0.1873300616 0.4141433262 0.553026780 26 0.8554507741 0.1028212600 0.855329942 27 0.6119108447 0.8649808620 0.325192982 28 0.2997493446 0.3446830176 0.702674859 29 0.6681878306 0.4829978379 0.869019716 30 0.5278012829 0.6876936750 0.849721408 31 0.7944821278 0.2671868550 0.872588813 32 0.3656178131 0.3114310633 0.124459583 33 0.9967463582 0.1866228166 0.459630596 34 0.3590255990 0.9903327457 0.988608231 35 0.2418361048 0.8444164961 0.473984825 36 0.5806110287 0.4397265683 0.489710954 37 0.6584511327 0.6120709099 0.122655185 38 0.9218367229 0.5005969931 0.271223680 39 0.8944407739 0.8447717724 0.088994969 40 0.7279412807 0.1609291413 0.455329117 41 0.2609252520 0.8845530343 0.210940112 42 0.1416404401 0.3299012582 0.467406417 43 0.2746811195 0.7520817884 0.792780091 44 0.0553447758 0.3767659578 0.026498032 45 0.5352210163 0.0118170890 0.846525728 46 0.0120629487 0.0339999525 0.166745803 47 0.4314778787 0.3114104606 0.714891932 48 0.2245725456 0.1469827355 0.018858860 49 0.7272778642 0.5995649623 0.449367810 50 0.6059436901 0.7773033865 0.684086278 51 0.4036247542 0.7489980566 0.366653045 52 0.5969315961 0.5205937014 0.886234621 53 0.0006523626 0.4913336923 0.175098365 54 0.2632054156 0.9477295359 0.419257508 55 0.3361853021 0.3503014820 0.565365737 56 0.3226710237 0.5014365071 0.358642312 57 0.4974331714 0.7017149401 0.191931669 58 0.1178657564 0.4157309311 0.556759141 59 0.3016790133 0.2646031538 0.755432571 60 0.0308654625 0.2068238084 0.846652366 61 0.4426214227 0.3930100391 0.313803358 62 0.7519367803 0.1991266685 0.373874883 63 0.0827221849 0.4457892301 0.021788683 64 0.8904469314 0.6041067331 0.965939397 65 0.2234720613 0.1910297391 0.286601910 66 0.8156521297 0.6457851853 0.387838364 67 0.4547833218 0.5390125816 0.146064876 68 0.2293814677 0.0510656524 0.234307113 69 0.4554125951 0.0007678478 0.579237812 70 0.9778966792 0.6642413577 0.981427724 71 0.4937450592 0.5499402732 0.159018812 72 0.1111480240 0.0701609459 0.231798086 73 0.4515543673 0.0222580845 0.169742819 74 0.3860736378 0.5143827731 0.339853824 75 0.6689951743 0.8895948618 0.331853422 76 0.5848921356 0.2110785230 0.208752149 77 0.8040093554 0.5435449358 0.155490631 78 0.8703865418 0.0060005118 0.535063164 79 0.6247375568 0.6896314544 0.220268407 80 0.5502351066 0.9226864551 0.188979229 81 0.8907975026 0.1401077805 0.012584140 82 0.0914727584 0.8452462007 0.567303002 83 0.1601004598 0.7238995091 0.347789467 84 0.3930475307 0.8140243778 0.828480477 85 0.2362872674 0.5648667500 0.536917268 86 0.0663146342 0.7406738328 0.967070757 87 0.6229225849 0.2372979226 0.681185639 88 0.5117010218 0.3952996521 0.237138671 89 0.8064635289 0.3335397751 0.565703884 90 0.3967762347 0.2208842041 0.354194236 91 0.6337079392 0.4096777418 0.079051379 92 0.1203049868 0.9191357759 0.658497361 93 0.9676421033 0.2858615983 0.453540877 94 0.2220023738 0.6622591268 0.935944299 95 0.0554578195 0.1503409792 0.458454961 96 0.0830594508 0.2259703281 0.460871697 97 0.8682666586 0.6073553285 0.618568527 98 0.6663306805 0.8752778720 0.774079820 99 0.4043017544 0.8350057611 0.125036883 100 0.1059333733 0.4167972640 0.458615416 101 0.6536240370 0.2890427141 0.524334089 102 0.6261212248 0.5107074548 0.048700516 103 0.2947792842 0.0883161519 0.316675600 104 0.6815966177 0.0293583267 0.242670482 105 0.9335999803 0.1404198715 0.179384844 106 0.6278110058 0.7190329526 0.200458986 107 0.5040061027 0.5368715730 0.488505151 108 0.5013903244 0.8250459556 0.930052144 109 0.1733192850 0.1501131626 0.766793029 110 0.3508784429 0.2671870328 0.727526051 111 0.1484719319 0.1510998618 0.631773586 112 0.0966769739 0.4766372084 0.924724548 113 0.7525902183 0.5807615130 0.878133470 114 0.8149411348 0.5931538984 0.529054842 115 0.9948693211 0.5770180277 0.497331441 116 0.8509829661 0.8519616781 0.331109564 117 0.1651958560 0.0783651082 0.222836789 118 0.8542163239 0.9436419141 0.642416314 119 0.2957376034 0.5372309394 0.328969282 120 0.1680100688 0.3125867224 0.536918208 121 0.7444190066 0.4906409730 0.539057107 122 0.9736629406 0.3209659122 0.420134468 123 0.4210970441 0.0146621745 0.281792870 124 0.4054784656 0.5823995634 0.021190652 125 0.1213328205 0.1566025014 0.499803931 126 0.3509171081 0.2642908753 0.828844704 127 0.8019870594 0.3684905737 0.597643980 128 0.3392660415 0.5528952766 0.362731453 129 0.2755800916 0.4141004644 0.263369496 130 0.0099175237 0.5354448098 0.867901461 131 0.5877318792 0.7913212397 0.356186446 132 0.7294213187 0.9341020375 0.426399379 133 0.3597076745 0.8540280554 0.690510602 134 0.1923128793 0.8539461282 0.470327384 135 0.4146025018 0.3285077966 0.174154247 136 0.2408502251 0.7539579268 0.974151978 137 0.6713664781 0.7490618394 0.018064682 138 0.5027397233 0.0004517024 0.371284921 139 0.1293624879 0.4354867004 0.719204444 140 0.1697100580 0.2979691955 0.825164750 141 0.1851382013 0.0322515192 0.209856055 142 0.9614997103 0.3880613439 0.210859042 143 0.2341886915 0.9113518861 0.569904638 144 0.3903274005 0.0529591336 0.977473462 145 0.5128328698 0.8339039281 0.460075917 146 0.6121268612 0.1546220388 0.120337063 147 0.5986534422 0.2321277794 0.255277535 148 0.2247179085 0.7172596608 0.901589938 149 0.7513462589 0.4561471040 0.682734193 150 0.4696188967 0.7050004448 0.276803780 151 0.9800768141 0.1836125506 0.087107321 152 0.9729995320 0.1371118471 0.786187609 153 0.0847310543 0.6312713376 0.981879490 154 0.5851558545 0.6476236130 0.915172418 155 0.0922475369 0.4266638111 0.594584180 156 0.5917444066 0.7326848062 0.536128583 157 0.8340488868 0.7819989014 0.743682370 158 0.6174082272 0.5634941428 0.103459783 159 0.1671749738 0.6828045091 0.075492039 160 0.6154925302 0.9422851873 0.305467341 161 0.6246360233 0.6056861058 0.008500006 162 0.6745371576 0.6684385163 0.103568461 163 0.1869240811 0.0430012536 0.847895277 164 0.6141133632 0.3480020654 0.071074953 165 0.7515403559 0.4299957363 0.545414267 166 0.4849006436 0.8814194989 0.211329428 167 0.2106576399 0.7893810470 0.213353900 168 0.5196030815 0.7781596312 0.544735801 169 0.1838931392 0.5161872935 0.615889838 170 0.8333251288 0.7455936016 0.366780687 171 0.0395859268 0.8005671275 0.916855233 172 0.4805583220 0.9664080332 0.414999548 173 0.5490431390 0.1338877140 0.664842228 174 0.0264012944 0.1551172663 0.822628885 175 0.0048946154 0.9222773779 0.955932898 176 0.9043120472 0.1706953272 0.072777700 177 0.8809468760 0.6098237170 0.719657137 178 0.8611115997 0.3326158209 0.364571405 179 0.4787534853 0.9453399039 0.835941072 180 0.2313803730 0.2082363386 0.499824949 181 0.6148180722 0.6784512543 0.777743101 182 0.0572387041 0.7870746215 0.120125221 183 0.8088287492 0.8559843907 0.259347438 184 0.9785300081 0.8136936259 0.709260552 185 0.3395234370 0.9604597690 0.022556264 186 0.2212379947 0.5665022719 0.951045089 187 0.3356777702 0.3005251158 0.084501502 188 0.3548264690 0.9781491724 0.799717161 189 0.7405867146 0.8685159674 0.821510137 190 0.0839215950 0.2648300638 0.943535396 191 0.8840797534 0.4699844518 0.236652090 192 0.8545890038 0.1840025745 0.977806848 193 0.5177294672 0.6102390429 0.245517909 194 0.8757470779 0.6909048136 0.651942725 195 0.6882468495 0.0161601556 0.784394057 196 0.1693982442 0.7199928972 0.756506318 197 0.3532229802 0.4630767726 0.897241289 198 0.3698309476 0.2420675876 0.738088635 199 0.1395692111 0.9148447975 0.002973847 200 0.5528753118 0.9722143693 0.530145853 201 0.0911104653 0.3148403978 0.858897036 202 0.1397922952 0.1736123802 0.794187781 203 0.9248953597 0.9052255033 0.426479524 204 0.3468633790 0.9629188555 0.392073041 205 0.2623246550 0.5600378469 0.582281488 206 0.5293257805 0.3108575726 0.160060341 207 0.5093579234 0.5014171167 0.736309079 208 0.4495433341 0.4862429700 0.392430910 209 0.9123518646 0.3051007891 0.203720998 210 0.1214102015 0.7437134380 0.704168370 211 0.7131768700 0.4649773457 0.543138901 212 0.9961626902 0.9921279163 0.345299343 213 0.1603799369 0.6949981558 0.597703475 214 0.6117657335 0.1799172421 0.127134755 215 0.2350276292 0.0418957674 0.210518366 216 0.3027018649 0.2641500751 0.995105395 217 0.7299060032 0.6030209041 0.274804636 218 0.6489603042 0.5311673591 0.184956109 219 0.2516879274 0.9647897545 0.341185981 220 0.2851197992 0.1207269041 0.697508165 221 0.5125428722 0.9168800218 0.117690779 222 0.0346664493 0.1519337283 0.779520737 223 0.7842136300 0.0845155257 0.978813627 224 0.3757599506 0.8362768008 0.939809005 225 0.5080087527 0.5440474791 0.235630636 226 0.7891359089 0.2309179760 0.190111594 227 0.6982576286 0.2912786996 0.414994985 228 0.4591538832 0.1998717184 0.226559677 229 0.0087905687 0.1122070975 0.679795615 230 0.2509475215 0.6780429292 0.506995059 231 0.0223060360 0.8224451512 0.976316434 232 0.1185056434 0.3756171942 0.999038306 233 0.0395731088 0.6464639278 0.744022369 234 0.5190228012 0.1079823810 0.281804436 235 0.8141576792 0.0351354424 0.302080039 236 0.2354405455 0.3621697398 0.462507161 237 0.1280833837 0.0429123768 0.452132628 238 0.9522720049 0.5174656950 0.667757805 239 0.8626499793 0.6394905939 0.116929316 240 0.2211448003 0.4862877068 0.187651631 241 0.2826683724 0.7685764139 0.427660304 242 0.2217451246 0.3901322596 0.527077607 243 0.9123899231 0.3484480807 0.940976249 244 0.7760570277 0.0196964843 0.994308030 245 0.7245896910 0.8625496353 0.669495775 246 0.3545963238 0.4262103755 0.925164024 247 0.8029720054 0.3310154788 0.190491454 248 0.5781933405 0.9248862842 0.819874573 249 0.7588987695 0.3347283669 0.723955127 250 0.4717798149 0.6719663588 0.033997452 251 0.5023258904 0.3928283551 0.863187823 252 0.8355372436 0.0686451662 0.909715006 253 0.8593941671 0.0926849283 0.817367764 254 0.5077188036 0.3954995340 0.768992276 255 0.2380844436 0.0159355055 0.582561954 256 0.5875904860 0.2722851331 0.175310384 257 0.4994263807 0.5650870153 0.936873940 258 0.0801673585 0.1591223918 0.240441793 259 0.5156380439 0.7538698432 0.836027631 260 0.2213389149 0.7571562580 0.967936541 261 0.5441098993 0.4593958042 0.259123230 262 0.8928442860 0.9421351061 0.866291614 263 0.6189724577 0.5877450497 0.485781986 264 0.7953684707 0.0485105908 0.582787094 265 0.7586312888 0.0050816601 0.654969776 266 0.9054281176 0.5962432623 0.478027690 267 0.9187670490 0.3565554691 0.993413433 268 0.2827955044 0.6572014024 0.191259727 269 0.7256454087 0.0734363843 0.191539427 270 0.7012636382 0.3691736532 0.951903309 271 0.7524833072 0.7902583296 0.406234259 272 0.1661315192 0.1737899601 0.991651452 273 0.8801912477 0.6872708595 0.278342104 274 0.1365374811 0.7761638940 0.887483506 275 0.7663580186 0.1873531982 0.729194304 276 0.6235552917 0.8355714106 0.314209814 277 0.5843322033 0.5417277587 0.167233856 278 0.8195801824 0.0575507390 0.816330421 279 0.4727422609 0.7954525829 0.935344556 280 0.4483542065 0.0807209269 0.061240495 281 0.3400470598 0.2865212108 0.406699197 282 0.9732844774 0.0074201201 0.504856665 283 0.3633071035 0.0794652763 0.697210894 284 0.8816565587 0.5742267463 0.926123858 285 0.4987751339 0.8233126488 0.320464924 286 0.3310508910 0.7643538795 0.709534760 287 0.2243376386 0.9196181563 0.627416339 288 0.7893124865 0.4731599311 0.791820529 289 0.3402598179 0.5928448008 0.836453283 290 0.2806643799 0.9530142890 0.190024565 291 0.8866525306 0.6742309285 0.305205277 292 0.6935368618 0.0542066889 0.255308195 293 0.9925473691 0.3263957959 0.660519107 294 0.7840081144 0.7939104903 0.554709732 295 0.8879083896 0.9838657875 0.047163502 296 0.9471069211 0.8180908239 0.667315026 297 0.7652483105 0.5011755559 0.672978379 298 0.1034767593 0.4843148482 0.765021885 299 0.5173658554 0.0807071435 0.149461499 300 0.2892686401 0.4461832349 0.774942254 301 0.4666807710 0.0640776940 0.937765861 302 0.1666596923 0.7189921502 0.176432163 303 0.6778312610 0.1221541967 0.505464738 304 0.7125912458 0.8826425248 0.330806704 305 0.2068997175 0.8807366712 0.525438205 306 0.8083262281 0.8745090137 0.359391946 307 0.0551950403 0.6054255778 0.010514143 308 0.1101146743 0.2348861066 0.428129612 309 0.3846570568 0.2119303760 0.407141645 310 0.9082448669 0.6626014228 0.642785348 311 0.5171715580 0.4968071438 0.974462328 312 0.4125887151 0.7784681309 0.425039892 313 0.5129293425 0.2333928708 0.442837440 314 0.3842024251 0.2738116076 0.910525002 315 0.8241676399 0.2300581113 0.871515020 316 0.7714029867 0.2671443268 0.820216401 317 0.1911138035 0.9620553681 0.130298762 318 0.9307477702 0.0430600131 0.650889409 319 0.8699085701 0.5043442317 0.645747940 320 0.1354798172 0.6577726228 0.614540099 321 0.9219136874 0.9839676502 0.527190544 322 0.2234225660 0.1896401674 0.277660568 323 0.7306426747 0.3842383027 0.147647479 324 0.1554096898 0.5335272748 0.714302043 325 0.7123912408 0.0143899969 0.362525354 326 0.4453180153 0.3131151772 0.545730561 327 0.6089746670 0.0491327595 0.556184235 328 0.7024399380 0.4329056886 0.196519810 329 0.7986177169 0.9928155858 0.675117668 330 0.9083080657 0.4011237149 0.939470744 331 0.2855855885 0.9352167114 0.064763133 332 0.0581693540 0.3703997759 0.489970324 333 0.2188624633 0.1252656823 0.082836872 334 0.3849162261 0.0405746803 0.599802457 335 0.5753972360 0.5074641234 0.025409097 336 0.8585140959 0.9870252733 0.960869779 337 0.3733973808 0.6797194134 0.456802760 338 0.4922717544 0.2692998312 0.738611541 339 0.7769749579 0.4674859173 0.649227513 340 0.9744688380 0.7548743489 0.444152255 341 0.5815969589 0.1518399711 0.602368516 342 0.7353012746 0.9282031991 0.711713375 343 0.9093837244 0.2169220045 0.387050143 344 0.2522708455 0.4072291572 0.123953464 345 0.1319291440 0.6047342650 0.586918667 346 0.4362283603 0.6418692195 0.057993301 347 0.7414943012 0.5013274332 0.708908894 348 0.4571445726 0.3489230359 0.690105600 349 0.1595911114 0.6860206951 0.762169352 350 0.4302436933 0.3642897324 0.242127506 351 0.5977981715 0.0813042349 0.103939542 352 0.5270542719 0.6627145151 0.735818434 353 0.6737627659 0.0368648355 0.026491203 354 0.9854462768 0.7834856187 0.373826430 355 0.4471681069 0.1585723683 0.202335776 356 0.4414274348 0.3658822568 0.603922377 357 0.3329481015 0.8038419769 0.171805945 358 0.3547660108 0.5783449381 0.172469585 359 0.5894069702 0.7482325078 0.388472585 360 0.0325384494 0.1603930925 0.764646099 361 0.4140505295 0.9774947141 0.973752263 362 0.3705947967 0.7234468276 0.418361759 363 0.1936284041 0.7779606243 0.166575791 364 0.2225097425 0.1565632462 0.553050242 365 0.2961740468 0.9200274399 0.913567509 366 0.0282904631 0.3159458183 0.394937945 367 0.9362333464 0.9701456351 0.449891258 368 0.5500495192 0.9458807199 0.152785856 369 0.9320686467 0.7501517776 0.102089221 370 0.3137839800 0.7753677391 0.062030176 371 0.5316903319 0.2782962408 0.946551740 372 0.9503071038 0.1564524630 0.373653485 373 0.8605535319 0.7530576503 0.381815319 374 0.4187593728 0.0342201130 0.807256441 375 0.7662927075 0.5564019093 0.099438679 376 0.4496172934 0.5887439523 0.059386802 377 0.4658499782 0.8791806730 0.505105258 378 0.1408295704 0.3202534819 0.866802158 379 0.6859266665 0.0451234847 0.183402799 380 0.4025848354 0.1113692916 0.956379740 381 0.5544540114 0.0858751207 0.142778292 382 0.5648256443 0.5336159950 0.924890156 383 0.6727542018 0.8860401020 0.915325739 384 0.6970705483 0.2500414085 0.768281287 385 0.3993773803 0.4700438085 0.316371213 386 0.7216110707 0.4414328204 0.099874506 387 0.7341371970 0.5050938698 0.653269146 388 0.1684000157 0.3985682614 0.863572443 389 0.6248012667 0.2710192350 0.276954715 390 0.8690601182 0.9936061928 0.115726013 391 0.8773299251 0.6416055518 0.339723316 392 0.8473763240 0.3966204205 0.818845225 393 0.6296141907 0.3974838224 0.466163284 394 0.0357694868 0.0644413177 0.624232494 395 0.4002664983 0.4900650519 0.125103871 396 0.5582493504 0.9668820433 0.078048980 397 0.5576146126 0.0716046500 0.190499994 398 0.3291113013 0.7323917581 0.997338800 399 0.5653475181 0.7782082327 0.569697561 400 0.5139552359 0.8691298212 0.528138879 401 0.1029199765 0.6415922479 0.738865932 402 0.9017956555 0.3681989533 0.014788399 403 0.1300081748 0.3727634479 0.084816476 404 0.5028760482 0.2834281570 0.753392662 405 0.6167644931 0.7392662682 0.451063136 406 0.7917854108 0.7611131743 0.813065219 407 0.1439542745 0.0858153393 0.649235810 408 0.0336355709 0.5852627060 0.615088174 409 0.6107383363 0.7312218356 0.835827948 410 0.2447151740 0.9148567214 0.042306389 411 0.5208800221 0.4018730107 0.781439138 412 0.3595253055 0.4949001716 0.004967534 413 0.6461015765 0.8702182316 0.973611509 414 0.1529908949 0.2444074138 0.765582877 415 0.1566277174 0.7693172845 0.068892283 416 0.5521451884 0.5519532149 0.177417176 417 0.6843436547 0.3240076548 0.900702647 418 0.1856415893 0.3318564647 0.096866918 419 0.0975673480 0.4760786693 0.471986917 420 0.7290326329 0.6597836316 0.955772876 421 0.5366336612 0.5549164847 0.553102183 422 0.6114070823 0.6587186693 0.007953998 423 0.0937127022 0.7113661575 0.876898227 424 0.0803781056 0.1035210942 0.598406682 425 0.3068068121 0.6794941770 0.188838216 426 0.4531581693 0.5950453898 0.409847462 427 0.3094340521 0.5425589513 0.650776462 428 0.2318717774 0.2191956949 0.648946756 429 0.5163603048 0.6094980745 0.447946401 430 0.5681897087 0.2336364791 0.295214692 431 0.4685455731 0.3028877801 0.068841520 432 0.7069857062 0.0218567387 0.233716408 433 0.3515282888 0.8465444185 0.803573804 434 0.4385973148 0.3473629637 0.617883441 435 0.4749626925 0.0890574595 0.188085022 436 0.3547779708 0.7379974185 0.791526760 437 0.1942676017 0.0079394246 0.997477157 438 0.0256246962 0.3535924666 0.698489423 439 0.8884308005 0.7628535840 0.809369163 440 0.0471414689 0.2817637813 0.904198337 441 0.1474339725 0.2587661603 0.242719773 442 0.7021593908 0.2530528193 0.524919445 443 0.4807156723 0.8349318991 0.070271240 444 0.8432966908 0.8468397048 0.470134102 445 0.3482438952 0.6815490499 0.064031017 446 0.1242048866 0.8927252733 0.522188024 447 0.6574711774 0.6846775450 0.890985590 448 0.7541771296 0.7206762349 0.774996555 449 0.7070043853 0.7600170255 0.267518109 450 0.9933493754 0.8513029092 0.694688216 451 0.5402909513 0.5576848101 0.181010432 452 0.0522799960 0.6944604991 0.640672027 453 0.5086942879 0.9485459770 0.182666690 454 0.2795328584 0.9230681325 0.071007218 455 0.2925957285 0.0272559715 0.396009628 456 0.4208056864 0.2842155597 0.657231304 457 0.9405288654 0.3529494354 0.067483188 458 0.3875212078 0.8732754863 0.913377441 459 0.2597758120 0.3927863508 0.867984843 460 0.2291357683 0.7875319326 0.051149221 461 0.1484227192 0.8090391355 0.705117003 462 0.4866195151 0.0734129762 0.442824473 463 0.7892362867 0.4453539846 0.121954784 464 0.5683366538 0.6619364661 0.186760232 465 0.2412845306 0.1936834501 0.890848876 466 0.1244373966 0.5603391614 0.533789977 467 0.4005386035 0.4215240139 0.446426343 468 0.6987896913 0.6909501497 0.352884758 469 0.8079235938 0.4607041548 0.625612257 470 0.5136809410 0.2786674271 0.611246112 471 0.1271985730 0.4093848013 0.702205135 472 0.4425768794 0.1256974903 0.462740155 473 0.2701393508 0.0287763786 0.963752114 474 0.0852672718 0.3647785818 0.785544906 475 0.8383072678 0.3441293056 0.229997027 476 0.5987685372 0.6069351842 0.916894967 477 0.3016595787 0.8286003179 0.098079569 478 0.9367990731 0.7363867506 0.099831548 479 0.8027157031 0.3406797077 0.764953703 480 0.5650394219 0.7784218604 0.571389669 481 0.8543107586 0.2325923357 0.177473234 482 0.5957680093 0.2863060525 0.621867881 483 0.5386852284 0.4697146376 0.561348902 484 0.9937895050 0.9476125687 0.195175298 485 0.8102212714 0.2234492698 0.496894690 486 0.3026532985 0.2417237794 0.904216400 487 0.8369588512 0.4713993466 0.017311705 488 0.3915421951 0.0331468829 0.286754538 489 0.8201161565 0.3621713065 0.306632450 490 0.0397220741 0.6384723403 0.862288375 491 0.6482997711 0.6928636658 0.498058393 492 0.5576362605 0.2238930671 0.915707878 493 0.6607955222 0.3236755331 0.891231451 494 0.4501192681 0.8065570174 0.833592845 495 0.9771914661 0.2452873208 0.684543002 496 0.7574431095 0.2715495902 0.929892377 497 0.1792897196 0.1909950075 0.430867825 498 0.6345388771 0.6050982235 0.213430690 499 0.7307245585 0.2590056360 0.491157572 500 0.9550017666 0.6125783336 0.430126610 501 0.4132789590 0.5587415013 0.707540820 502 0.1261792867 0.3612853771 0.656284911 503 0.9251572632 0.4820273374 0.021563028 504 0.4478524600 0.6163896762 0.859326350 505 0.7165222999 0.4614907785 0.907702564 506 0.6541780606 0.2412444674 0.490135853 507 0.9442219913 0.6415377590 0.456914689 508 0.9467369465 0.8996300348 0.901641032 509 0.9640912083 0.0410869946 0.980695251 510 0.3732388653 0.4830861366 0.464831451 511 0.1478042577 0.7105469978 0.361243103 512 0.7865168659 0.0258799670 0.610352629 513 0.4026820601 0.1141515123 0.073774567 514 0.9134162779 0.1482030996 0.838317783 515 0.9313118907 0.7333646615 0.186578115 516 0.7229551040 0.0172408349 0.876216590 517 0.1166268843 0.2034378981 0.622242737 518 0.5041656836 0.2613980127 0.729145212 519 0.0489721072 0.5793831972 0.163670554 520 0.9104810045 0.4284867200 0.348988740 521 0.8480586212 0.6895436323 0.741701515 522 0.4400068077 0.9950883300 0.472101074 523 0.5311336038 0.3148409440 0.508500267 524 0.4360992794 0.8477999608 0.074460126 525 0.5760545996 0.0326191103 0.332112870 526 0.6865018832 0.5540374245 0.623823412 527 0.5416533023 0.4976719327 0.197002878 528 0.4076520230 0.2285872947 0.757847161 529 0.2378068483 0.2019681430 0.869347882 530 0.6370063778 0.0563453555 0.534364452 531 0.6947946933 0.7111502644 0.327195446 532 0.8854648860 0.9748951907 0.583619741 533 0.5297113250 0.1012475549 0.639655538 534 0.5360497350 0.7042040324 0.703935319 535 0.8615456868 0.9087736702 0.259641427 536 0.3270071556 0.9687156640 0.510379517 537 0.5498819028 0.7116793904 0.800633240 538 0.3981204438 0.1055317647 0.332887672 539 0.1884881954 0.8991657628 0.651935739 540 0.9430633688 0.8470065235 0.985933232 541 0.2957795497 0.4845798714 0.620495155 542 0.5941085669 0.7551016384 0.626696358 543 0.6616673837 0.4585884756 0.332763716 544 0.2252898337 0.8735535287 0.250021933 545 0.8492385307 0.6050229876 0.976643395 546 0.3284978110 0.0208509073 0.552758015 547 0.5258779998 0.0941185125 0.712287780 548 0.0221860658 0.9694864992 0.365406210 549 0.9999005587 0.8640711615 0.799345604 550 0.2249033919 0.5597511516 0.985683206 551 0.2581388494 0.5881800281 0.297886255 552 0.0824325080 0.6931518228 0.818499598 553 0.2511851306 0.1219530550 0.073947041 554 0.5003025474 0.9877631674 0.345837254 555 0.1924816635 0.2932026354 0.722906101 556 0.9450151417 0.1164777910 0.899795833 557 0.0556947265 0.3653142145 0.833011540 558 0.7865477495 0.5493993000 0.922149335 559 0.3554171724 0.9726744783 0.872534773 560 0.3390859307 0.9219060326 0.820741070 561 0.7379814328 0.2120381724 0.693108915 562 0.0683563130 0.5029085143 0.827244471 563 0.1917957552 0.1268493799 0.267469959 564 0.9083188039 0.0586665533 0.942383993 565 0.9460777743 0.8651171569 0.837674263 566 0.5658098406 0.4958966884 0.791546705 567 0.3632471245 0.2307066286 0.804219844 568 0.8588721370 0.8229775466 0.228302267 569 0.5019012033 0.8366414213 0.455467683 570 0.2970695554 0.8778691809 0.141149248 571 0.3248096840 0.5907368434 0.104615112 572 0.5410586749 0.4280349151 0.594926988 573 0.2025833130 0.4288823237 0.677922638 574 0.9951313485 0.8449413823 0.481459997 575 0.5276287389 0.0723635927 0.321710046 576 0.1055847048 0.7462469260 0.912850004 577 0.7024908115 0.3999357389 0.186478262 578 0.7943248851 0.5395993334 0.920239061 579 0.0276398500 0.3101629145 0.118177905 580 0.1592246417 0.6026619161 0.576358764 581 0.7662116871 0.5950969979 0.813566438 582 0.4213486142 0.3764504716 0.827479247 583 0.7426504176 0.8308128808 0.507187529 584 0.6318207965 0.6347201027 0.125925017 585 0.7360877271 0.9745964138 0.068576889 586 0.3384028485 0.9400807733 0.005205019 587 0.4784741311 0.2316760146 0.342613811 588 0.4661606846 0.5499884067 0.588007716 589 0.2016149708 0.2244607336 0.990291373 590 0.1429585121 0.3208649538 0.698472717 591 0.5382393540 0.0899150085 0.088298887 592 0.6516490525 0.5870290762 0.460073830 593 0.7825980638 0.6463206944 0.834109541 594 0.0360894774 0.9555895720 0.374218001 595 0.0227680141 0.4798172456 0.230312304 596 0.6223328020 0.8188983048 0.259263913 597 0.0481542398 0.8776977926 0.928196097 598 0.4031829375 0.8861788490 0.386161405 599 0.3800641324 0.8961149775 0.744411892 600 0.9373641352 0.2962837527 0.228169761 601 0.1529641864 0.4246568638 0.561339771 602 0.3132382028 0.7459880235 0.701694527 603 0.6395901495 0.5864937957 0.118726775 604 0.3812313871 0.9908500304 0.687834089 605 0.6585895927 0.6807643520 0.807358899 606 0.0407362625 0.3835575725 0.941637933 607 0.6343256603 0.9410840643 0.201974260 608 0.7297780693 0.7785401796 0.685937374 609 0.8169210239 0.0184312759 0.995562205 610 0.6954687983 0.6902421599 0.107519840 611 0.3587358908 0.8174600382 0.887226315 612 0.4950156456 0.9988616975 0.460948817 613 0.5124422519 0.9000427283 0.311366444 614 0.4193161500 0.2330806397 0.331944056 615 0.1372738420 0.8558078955 0.634889112 616 0.6487600016 0.4483145340 0.200628157 617 0.6930406457 0.1405613457 0.946231753 618 0.5379465765 0.7251331760 0.693917649 619 0.3548438435 0.0162725917 0.684812097 620 0.5906515468 0.7329806052 0.904670520 621 0.6603484906 0.6076894940 0.091763721 622 0.3636276750 0.0564669154 0.066468826 623 0.8929951529 0.0173468851 0.913456712 624 0.5193466372 0.4707073863 0.934256916 625 0.6822469798 0.3953355562 0.289309436 626 0.0824959101 0.2921910062 0.044693490 627 0.7019017276 0.9668182791 0.750223392 628 0.4275053341 0.8367497751 0.332937845 629 0.0922464526 0.4102489238 0.812326842 630 0.4956276130 0.8354235855 0.744443205 631 0.5282284787 0.3624600177 0.437094010 632 0.4597527271 0.8304499635 0.176761237 633 0.0696412656 0.6655701364 0.389712420 634 0.3637543309 0.0200155203 0.947725208 635 0.1510170416 0.2281706810 0.351725206 636 0.6223586730 0.2150866047 0.854345126 637 0.7047801397 0.0133177778 0.107109227 638 0.0828919602 0.5865104373 0.861315191 639 0.3410899027 0.5765409295 0.923982420 640 0.1366543320 0.7885096699 0.751887410 641 0.2085322624 0.8079397904 0.664778188 642 0.4698440309 0.3802732481 0.143456051 643 0.3877236466 0.6920087750 0.195913500 644 0.5385239455 0.7067448399 0.208434227 645 0.7802318172 0.4299515062 0.629565495 646 0.0606531394 0.1196082544 0.712476384 647 0.7366263098 0.1238772105 0.722834192 648 0.4990216279 0.9745745591 0.335687502 649 0.1208609531 0.7079302194 0.138499342 650 0.5504307044 0.3144544242 0.634861803 651 0.5357888581 0.1576599039 0.895698367 652 0.8372551946 0.6539439561 0.121514479 653 0.8663264213 0.9303858271 0.101986314 654 0.8151593243 0.7399452543 0.564897440 655 0.5766503746 0.7543237149 0.178147843 656 0.7617674056 0.3053654318 0.211049212 657 0.5318685232 0.5440830449 0.438073355 658 0.3935762031 0.1543934196 0.792410720 659 0.6021584028 0.1720456653 0.780616953 660 0.4456974030 0.2089627727 0.670902713 661 0.1230438619 0.8262171666 0.825892488 662 0.1998506242 0.1914696826 0.688846716 663 0.8685076090 0.0552638469 0.592013019 664 0.1232273690 0.4114682395 0.111715549 665 0.5658827955 0.9385932428 0.630991100 666 0.0928694389 0.3512963140 0.551601721 667 0.6724121408 0.6340143762 0.619171673 668 0.7189048890 0.4819496642 0.394437478 669 0.7450684533 0.6334214911 0.120663491 670 0.4686049288 0.3850550903 0.225975410 671 0.3261675073 0.5564902842 0.439655563 672 0.7405954481 0.1667849601 0.778386813 673 0.5571954157 0.3600399296 0.341081738 674 0.3966088423 0.4169549660 0.984560798 675 0.8554791166 0.3829855390 0.851567330 676 0.2645492698 0.3424455107 0.810737652 677 0.9462965208 0.7442224536 0.081737134 678 0.2153703177 0.2868734032 0.846269100 679 0.3989983632 0.6966406952 0.934992647 680 0.2243203325 0.8841968733 0.506934221 681 0.7125027745 0.7271546698 0.810377810 682 0.0787354475 0.2923811390 0.245231815 683 0.2284422005 0.7890823116 0.217577334 684 0.2295348563 0.3330317193 0.373440096 685 0.7260223969 0.7212731063 0.153669810 686 0.2057637249 0.2594939065 0.245352808 687 0.5401250874 0.8030458153 0.469113623 688 0.4596121595 0.3054662987 0.468811055 689 0.3042413592 0.1917874399 0.576269256 690 0.2890700931 0.9998134219 0.942503915 691 0.8854543604 0.1116818776 0.383314030 692 0.6776874792 0.5922678534 0.693309897 693 0.5526406986 0.1233293717 0.087034084 694 0.1912126953 0.0772855950 0.887516976 695 0.7959685409 0.3028606996 0.898052296 696 0.3024255738 0.5259479519 0.319401479 697 0.6322551961 0.9411505456 0.893078855 698 0.0522503981 0.2155010924 0.074356175 699 0.4422330016 0.0192078033 0.237554085 700 0.2619074290 0.6082642223 0.982594245 701 0.9869507232 0.8930197172 0.273582796 702 0.4603780659 0.4993943570 0.096071468 703 0.2949062802 0.8040989162 0.918891617 704 0.4938740865 0.5205094554 0.076841577 705 0.0972780201 0.2060259327 0.607541317 706 0.8583569909 0.7992311558 0.047517068 707 0.7276338739 0.5565277084 0.241794257 708 0.4677256306 0.1712370019 0.228214926 709 0.0024954216 0.2747133188 0.322586709 710 0.3169592826 0.9069585775 0.423306790 711 0.7098196479 0.9432440118 0.358426370 712 0.2738314560 0.0971547274 0.470391238 713 0.3008469422 0.0383837584 0.365313627 714 0.7889818386 0.9164815550 0.875330874 715 0.2325974195 0.1971113200 0.561480183 716 0.5846996335 0.2200600684 0.825061466 717 0.5494406375 0.6702300981 0.063295416 718 0.1967900842 0.6804504094 0.817557294 719 0.9293358885 0.9159706607 0.612163059 720 0.4464485829 0.1391113657 0.016358729 721 0.1984538860 0.9922945008 0.260804367 722 0.6011682674 0.0078241592 0.994232268 723 0.4368371693 0.6018165236 0.271021605 724 0.0359789054 0.0309819141 0.291022451 725 0.4358850084 0.3571438494 0.672495972 726 0.0843085411 0.7200475056 0.543844765 727 0.9935347575 0.2231217008 0.162310042 728 0.8694156781 0.7412591046 0.116366554 729 0.3230546513 0.6869672353 0.355232729 730 0.2692568868 0.7453184524 0.318895462 731 0.9205692166 0.0721968049 0.747437760 732 0.9830821056 0.9566366056 0.735812146 733 0.3031027087 0.1807147223 0.515523697 734 0.8074569593 0.7621163037 0.277175065 735 0.1328059128 0.2716216373 0.386344013 736 0.3731001110 0.4226538686 0.893595606 737 0.8101217598 0.1615734680 0.545682621 738 0.9005346450 0.9483917356 0.162716153 739 0.5641075023 0.9057268400 0.315164645 740 0.7819188943 0.0401506533 0.443097079 741 0.7546357475 0.8867348845 0.545873337 742 0.9840175221 0.1725108344 0.646062576 743 0.1318724579 0.0028678142 0.238467700 744 0.2567289958 0.1308603347 0.707534412 745 0.6086509489 0.3696970518 0.405707285 746 0.7872655632 0.5497150687 0.446210140 747 0.6726261911 0.8797215447 0.694525995 748 0.2360209036 0.1031810008 0.939940774 749 0.7058318090 0.8746639634 0.876881678 750 0.5646322125 0.5175348681 0.899540994 751 0.3558960068 0.0915856815 0.071928061 752 0.7108148087 0.9289045038 0.409138034 753 0.3208266883 0.6941444234 0.420427548 754 0.7587849370 0.8223717990 0.617787307 755 0.8554418513 0.6104312195 0.685158405 756 0.5451220218 0.1108894546 0.754115731 757 0.7196569699 0.8765047307 0.742762204 758 0.0215774111 0.7833220700 0.358652892 759 0.7936053767 0.1422403960 0.228015036 760 0.5894972838 0.6867280926 0.727455930 761 0.5965793543 0.2810900006 0.049796629 762 0.6823342303 0.5309740093 0.709166650 763 0.4851749572 0.8965564165 0.270272462 764 0.2186638459 0.4845173971 0.223458015 765 0.5240909741 0.1192635912 0.836980513 766 0.7615321467 0.2461064239 0.305156313 767 0.5604892469 0.1504576304 0.778905628 768 0.5576477910 0.1195758597 0.729724572 769 0.0298766780 0.3231485880 0.810902923 770 0.6399024175 0.1802191990 0.031875964 771 0.4483339144 0.8136381572 0.266554726 772 0.1456385537 0.6185407676 0.424736338 773 0.9980573370 0.4370639510 0.906303392 774 0.1771693868 0.6572026338 0.648354213 775 0.3075491870 0.2273569726 0.480896802 776 0.3877885581 0.1247133319 0.641370056 777 0.3255822414 0.4710750806 0.601308974 778 0.9333021557 0.2480874057 0.644261836 779 0.6875130839 0.1116264574 0.975304031 780 0.3070184211 0.5074775650 0.232165708 781 0.5424951171 0.2500219222 0.605900436 782 0.9170924036 0.3036775775 0.130279193 783 0.3457037862 0.1089829586 0.030755766 784 0.2805052300 0.3548592539 0.194080905 785 0.1462095752 0.0438470747 0.631534894 786 0.7462217780 0.2378483214 0.231059715 787 0.6584212827 0.7611358352 0.918849279 788 0.2268953726 0.0877234547 0.932185473 789 0.5527597035 0.3166887127 0.353739069 790 0.2126589350 0.4487409566 0.044028442 791 0.6011075461 0.7731962018 0.346391862 792 0.4955664894 0.4370704067 0.739043013 793 0.8423375196 0.1270604522 0.317217213 794 0.1852961355 0.2097269609 0.088107983 795 0.9665051368 0.2191235535 0.096969624 796 0.7390349819 0.7743024451 0.050907627 797 0.3564518855 0.6329019496 0.299431361 798 0.1001586283 0.9697424432 0.119666547 799 0.2597023719 0.6517637181 0.153376347 800 0.6991052001 0.9141410457 0.265971137 801 0.4336216610 0.6204367809 0.287241898 802 0.9722799619 0.9041905978 0.949226398 803 0.3676507603 0.7783370393 0.605557245 804 0.9472060276 0.1846980839 0.961454876 805 0.7027460397 0.0863679377 0.350598877 806 0.1309114473 0.5038668043 0.978715864 807 0.2550966237 0.5919990689 0.726682640 808 0.2965044272 0.7501525732 0.205615580 809 0.2213688432 0.3952954053 0.998190792 810 0.4808703209 0.4208385709 0.995545863 811 0.3624091498 0.4595744461 0.799096379 812 0.8317626240 0.2396454047 0.548187687 813 0.6742728841 0.1104490117 0.598536491 814 0.1261447764 0.1552675441 0.272370511 815 0.9580946744 0.4285296819 0.968113240 816 0.6149920234 0.8321298480 0.698046484 817 0.5011624463 0.6944097814 0.278748058 818 0.9852243848 0.2406683902 0.018022690 819 0.0772609098 0.7450529367 0.039625611 820 0.5415186076 0.4747119257 0.896825491 821 0.3348767199 0.6098611136 0.577773161 822 0.4783724891 0.3886900644 0.677212795 823 0.3442536590 0.7314892649 0.086071891 824 0.5835320374 0.4907233738 0.177435393 825 0.9339889446 0.3731712026 0.393874652 826 0.9251223728 0.6926013883 0.662840765 827 0.3417600116 0.5710211955 0.394355623 828 0.4704917639 0.5288532262 0.204753422 829 0.6319271945 0.8954059565 0.101903225 830 0.7197887672 0.7918050652 0.871796291 831 0.6301961564 0.2209867758 0.095405005 832 0.7079224242 0.9972060299 0.683585294 833 0.5663749434 0.4601809371 0.591159837 834 0.8835416073 0.9517572247 0.021346556 835 0.3805341318 0.7682048625 0.607100267 836 0.1149117290 0.3352859647 0.221796387 837 0.6165271252 0.1312675015 0.518161186 838 0.1176810323 0.2363307266 0.026905061 839 0.3870377922 0.6967954086 0.984365234 840 0.5219549667 0.0975140380 0.633635873 841 0.7895589520 0.0749162380 0.677225510 842 0.7397307083 0.4174277787 0.146708182 843 0.3478056667 0.4072185773 0.446643704 844 0.1604789551 0.2810919166 0.376148333 845 0.9251088433 0.8910745184 0.184053494 846 0.0136304810 0.8324750320 0.012508075 847 0.8104454086 0.9549242419 0.174654766 848 0.2838601123 0.4352603666 0.662441208 849 0.9989845387 0.4433427551 0.085998558 850 0.3344213252 0.8014390990 0.625002468 851 0.0715592853 0.2017857837 0.922036019 852 0.5103901837 0.6150838642 0.096749347 853 0.0633041605 0.1500133087 0.950916851 854 0.3187373579 0.5897086798 0.941266669 855 0.2308404257 0.6086097441 0.500319101 856 0.0412874152 0.1480539243 0.938956048 857 0.7764879493 0.2771860566 0.941295495 858 0.6102122869 0.8902521830 0.208078722 859 0.6847100409 0.1657704103 0.559949947 860 0.4759049457 0.2603934291 0.265524188 861 0.7726181280 0.9765032728 0.764477738 862 0.1444186131 0.4385247866 0.596427928 863 0.8826502324 0.5756433317 0.878524726 864 0.9136634860 0.1600622404 0.461303858 865 0.0380516683 0.7695483940 0.921434690 866 0.8698804867 0.4721365480 0.359045910 867 0.9890793655 0.4400611159 0.450260557 868 0.2904401754 0.1828926203 0.895283058 869 0.8492796938 0.0380649534 0.518706080 870 0.7597751443 0.3915797304 0.304194722 871 0.5994729039 0.2184883629 0.986610676 872 0.2150855558 0.0174954759 0.904102191 873 0.8826147132 0.5637028904 0.361423424 874 0.9353977251 0.9109342301 0.867015991 875 0.5928153037 0.4265120404 0.147806567 876 0.2736188730 0.3341847470 0.252160472 877 0.4527158521 0.2142571628 0.992705174 878 0.3849735709 0.0373925450 0.334103196 879 0.7877801368 0.9842010399 0.195866342 880 0.1202234565 0.1930400908 0.652330256 881 0.2110027948 0.5344378769 0.642685626 882 0.7905226327 0.3779459668 0.635173605 883 0.2934547048 0.3679792546 0.593422699 884 0.4573139457 0.1582164683 0.023487672 885 0.8029606193 0.7010082020 0.284318620 886 0.9665408591 0.6073600804 0.660012832 887 0.2198708144 0.1453944338 0.889665270 888 0.2914561022 0.7143639419 0.116632195 889 0.9410663755 0.6670784273 0.292615006 890 0.7086967225 0.8629533281 0.860167607 891 0.0894650980 0.1249317278 0.458329211 892 0.4917318639 0.5387740363 0.287814197 893 0.9280869819 0.7142212233 0.937446594 894 0.4264453109 0.0457125187 0.646521847 895 0.7130430103 0.2010276795 0.914412531 896 0.7420520796 0.7263595280 0.231711044 897 0.7444685814 0.4017101536 0.065026434 898 0.8719011499 0.9411909385 0.356957232 899 0.3430910874 0.3079510119 0.265917442 900 0.5024578348 0.2550702172 0.718842493 901 0.4036464305 0.0388136716 0.539094977 902 0.7733309979 0.7575183806 0.438675189 903 0.7544153144 0.0066090750 0.055222656 904 0.6703264886 0.6471884502 0.448854770 905 0.2629647448 0.9012367099 0.881672440 906 0.1803582727 0.3752786340 0.852202812 907 0.8431271741 0.5248582852 0.310761757 908 0.2284120221 0.1554611062 0.271397468 909 0.9815748923 0.3106818793 0.503561401 910 0.5820819039 0.8009110682 0.313130387 911 0.3962558024 0.5859679591 0.967218323 912 0.2827562944 0.0372282511 0.217953848 913 0.8501226422 0.3453022940 0.648826417 914 0.6198121691 0.5673128264 0.431295119 915 0.8348960320 0.0189236847 0.810088007 916 0.9729937878 0.1587446914 0.135784560 917 0.6216949238 0.5137279432 0.302157789 918 0.3172746876 0.7171976375 0.318524733 919 0.2923211546 0.7407354964 0.835473312 920 0.9858742347 0.5725584505 0.127078771 921 0.1445829689 0.2193578896 0.297499642 922 0.1202723817 0.9384786098 0.926139504 923 0.5611545546 0.5124028493 0.830471531 924 0.6683973325 0.0950436117 0.228388001 925 0.3101932399 0.1064313874 0.696718747 926 0.0065761928 0.5799916589 0.728362826 927 0.2705754482 0.6334289019 0.213996048 928 0.6163870748 0.9408771771 0.305505149 929 0.5825870181 0.7055658610 0.854551745 930 0.0560807060 0.2184536899 0.973126440 931 0.0305702714 0.2604252915 0.294440166 932 0.1147877721 0.2299285294 0.343987265 933 0.6380620983 0.7959253674 0.404231852 934 0.5519847185 0.9795302413 0.858354224 935 0.9288181656 0.8940306874 0.888728004 936 0.3502963928 0.8894755677 0.944722258 937 0.0143268388 0.0346940127 0.235338558 938 0.0341333786 0.7793426653 0.963776887 939 0.7106323622 0.5434363140 0.751127602 940 0.1490545860 0.4038637085 0.716740757 941 0.4734449964 0.4437388908 0.475056946 942 0.7208320657 0.9866875494 0.543951573 943 0.2622331488 0.2832836569 0.035830080 944 0.7833650876 0.7233259575 0.428406089 945 0.5218209329 0.1005121029 0.218219518 946 0.7760500603 0.6374043820 0.052455828 947 0.6739972050 0.0909351802 0.904614732 948 0.8914999729 0.1385136431 0.479137238 949 0.1951620504 0.8737525195 0.972547704 950 0.4718771121 0.2480492322 0.041709913 951 0.2666831575 0.6943463988 0.988687727 952 0.7738986546 0.1257526183 0.893773123 953 0.1496802690 0.3521917525 0.754022287 954 0.4720940117 0.6001135744 0.338170915 955 0.8082588040 0.8172353797 0.916966529 956 0.5283849533 0.7120828282 0.081214865 957 0.8441957091 0.4700241142 0.493242827 958 0.0938648465 0.6583943067 0.981820053 959 0.3471312686 0.0675094635 0.037047612 960 0.6650372180 0.3251894296 0.116741180 961 0.7140586991 0.0734348153 0.631955886 962 0.6655903978 0.7986309046 0.094634300 963 0.1438620712 0.3221085093 0.243953107 964 0.7278595115 0.2289379491 0.362771598 965 0.1802145545 0.8742005217 0.723245273 966 0.7823882208 0.3580379246 0.054962467 967 0.6995504668 0.0912349010 0.632198407 968 0.7469125430 0.3847571795 0.911859033 969 0.1873785281 0.8334370614 0.705362193 970 0.2881696546 0.5870317714 0.125191815 971 0.4128162214 0.0516679150 0.382608344 972 0.4137606274 0.0732873864 0.657100869 973 0.7626614401 0.3218054264 0.278459829 974 0.6579748753 0.9010156130 0.481620893 975 0.7438406860 0.4247153068 0.448899247 976 0.0268771388 0.9448438413 0.121263162 977 0.4899677269 0.7269265945 0.664452199 978 0.8597049168 0.5871146580 0.460823134 979 0.2966905709 0.5181974168 0.803420998 980 0.3388050934 0.8238629079 0.973437019 981 0.4403209696 0.8365467857 0.604767612 982 0.3581107967 0.7897584133 0.748093019 983 0.9181199386 0.7010477460 0.974371480 984 0.4855272081 0.5026488579 0.734248891 985 0.7215003686 0.7091594590 0.313281513 986 0.6219019089 0.2435479793 0.284571647 987 0.5285257522 0.3192963838 0.505222295 988 0.9388092165 0.8667535833 0.026928221 989 0.2814255413 0.7957225109 0.526132444 990 0.5068233504 0.2621329527 0.250766840 991 0.2548627399 0.4510898469 0.885296197 992 0.8310871634 0.1405788586 0.569291269 993 0.4782470900 0.0357218925 0.887661504 994 0.1346597481 0.9022380263 0.849145710 995 0.1659126263 0.7283752465 0.633044650 996 0.1651590609 0.6786643637 0.996066710 997 0.6562744505 0.5517038938 0.438446892 998 0.3805340903 0.9445086999 0.743781529 999 0.8062844079 0.7442193504 0.075846629 1000 0.9749252927 0.7738602485 0.271969781","tags":"R Stats","url":"http://chrisalbon.com/r-stats/indexing-tricks.html"},{"title":"Indexing And Slicing Numpy Arrays","loc":"http://chrisalbon.com/python/indexing_and_slicing_numpy_arrays.html","text":"Slicing Arrays Explanation Of Broadcasting Unlike many other data types, slicing an array into a new variable means that any chances to that new variable are broadcasted to the original variable. Put other way, a slice is a hotlink to the original array variable, not a seperate and independent copy of it. # Import Modules import numpy as np # Create an array of battle casualties from the first to the last battle battleDeaths = np . array ([ 1245 , 2732 , 3853 , 4824 , 5292 , 6184 , 7282 , 81393 , 932 , 10834 ]) # Divide the array of battle deaths into start, middle, and end of the war warStart = battleDeaths [ 0 : 3 ]; print ( 'Death from battles at the start of war:' , warStart ) warMiddle = battleDeaths [ 3 : 7 ]; print ( 'Death from battles at the middle of war:' , warMiddle ) warEnd = battleDeaths [ 7 : 10 ]; print ( 'Death from battles at the end of war:' , warEnd ) Death from battles at the start of war: [1245 2732 3853] Death from battles at the middle of war: [4824 5292 6184 7282] Death from battles at the end of war: [81393 932 10834] # Change the battle death numbers from the first battle warStart [ 0 ] = 11101 # View that change reflected in the warStart slice of the battleDeaths array warStart array([11101, 2732, 3853]) # View that change reflected in (i.e. \"broadcasted to) the original battleDeaths array battleDeaths array([11101, 2732, 3853, 4824, 5292, 6184, 7282, 81393, 932, 10834]) Indexing Arrays Note: This multidimensional array behaves like a dataframe or matrix (i.e. columns and rows) # Create an array of regiment information regimentNames = [ 'Nighthawks' , 'Sky Warriors' , 'Rough Riders' , 'New Birds' ] regimentNumber = [ 1 , 2 , 3 , 4 ] regimentSize = [ 1092 , 2039 , 3011 , 4099 ] regimentCommander = [ 'Mitchell' , 'Blackthorn' , 'Baker' , 'Miller' ] regiments = np . array ([ regimentNames , regimentNumber , regimentSize , regimentCommander ]) regiments array([['Nighthawks', 'Sky Warriors', 'Rough Riders', 'New Birds'], ['1', '2', '3', '4'], ['1092', '2039', '3011', '4099'], ['Mitchell', 'Blackthorn', 'Baker', 'Miller']], dtype='<U12') # View the first column of the matrix regiments [:, 0 ] array(['Nighthawks', '1', '1092', 'Mitchell'], dtype='<U12') # View the second row of the matrix regiments [ 1 ,] array(['1', '2', '3', '4'], dtype='<U12') # View the top-right quarter of the matrix regiments [: 2 , 2 :] array([['Rough Riders', 'New Birds'], ['3', '4']], dtype='<U12')","tags":"Python","url":"http://chrisalbon.com/python/indexing_and_slicing_numpy_arrays.html"},{"title":"Inner, Outer, Right, And Left Joins","loc":"http://chrisalbon.com/r-stats/inner-outter-right-left-join.html","text":"Additional explaination: http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html Original source: http://stackoverflow.com/questions/1299871/how-to-join-data-frames-in-r-inner-outer-left-right # create two dataframes are fake data survey.1 <- data.frame ( response.id = c ( 1 : 6 ), region = c ( rep ( \"Kisumu\" , 3 ), rep ( \"Mumbasa\" , 3 ))) survey.2 <- data.frame ( response.id = c ( 2 , 4 , 6 ), region = c ( rep ( \"Nairobi\" , 2 ), rep ( \"Tankana\" , 1 ))) Inner Join Only Merges Observations Shared By Both Data Frames # merge survey.1 and survey.2 by response.id merge ( survey.1 , survey.2 , by = \"response.id\" ) response.id region.x region.y 1 2 Kisumu Nairobi 2 4 Mumbasa Nairobi 3 6 Mumbasa Tankana Outer Join Includes Merges All Observations, Leaving A NULL Observation When There Is No Match # merge survey.1 and survey.2 by response.id, but include observations that don't match merge ( x = survey.1 , y = survey.2 , by = \"response.id\" , all = TRUE ) response.id region.x region.y 1 1 Kisumu <NA> 2 2 Kisumu Nairobi 3 3 Kisumu <NA> 4 4 Mumbasa Nairobi 5 5 Mumbasa <NA> 6 6 Mumbasa Tankana Left Join Includes All Observations From The First Dataframe But Only Matching Observations From The Second Dataframe # merge survey.1 and survey.2 by response.id, but include survey.1 observations merge ( x = survey.1 , y = survey.2 , by = \"response.id\" , all.x = TRUE ) response.id region.x region.y 1 1 Kisumu <NA> 2 2 Kisumu Nairobi 3 3 Kisumu <NA> 4 4 Mumbasa Nairobi 5 5 Mumbasa <NA> 6 6 Mumbasa Tankana Left Join Includes All Observations From The Second Dataframe But Only Matching Observations From The First Dataframe # merge survey.1 and survey.2 by response.id, but include survey.2 observations merge ( x = survey.1 , y = survey.2 , by = \"response.id\" , all.y = TRUE ) response.id region.x region.y 1 2 Kisumu Nairobi 2 4 Mumbasa Nairobi 3 6 Mumbasa Tankana","tags":"R Stats","url":"http://chrisalbon.com/r-stats/inner-outter-right-left-join.html"},{"title":"Installing and loading packages","loc":"http://chrisalbon.com/r-stats/installing-and-loading-packages.html","text":"# Install the package called \"mapdata\" # install.packages(\"mapdata\") # Load the package called \"mapdata\" # library(mapdata)","tags":"R Stats","url":"http://chrisalbon.com/r-stats/installing-and-loading-packages.html"},{"title":"Iterate An Ifelse Over A List","loc":"http://chrisalbon.com/python/iterate_ifelse_over_list.html","text":"Create some data word_list = [ 'Egypt' , 'Watching' , 'Eleanor' ] vowels = [ 'A' , 'E' , 'I' , 'O' , 'U' ] Create a for loop # for each item in the word_list, for word in word_list : # if any word starts with e, where e is vowels, if any ([ word . startswith ( e ) for e in vowels ]): # then print is valid, print ( 'Is valid' ) # if not, else : # print invalid print ( 'Invalid' ) Is valid Invalid Is valid","tags":"Python","url":"http://chrisalbon.com/python/iterate_ifelse_over_list.html"},{"title":"Jitterplot","loc":"http://chrisalbon.com/r-stats/jitterplot.html","text":"Original source: ggplot2 book # load the ggplot2 library library ( ggplot2 ) # set the seed so we can reproduce the results set.seed ( 1410 ) # create a variable that is the first 100 rows of the diamonds dataset dsmall <- diamonds [ sample ( nrow ( diamonds ), 100 ), ] # create a jitter plot that displays the price per carat varies with the colour of the diamond using jittering qplot ( color , price / carat , data = diamonds , geom = \"jitter\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/jitterplot.html"},{"title":"Lambda Functions","loc":"http://chrisalbon.com/python/lambda_functions.html","text":"In Python it is possible to string lambda functions together. Create a series, called pipeline, that contains three mini functions pipeline = [ lambda x : x ** 2 - 1 + 5 , lambda x : x ** 20 - 2 + 3 , lambda x : x ** 200 - 1 + 4 ] For each item in pipeline, run the lambda function with x = 3 for f in pipeline : print ( f ( 3 )) 13 3486784402 265613988875874769338781322035779626829233452653394495974574961739092490901302182994384699044004","tags":"Python","url":"http://chrisalbon.com/python/lambda_functions.html"},{"title":"Line and Path Plots","loc":"http://chrisalbon.com/r-stats/line-and-path-plots.html","text":"Original source: ggplot2 book Line plots is a path plot sorted by the x value # load the ggplot2 library library ( ggplot2 ) # set the seed so we can reproduce the results set.seed ( 1410 ) # plot a line plot with the date on the x axis and unemployment rate qplot ( date , unemploy / pop , data = economics , geom = \"line\" ) # plot a line plot with the date being the x axis and the avg number of weeks unemployed on the y axis qplot ( date , uempmed , data = economics , geom = \"line\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/line-and-path-plots.html"},{"title":"Line Graph","loc":"http://chrisalbon.com/r-stats/line-graph.html","text":"# load the ggplot2 package library ( ggplot2 ) # plot pressure vs. temperature with a line ggplot ( pressure , aes ( x = temperature , y = pressure )) + geom_line () + geom_point ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/line-graph.html"},{"title":"ggplot2  line plot","loc":"http://chrisalbon.com/r-stats/line-plot.html","text":"Original source: r graphics cookbook # load the ggplot2 library library ( ggplot2 ) # load the gcookbook library library ( gcookbook ) # create the ggplot2 data ggplot ( BOD , aes ( x = Time , y = demand )) + # draw the line geom_line () + # expand the y axis to include 0 expand_limits ( y = 0 ) + # add points geom_point () Line plot with multiple lines # load plyr package library ( plyr ) # summarize ToothGrowth tg <- ddply ( ToothGrowth , c ( \"supp\" , \"dose\" ), summarise , length = mean ( len )) # by assigning supp to color, we tell ggplot2 to draw these as two different lines ggplot ( tg , aes ( x = dose , y = length , colour = supp )) + geom_line () # map supp to linetype to draw two different lines (this line with different line types) ggplot ( tg , aes ( x = dose , y = length , linetype = supp )) + geom_line () # same as above, but this time since x is a factor, we have to specify to ggplot how to group the data ggplot ( tg , aes ( x = factor ( dose ), y = length , colour = supp , group = supp )) + geom_line ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/line-plot.html"},{"title":"Line Plot With Multiple Lines","loc":"http://chrisalbon.com/r-stats/line-plot-with-multiple-lines.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # load plyr package library ( plyr ) # reset the graphing device dev.off () null device 1 # summarize the ToothGrowth data tg <- ddply ( ToothGrowth , c ( \"supp\" , \"dose\" ), summarise , length = mean ( len )) # create a ggplot with lines colored by the tg$supp variable ggplot ( tg , aes ( x = dose , y = length , colour = supp )) + geom_line () # create a ggplot with line-types determined by the tg$supp variable ggplot ( tg , aes ( x = dose , y = length , linetype = supp )) + geom_line ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/line-plot-with-multiple-lines.html"},{"title":"Plot With Both Lines And Poiints","loc":"http://chrisalbon.com/r-stats/line-plot-with-points.html","text":"# load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # create a plot with both lines and dots ggplot ( BOD , aes ( x = Time , y = demand )) + geom_line () + geom_point ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/line-plot-with-points.html"},{"title":"Indexing Lists","loc":"http://chrisalbon.com/r-stats/list-indexing.html","text":"Source: The R Book # create a list with simulated values score <- runif ( 100 ) states.df <- data.frame ( state1 = state.name [ 1 : 10 ], state2 = state.name [ 11 : 20 ], state3 = state.name [ 21 : 30 ]) name <- letters [ 1 : 20 ] data.ls <- list ( score , states.df , name ) rm ( score , states.df , name ) # view the list data.ls [[1]] [1] 0.63621710 0.59751371 0.01766910 0.73041249 0.54653846 0.43043145 [7] 0.11636422 0.89274444 0.44612503 0.39206699 0.12284822 0.26404920 [13] 0.92865699 0.95513231 0.58906710 0.10746887 0.60970037 0.22867149 [19] 0.35527089 0.60892570 0.35784036 0.72655682 0.84694322 0.39318969 [25] 0.62687130 0.43777173 0.66495234 0.20309509 0.59805951 0.83228360 [31] 0.03682167 0.65222574 0.39590677 0.84520655 0.65905423 0.58668714 [37] 0.97529621 0.03153370 0.21449200 0.60840212 0.11533601 0.98922948 [43] 0.28021083 0.93354662 0.87155782 0.13732277 0.32347877 0.41667773 [49] 0.78557746 0.03647728 0.14362887 0.75381004 0.83698159 0.47739142 [55] 0.49435061 0.93762428 0.76667158 0.43727024 0.92981355 0.47720586 [61] 0.25126170 0.52683971 0.23403415 0.93919411 0.38310410 0.93215790 [67] 0.32509680 0.46385170 0.52321614 0.11183669 0.49761109 0.12499426 [73] 0.32904283 0.10927183 0.51750471 0.24286425 0.59768396 0.56955455 [79] 0.86041375 0.83515259 0.20773829 0.69600639 0.83626496 0.36528740 [85] 0.70885737 0.72743847 0.14066207 0.65039423 0.43245807 0.13238103 [91] 0.35716796 0.24420694 0.76592652 0.43970478 0.36873838 0.48452003 [97] 0.25268138 0.69132921 0.48043993 0.54034955 [[2]] state1 state2 state3 1 Alabama Hawaii Massachusetts 2 Alaska Idaho Michigan 3 Arizona Illinois Minnesota 4 Arkansas Indiana Mississippi 5 California Iowa Missouri 6 Colorado Kansas Montana 7 Connecticut Kentucky Nebraska 8 Delaware Louisiana Nevada 9 Florida Maine New Hampshire 10 Georgia Maryland New Jersey [[3]] [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\" \"k\" \"l\" \"m\" \"n\" \"o\" \"p\" \"q\" \"r\" \"s\" [20] \"t\" # select 1st list element data.ls [[ 1 ]] [1] 0.63621710 0.59751371 0.01766910 0.73041249 0.54653846 0.43043145 [7] 0.11636422 0.89274444 0.44612503 0.39206699 0.12284822 0.26404920 [13] 0.92865699 0.95513231 0.58906710 0.10746887 0.60970037 0.22867149 [19] 0.35527089 0.60892570 0.35784036 0.72655682 0.84694322 0.39318969 [25] 0.62687130 0.43777173 0.66495234 0.20309509 0.59805951 0.83228360 [31] 0.03682167 0.65222574 0.39590677 0.84520655 0.65905423 0.58668714 [37] 0.97529621 0.03153370 0.21449200 0.60840212 0.11533601 0.98922948 [43] 0.28021083 0.93354662 0.87155782 0.13732277 0.32347877 0.41667773 [49] 0.78557746 0.03647728 0.14362887 0.75381004 0.83698159 0.47739142 [55] 0.49435061 0.93762428 0.76667158 0.43727024 0.92981355 0.47720586 [61] 0.25126170 0.52683971 0.23403415 0.93919411 0.38310410 0.93215790 [67] 0.32509680 0.46385170 0.52321614 0.11183669 0.49761109 0.12499426 [73] 0.32904283 0.10927183 0.51750471 0.24286425 0.59768396 0.56955455 [79] 0.86041375 0.83515259 0.20773829 0.69600639 0.83626496 0.36528740 [85] 0.70885737 0.72743847 0.14066207 0.65039423 0.43245807 0.13238103 [91] 0.35716796 0.24420694 0.76592652 0.43970478 0.36873838 0.48452003 [97] 0.25268138 0.69132921 0.48043993 0.54034955 # select the 1st list element, then select it's 2nd value data.ls [[ 1 ]][ 2 ] [1] 0.5975137 # select the 2nd list element, then select it's value in the 3rd row and 1st column data.ls [[ 2 ]][ 3 , 1 ] [1] Arizona 10 Levels: Alabama Alaska Arizona Arkansas California Colorado ... Georgia","tags":"R Stats","url":"http://chrisalbon.com/r-stats/list-indexing.html"},{"title":"Convert A List Into A Dataframe","loc":"http://chrisalbon.com/r-stats/list-to-dataframe.html","text":"# create a list of two elements, the first a vector of 20 obs, the second 20 state names the.list <- list ( scores = runif ( 20 ), states = state.name [ 1 : 20 ]); the.list $scores [1] 0.72770677 0.61598787 0.96234018 0.19366523 0.26006986 0.07253386 [7] 0.73779295 0.58013676 0.18979002 0.72309143 0.48458231 0.51446047 [13] 0.88990973 0.53759564 0.62698847 0.08148905 0.31585633 0.88666670 [19] 0.87910724 0.59395343 $states [1] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" \"California\" [6] \"Colorado\" \"Connecticut\" \"Delaware\" \"Florida\" \"Georgia\" [11] \"Hawaii\" \"Idaho\" \"Illinois\" \"Indiana\" \"Iowa\" [16] \"Kansas\" \"Kentucky\" \"Louisiana\" \"Maine\" \"Maryland\" # create a dataframe with identifying each element of the list as a dataframe column df <- data.frame ( the.list $ scores , the.list $ states ); df the.list.scores the.list.states 1 0.72770677 Alabama 2 0.61598787 Alaska 3 0.96234018 Arizona 4 0.19366523 Arkansas 5 0.26006986 California 6 0.07253386 Colorado 7 0.73779295 Connecticut 8 0.58013676 Delaware 9 0.18979002 Florida 10 0.72309143 Georgia 11 0.48458231 Hawaii 12 0.51446047 Idaho 13 0.88990973 Illinois 14 0.53759564 Indiana 15 0.62698847 Iowa 16 0.08148905 Kansas 17 0.31585633 Kentucky 18 0.88666670 Louisiana 19 0.87910724 Maine 20 0.59395343 Maryland","tags":"R Stats","url":"http://chrisalbon.com/r-stats/list-to-dataframe.html"},{"title":"List All Files And Folders In A Directory","loc":"http://chrisalbon.com/command-line/list_all_files_and_folders_in_a_directory.html","text":"%% bash -- out output # Line above: Run bash, with the output being a python variable called 'output' # Change the working directory to the current directory cd \"$(dirname \" $ 0 \")\" # For all filenames, print the filename, then end for f in * ; do echo \"$f\" ; done # Print the variable with the filenames print ( output ) list_all_files_and_folders_in_a_directory.ipynb","tags":"Command Line","url":"http://chrisalbon.com/command-line/list_all_files_and_folders_in_a_directory.html"},{"title":"List Operations","loc":"http://chrisalbon.com/python/list_operations.html","text":"Create a list of simulated data countries = [ \"Spain\" , \"Japan\" , \"Kenya\" , \"Tanzania\" , \"Ghana\" ] Add an item to the end of the list Note: If the appending item contains multiple items, it is nested. countries . append ( \"Nigeria\" ); countries ['Spain', 'Japan', 'Kenya', 'Tanzania', 'Ghana', 'Nigeria'] Add all items of a list to a list countries . extend ([ \"France\" , \"Switzerland\" , \"China\" ]); countries ['Spain', 'Japan', 'Kenya', 'Tanzania', 'Ghana', 'Nigeria', 'France', 'Switzerland', 'China'] Insert Italy into the second position of the list countries countries . insert ( 1 , \"Italy\" ); countries ['Spain', 'Italy', 'Japan', 'Kenya', 'Tanzania', 'Ghana', 'Nigeria', 'France', 'Switzerland', 'China'] Remove Spain from the list countries countries . remove ( \"Spain\" ); countries ['Italy', 'Japan', 'Kenya', 'Tanzania', 'Ghana', 'Nigeria', 'France', 'Switzerland', 'China'] Delete the second item from the list countries del countries [ 1 ]; countries ['Italy', 'Kenya', 'Tanzania', 'Ghana', 'Nigeria', 'France', 'Switzerland', 'China'] Remove the third item from the list, and returns it (i.e. \"pops it out\") countries ['Italy', 'Kenya', 'Tanzania', 'Ghana', 'Nigeria', 'France', 'Switzerland', 'China'] countries . pop ( 3 ) 'Ghana' countries ['Italy', 'Kenya', 'Tanzania', 'Nigeria', 'France', 'Switzerland', 'China'] Return the index of the first item that matches \"Kenya\" kenya = countries . index ( \"Kenya\" ); kenya 1 Count all number of times \"Kenya\" appears in the list countries kenya_count = countries . count ( \"Kenya\" ); kenya_count 1 Sort the list in alphabetical order countries . sort (); countries ['China', 'France', 'Italy', 'Kenya', 'Nigeria', 'Switzerland', 'Tanzania'] Reverse the elements of the list countries . reverse (); countries ['Tanzania', 'Switzerland', 'Nigeria', 'Kenya', 'Italy', 'France', 'China'] Copy the list, same as countries[:] countries_copy = countries . copy (); countries_copy ['Tanzania', 'Switzerland', 'Nigeria', 'Kenya', 'Italy', 'France', 'China'] Remove all the items from the list countries countries . clear (); countries []","tags":"Python","url":"http://chrisalbon.com/python/list_operations.html"},{"title":"Lists","loc":"http://chrisalbon.com/r-stats/lists.html","text":"Lists are objects whose elements can be anything, from matrices to functions. # Create a numeric vector x <- runif ( 50 ) # Create a character string vector y <- state.name # Create a matrix m <- matrix ( c ( 3 , 2 , 4 , 3 ), nrow = 2 ) # Create a list containing all of the above l <- list ( x , y , m ) # You can name each element in a list names ( l ) <- c ( \"percent voted\" , \"state\" , \"vote matrix\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/lists.html"},{"title":"Logical Operations","loc":"http://chrisalbon.com/python/logical_operations.html","text":"Create some simulated variables x = 6 y = 9 z = 12 x or y x or y 6 x and y x and y 9 not x not x False x is equal to y x == y False x is not equal to y x != y True One is less than two 1 < 2 True Two is less than or equal to four 2 <= 4 True Three is equal to five 3 == 5 False Three is not equal to four 3 != 4 True x is less than y which is less than z x < y < z True x is less than y and y is less than z x < y and y < z True","tags":"Python","url":"http://chrisalbon.com/python/logical_operations.html"},{"title":"Loops","loc":"http://chrisalbon.com/r-stats/loops.html","text":"R has three types of loops: repeat, while, and for. repeat Loops Repeat loops executes the same code again and again until told to stop. # print 1 to 10, stop at 10 for ( i in 1 : 10 ) { print ( paste ( \"i =\" , i )); } [1] \"i = 1\" [1] \"i = 2\" [1] \"i = 3\" [1] \"i = 4\" [1] \"i = 5\" [1] \"i = 6\" [1] \"i = 7\" [1] \"i = 8\" [1] \"i = 9\" [1] \"i = 10\" while Loops Unlike repeat loops, while loops check at the beginning of the code if that code should end, and then (maybe) execute. This is the oppsite to repeat loops that execute and then check if they should end. # create an object that samples one of the three character strings action <- sample ( c ( \"Make pancakes\" , \"Make burritos\" , \"Make tacos\" ), 1 ) # If the variable \"action\" (created above) is not \"Make tacos\" then run action (recreated in the loop) again. Do so until it is make tacos. while ( action != \"Make tacos\" ) { action <- sample ( c ( \"Make pancakes\" , \"Make burritos\" , \"Make tacos\" ), 1 ) message ( action ) } Make burritos Make tacos for Loops For loops are for when you want to repeat a code for a known and finite amount of time. for loops operate on each element of the vector. # for each element in a 1:5 vector, say a message for ( i in 1 : 5 ) message ( \"i = \" , i ) # For each month in the variable, month.name, say a message for ( month in month.name ) { message ( \"The month of \" , month ) } i = 1 i = 2 i = 3 i = 4 i = 5 The month of January The month of February The month of March The month of April The month of May The month of June The month of July The month of August The month of September The month of October The month of November The month of December","tags":"R Stats","url":"http://chrisalbon.com/r-stats/loops.html"},{"title":"Loops With Lists","loc":"http://chrisalbon.com/r-stats/loops-with-lists.html","text":"# create list object of fake data names <- c ( \"John\" , \"Jack\" , \"John\" , \"Jack\" , \"Jack\" ) numbers <- c ( 3434 , 4352 , 3452 , 3452 , 2345 ) numbers2 <- c ( 3434 , 4352 , 3452 , 3452 , 2345 ) win <- c ( T , F , T , F , T , F ) data.list <- list ( names , numbers , win ) data.list.numbers <- list ( numbers , numbers2 ) # apply (via list apply) the unique() function to every element in the list lapply ( data.list , unique ) [[1]] [1] \"John\" \"Jack\" [[2]] [1] 3434 4352 3452 2345 [[3]] [1] TRUE FALSE vapply is like lapply, but it returns a vector # apply (via list apply) the unique() function to every element in the list vapply ( data.list.numbers , unique , numeric ( 4 )) [,1] [,2] [1,] 3434 3434 [2,] 4352 4352 [3,] 3452 3452 [4,] 2345 2345","tags":"R Stats","url":"http://chrisalbon.com/r-stats/loops-with-lists.html"},{"title":"Managing Data Frames","loc":"http://chrisalbon.com/r-stats/managing-data-frames.html","text":"# Load LearningR package library ( learningr ) # Load english_monarchs data from the LearningR package data ( english_monarchs , package = \"learningr\" ) Altering A Single Column # Create a new column from the differece between two date columns english_monarchs $ length.of.reign.years <- with ( english_monarchs , end.of.reign - start.of.reign ) english_monarchs name house 1 Wehha Wuffingas 2 Wuffa Wuffingas 3 Tytila Wuffingas 4 Rædwald Wuffingas 5 Eorpwald Wuffingas 6 Ricberht Wuffingas 7 Sigeberht and Ecgric Wuffingas 8 Ecgric Wuffingas 9 Anna Wuffingas 10 Æthelhere Wuffingas 11 Æthelwold Wuffingas 12 Ealdwulf Wuffingas 13 Ælfwald Wuffingas 14 Hun, Beonna and Alberht Wuffingas 15 Æthelred I Wuffingas 16 Æthelberht II Wuffingas 17 Offa Mercia 18 Offa Mercia 19 Offa and Ecgfrith Mercia 20 Ecgfrith Mercia 21 Eadwald East Anglia 22 C\\u009cnwulf Mercia 23 C\\u009cnwulf and Cynehelm Mercia 24 C\\u009cnwulf Mercia 25 Ceolwulf Mercia 26 Beornwulf Mercia 27 Æthelstan East Anglia 28 Æthelweard East Anglia 29 Edmund East Anglia 30 <NA> <NA> 31 Oswald Norse 32 Æthelred II Norse 33 Guthrum Dane 34 Eohric Dane 35 Æthelwold Dane 36 Guthrum II Dane 37 Aescwine East Saxon 38 Sledda East Saxon 39 Saebert East Saxon 40 Saeward East Saxon 41 Sigeberht the Little East Saxon 42 Sigeberht the Good East Saxon 43 Swithelm East Saxon 44 Sighere and Sebbi East Saxon 45 Sebbi East Saxon 46 Sigeheard East Saxon 47 Sigeheard and Swaefred East Saxon 48 Offa East Saxon 49 Saelred East Saxon 50 Swaefbert East Saxon 51 <NA> <NA> 52 Svvithred East Saxon 53 Sigeric East Saxon 54 Sigered East Saxon 55 Hengist Kent 56 Horsa Kent 57 Oisc Kent 58 Octa Kent 59 Eormenric Kent 60 Æðelberht I Kent 61 Eadbald Kent 62 Æðelwald Kent 63 Eorcenberht and Eormenred Kent 64 Eorcenberht Kent 65 Ecgbehrt I Kent 66 Hlothhere Kent 67 Eadric Kent 68 Mul Kent 69 Swæfheard Kent 70 Oswine, Swæfbehrt, Swæfheard Kent 71 Swæfbehrt, Swæfheard, Wihtred Kent 72 Wihtred Kent 73 Æðelberht II, Ælfric and Eadberht I Kent 74 Æðelberht II and Eardwulf Kent 75 Eadberht II, Eanmund and Sigered Kent 76 Heaberht Kent 77 Heaberht and Ecgbehrt II Kent 78 Ealhmund Kent 79 Ecgbehrt III Præn Kent 80 Cuðred Kent 81 Baldred Kent 82 Ecgbehrt and Æthelwulf Cerdicing 83 Ecgbehrt and Æthelwulf Cerdicing 84 Ecgbehrt and Æthelwulf Cerdicing 85 Æthelwulf and Æðelstan I Cerdicing 86 Æthelwulf Cerdicing 87 Æthelwulf and Æðelberht III Cerdicing 88 Æðelberht III Cerdicing 89 Æthelred I Cerdicing 90 Cearl Mercia 91 Penda Mercia 92 Penda and Eowa Mercia 93 Penda Mercia 94 Penda and Peada Mercia 95 Oswiu Bernicia 96 Wulfhere Mercia 97 Æthelred I Mercia 98 Coenred Mercia 99 Ceolred Mercia 100 Ceolwald Mercia 101 Æthelbald Mercia 102 Beornred Mercia 103 Ludeca Mercia 104 Wiglaf Mercia 105 Wiglaf Mercia 106 Wigmund Mercia 107 Wigstan Mercia 108 Ælfflæd Mercia 109 Beorhtwulf Mercia 110 Burgred Mercia 111 Ceolwulf II Mercia 112 <NA> Mercia 113 Æthelred, Lord of the Mercians Mercia 114 Æthelflæd, Lady of the Mercians Mercia 115 Ælfwynn, Second Lady of the Mercians Mercia 116 Esa Bernicia 117 Eoppa Bernicia 118 Ida Bernicia 119 Glappa Bernicia 120 Adda Bernicia 121 Æthelric Bernicia 122 Theodric Bernicia 123 Frithuwald Bernicia 124 Hussa Bernicia 125 Æthelfrith Bernicia 126 Edwin Deira 127 Eanfrith Bernicia 128 Oswald Bernicia 129 Oswiu Bernicia 130 Oswiu Bernicia 131 Oswiu Bernicia 132 Oswiu Bernicia 133 Ælla Deira 134 Æthelric Deira 135 Osric Deira 136 Oswine Deira 137 Æthelwold Deira 138 Alchfrith Deira 139 Ælfwine Deira 140 Ecgfrith Northumbria 141 Aldfrith Northumbria 142 Eadwulf Northumbria 143 Osred I Northumbria 144 Coenred Northumbria 145 Osric Northumbria 146 Coelwulf Northumbria 147 Coelwulf Northumbria 148 Eadberht Northumbria 149 Oswulf Northumbria 150 Æthelwald Moll Northumbria 151 Alhred Northumbria 152 Æthelred I Northumbria 153 Æthelwald I Northumbria 154 Osred II Northumbria 155 Æthelred I Northumbria 156 Osbald Northumbria 157 Eardwulf Northumbria 158 Ælfwald II Northumbria 159 Eardwulf Northumbria 160 Eanred Northumbria 161 Æthelred II Northumbria 162 Rædwulf Northumbria 163 Æthelred II Northumbria 164 Osberht Northumbria 165 Ælle II Northumbria 166 Osberht Northumbria 167 Ecgberht I Norse 168 Ricsige Norse 169 Ecgberht II Norse 170 Guðroðr Norse 171 Sigfroðr Norse 172 Knútr Norse 173 Æthelwold Norse 174 Hálfdan and Eowils Norse 175 Eadwulf II Norse 176 Ealdred Norse 177 Ragnall ua Ímair Norse 178 Sigtrygg Caech Norse 179 Amlaíb mac Gofraid Norse 180 Amlaíb Cuarán Norse 181 Ragnall mac Gofraid Norse 182 Eric Norse 183 Amlaíb Cuarán Norse 184 Eric Norse 185 Eadred Norse 186 Ælle South Saxon 187 Cissa South Saxon 188 <NA> <NA> 189 Æðelwealh South Saxon 190 Eadwulf South Saxon 191 Ecgwald South Saxon 192 Berhthun South Saxon 193 Andhun South Saxon 194 Noðhelm and Watt Wessex 195 Noðhelm and Bryni Wessex 196 Noðhelm and Osric Wessex 197 Noðhelm and Æðelstan Wessex 198 Æðelstan Wessex 199 <NA> Wessex 200 <NA> <NA> 201 Æðelbehrt Wessex 202 Osmund Wessex 203 Ælfwald, Oslac and Osmund Wessex 204 Ælfwald, Ealdwulf, Oslac and Osmund Mercia 205 Ælfwald, Ealdwulf, Oslac, Osmund and Oswald Mercia 206 Ealdwulf Mercia 207 <NA> <NA> 208 Cerdic Gewissae 209 Cynric Gewissae 210 Ceawlin Gewissae 211 Ceol Gewissae 212 Ceolwulf Gewissae 213 Cynegils Gewissae 214 Cwichelm Gewissae 215 Cenwalh Gewissae 216 Penda Mercia 217 Cenwalh Cerdicing 218 Cenwalh and Seaxburh Cerdicing 219 Cenfus Cerdicing 220 Æscwine Cerdicing 221 Centwine Cerdicing 222 Caedwalla Cerdicing 223 Ine Cerdicing 224 Æthelheard Cerdicing 225 Cuthred Cerdicing 226 Sigeberht Cerdicing 227 Cynewulf Cerdicing 228 Beorhtric Cerdicing 229 Ecgbehrt Cerdicing 230 Æthelbald Cerdicing 231 Æthelbret Cerdicing 232 Alfred the Great Cerdicing 233 Edward the Elder Cerdicing 234 Ælfweard Cerdicing 235 Æthelstan Cerdicing 236 Æthelstan Wessex 237 Edmund I Wessex 238 Eadred Wessex 239 Eadwig Wessex 240 Edgar the Peaceful Wessex 241 Edward the Martyr Wessex 242 Æthelred the Unready Wessex 243 Sweyn Forkbeard Knýtlinga 244 Æthelred the Unready Wessex 245 Edmund Ironside Wessex 246 Cnut Knýtlinga 247 Harold Harefoot Knýtlinga 248 Harthacnut Knýtlinga 249 Edward the Confessor Wessex 250 Harold Godwinson Wessex 251 Edgar the Ætheling Wessex 252 William I Normandy 253 William II Normandy 254 Henry I Normandy 255 Stephen Normandy 256 Matilda Plantagenet/Angevin 257 Stephen Normandy 258 Henry II Plantagenet/Angevin 259 Richard I Plantagenet/Angevin start.of.reign end.of.reign domain length.of.reign.years 1 NA 571 East Anglia NA 2 571 578 East Anglia 7 3 578 616 East Anglia 38 4 616 627 East Anglia 11 5 627 627 East Anglia 0 6 627 630 East Anglia 3 7 630 634 East Anglia 4 8 634 NA East Anglia NA 9 NA 653 East Anglia NA 10 653 655 East Anglia 2 11 655 663 East Anglia 8 12 663 713 East Anglia 50 13 713 749 East Anglia 36 14 749 NA East Anglia NA 15 NA 779 East Anglia NA 16 779 794 East Anglia 15 17 757 785 East Anglia, Mercia 28 18 785 787 East Anglia, Kent, Mercia 2 19 787 796 East Anglia, Kent, Mercia 9 20 796 796 East Anglia, Kent, Mercia 0 21 796 800 East Anglia 4 22 796 798 East Anglia, Kent, Mercia 2 23 798 812 East Anglia, Kent, Mercia 14 24 812 821 East Anglia, Kent, Mercia 9 25 821 823 East Anglia, Kent, Mercia 2 26 823 826 East Anglia, Mercia 3 27 826 845 East Anglia 19 28 845 855 East Anglia 10 29 855 869 East Anglia 14 30 869 875 East Anglia 6 31 875 875 East Anglia 0 32 875 879 East Anglia 4 33 879 890 East Anglia 11 34 890 902 East Anglia 12 35 902 902 East Anglia 0 36 902 918 East Anglia 16 37 527 587 Essex 60 38 587 604 Essex 17 39 604 616 Essex 12 40 616 617 Essex 1 41 617 653 Essex 36 42 653 660 Essex 7 43 660 664 Essex 4 44 664 683 Essex 19 45 683 694 Essex 11 46 694 695 Essex 1 47 695 709 Essex 14 48 709 709 Essex 0 49 709 715 Essex 6 50 715 738 Essex 23 51 738 746 Essex 8 52 746 758 Essex 12 53 758 798 Essex 40 54 798 812 Essex 14 55 NA NA Kent NA 56 NA NA Kent NA 57 NA NA Kent NA 58 512 534 Kent 22 59 534 590 Kent 56 60 590 616 Kent 26 61 616 640 Kent 24 62 NA NA Kent NA 63 640 NA Kent NA 64 NA 664 Kent NA 65 664 673 Kent 9 66 674 685 Kent 11 67 685 686 Kent 1 68 686 687 Kent 1 69 687 689 Kent 2 70 689 690 Kent 1 71 689 692 Kent 3 72 692 725 Kent 33 73 725 748 Kent 23 74 748 762 Kent 14 75 762 764 Kent 2 76 764 765 Kent 1 77 765 779 Kent 14 78 NA 785 Kent NA 79 796 798 Kent 2 80 798 807 Kent 9 81 823 825 Kent 2 82 825 829 Kent, Wessex 4 83 829 830 Kent, Mercia, Wessex 1 84 830 839 Kent, Wessex 9 85 839 851 Kent, Wessex 12 86 851 855 Kent, Wessex 4 87 855 858 Kent, Wessex 3 88 858 866 Kent, Wessex 8 89 866 871 Kent, Wessex 5 90 606 NA Mercia NA 91 626 635 Mercia 9 92 635 642 Mercia 7 93 642 653 Mercia 11 94 653 655 Mercia 2 95 655 658 Mercia, Northumbria 3 96 658 675 Mercia 17 97 675 704 Mercia 29 98 704 709 Mercia 5 99 709 716 Mercia 7 100 716 716 Mercia 0 101 716 757 Mercia 41 102 757 757 Mercia 0 103 826 827 Mercia 1 104 827 829 Mercia 2 105 830 839 Mercia 9 106 839 840 Mercia 1 107 840 840 Mercia 0 108 840 840 Mercia 0 109 840 852 Mercia 12 110 852 874 Mercia 22 111 874 879 Mercia 5 112 879 883 Mercia 4 113 883 911 Mercia 28 114 911 918 Mercia 7 115 918 918 Mercia 0 116 500 520 Bernicia 20 117 520 547 Bernicia 27 118 547 559 Bernicia 12 119 559 560 Bernicia 1 120 560 568 Bernicia 8 121 568 572 Bernicia 4 122 572 579 Bernicia 7 123 579 585 Bernicia 6 124 585 593 Bernicia 8 125 593 616 Northumbria 23 126 616 632 Northumbria 16 127 632 633 Bernicia 1 128 634 642 Northumbria 8 129 642 644 Northumbria 2 130 644 654 Bernicia 10 131 654 655 Northumbria 1 132 658 670 Northumbria 12 133 559 589 Deira 30 134 589 604 Deira 15 135 633 634 Deira 1 136 644 651 Northumbria 7 137 651 654 Northumbria 3 138 656 664 Deira 8 139 670 679 Deira 9 140 670 685 Northumbria 15 141 685 704 Northumbria 19 142 704 705 Northumbria 1 143 705 716 Northumbria 11 144 716 718 Northumbria 2 145 718 729 Northumbria 11 146 729 731 Northumbria 2 147 731 737 Northumbria 6 148 737 758 Northumbria 21 149 758 759 Northumbria 1 150 759 765 Northumbria 6 151 765 774 Northumbria 9 152 774 779 Northumbria 5 153 779 788 Northumbria 9 154 788 790 Northumbria 2 155 790 796 Northumbria 6 156 796 796 Northumbria 0 157 796 806 Northumbria 10 158 806 808 Northumbria 2 159 808 810 Northumbria 2 160 810 841 Northumbria 31 161 841 844 Northumbria 3 162 844 844 Northumbria 0 163 844 849 Northumbria 5 164 849 862 Northumbria 13 165 862 867 Northumbria 5 166 867 867 Northumbria 0 167 867 872 Northumbria 5 168 872 876 Northumbria 4 169 876 878 Northumbria 2 170 878 895 Northumbria 17 171 NA NA Northumbria NA 172 NA NA Northumbria NA 173 900 902 Northumbria 2 174 902 910 Northumbria 8 175 910 913 Northumbria 3 176 913 918 Northumbria 5 177 914 921 Northumbria 7 178 921 927 Northumbria 6 179 939 941 Northumbria 2 180 941 944 Northumbria 3 181 943 944 Northumbria 1 182 947 948 Northumbria 1 183 949 952 Northumbria 3 184 952 954 Northumbria 2 185 954 954 Northumbria 0 186 477 514 Sussex 37 187 514 567 Sussex 53 188 567 660 Sussex 93 189 660 685 Sussex 25 190 683 683 Sussex 0 191 683 685 Sussex 2 192 685 NA Sussex NA 193 685 NA Sussex NA 194 692 700 Sussex 8 195 700 710 Sussex 10 196 710 717 Sussex 7 197 717 717 Sussex 0 198 717 724 Sussex 7 199 724 726 Sussex 2 200 726 740 Sussex 14 201 740 NA Sussex NA 202 760 765 Sussex 5 203 765 771 Sussex 6 204 771 772 Sussex 1 205 772 772 Sussex 0 206 772 791 Sussex 19 207 791 825 Sussex 34 208 519 534 Wessex 15 209 534 560 Wessex 26 210 560 591 Wessex 31 211 591 597 Wessex 6 212 597 611 Wessex 14 213 611 643 Wessex 32 214 626 636 Wessex 10 215 643 645 Wessex 2 216 645 648 Wessex 3 217 648 672 Wessex 24 218 672 674 Wessex 2 219 674 674 Wessex 0 220 674 676 Wessex 2 221 676 685 Wessex 9 222 685 688 Wessex 3 223 688 726 Wessex 38 224 726 740 Wessex 14 225 740 756 Wessex 16 226 756 757 Wessex 1 227 757 786 Wessex 29 228 786 802 Wessex 16 229 802 825 Wessex 23 230 858 860 Wessex 2 231 860 865 Wessex 5 232 871 899 Wessex 28 233 899 924 Wessex 25 234 924 924 Wessex 0 235 924 927 Wessex 3 236 927 939 England 12 237 939 946 England 7 238 946 955 England 9 239 955 959 England 4 240 959 975 England 16 241 975 978 England 3 242 978 1013 England 35 243 1013 1014 England 1 244 1014 1016 England 2 245 1016 1016 England 0 246 1016 1035 England 19 247 1035 1040 England 5 248 1040 1042 England 2 249 1042 1066 England 24 250 1066 1066 England 0 251 1066 1066 England 0 252 1066 1087 England 21 253 1087 1100 England 13 254 1100 1135 England 35 255 1135 1141 England 6 256 1141 1141 England 0 257 1141 1154 England 13 258 1154 1189 England 35 259 1189 1199 England 10 Altering Multiple Columns # Within the english_monarchs dataframe, create two new columns english_monarchs <- within ( english_monarchs , { length.of.reign.years <- end.of.reign - start.of.reign reign.was.more.than.30.years <- length.of.reign.years > 30 } ) english_monarchs name house 1 Wehha Wuffingas 2 Wuffa Wuffingas 3 Tytila Wuffingas 4 Rædwald Wuffingas 5 Eorpwald Wuffingas 6 Ricberht Wuffingas 7 Sigeberht and Ecgric Wuffingas 8 Ecgric Wuffingas 9 Anna Wuffingas 10 Æthelhere Wuffingas 11 Æthelwold Wuffingas 12 Ealdwulf Wuffingas 13 Ælfwald Wuffingas 14 Hun, Beonna and Alberht Wuffingas 15 Æthelred I Wuffingas 16 Æthelberht II Wuffingas 17 Offa Mercia 18 Offa Mercia 19 Offa and Ecgfrith Mercia 20 Ecgfrith Mercia 21 Eadwald East Anglia 22 C\\u009cnwulf Mercia 23 C\\u009cnwulf and Cynehelm Mercia 24 C\\u009cnwulf Mercia 25 Ceolwulf Mercia 26 Beornwulf Mercia 27 Æthelstan East Anglia 28 Æthelweard East Anglia 29 Edmund East Anglia 30 <NA> <NA> 31 Oswald Norse 32 Æthelred II Norse 33 Guthrum Dane 34 Eohric Dane 35 Æthelwold Dane 36 Guthrum II Dane 37 Aescwine East Saxon 38 Sledda East Saxon 39 Saebert East Saxon 40 Saeward East Saxon 41 Sigeberht the Little East Saxon 42 Sigeberht the Good East Saxon 43 Swithelm East Saxon 44 Sighere and Sebbi East Saxon 45 Sebbi East Saxon 46 Sigeheard East Saxon 47 Sigeheard and Swaefred East Saxon 48 Offa East Saxon 49 Saelred East Saxon 50 Swaefbert East Saxon 51 <NA> <NA> 52 Svvithred East Saxon 53 Sigeric East Saxon 54 Sigered East Saxon 55 Hengist Kent 56 Horsa Kent 57 Oisc Kent 58 Octa Kent 59 Eormenric Kent 60 Æðelberht I Kent 61 Eadbald Kent 62 Æðelwald Kent 63 Eorcenberht and Eormenred Kent 64 Eorcenberht Kent 65 Ecgbehrt I Kent 66 Hlothhere Kent 67 Eadric Kent 68 Mul Kent 69 Swæfheard Kent 70 Oswine, Swæfbehrt, Swæfheard Kent 71 Swæfbehrt, Swæfheard, Wihtred Kent 72 Wihtred Kent 73 Æðelberht II, Ælfric and Eadberht I Kent 74 Æðelberht II and Eardwulf Kent 75 Eadberht II, Eanmund and Sigered Kent 76 Heaberht Kent 77 Heaberht and Ecgbehrt II Kent 78 Ealhmund Kent 79 Ecgbehrt III Præn Kent 80 Cuðred Kent 81 Baldred Kent 82 Ecgbehrt and Æthelwulf Cerdicing 83 Ecgbehrt and Æthelwulf Cerdicing 84 Ecgbehrt and Æthelwulf Cerdicing 85 Æthelwulf and Æðelstan I Cerdicing 86 Æthelwulf Cerdicing 87 Æthelwulf and Æðelberht III Cerdicing 88 Æðelberht III Cerdicing 89 Æthelred I Cerdicing 90 Cearl Mercia 91 Penda Mercia 92 Penda and Eowa Mercia 93 Penda Mercia 94 Penda and Peada Mercia 95 Oswiu Bernicia 96 Wulfhere Mercia 97 Æthelred I Mercia 98 Coenred Mercia 99 Ceolred Mercia 100 Ceolwald Mercia 101 Æthelbald Mercia 102 Beornred Mercia 103 Ludeca Mercia 104 Wiglaf Mercia 105 Wiglaf Mercia 106 Wigmund Mercia 107 Wigstan Mercia 108 Ælfflæd Mercia 109 Beorhtwulf Mercia 110 Burgred Mercia 111 Ceolwulf II Mercia 112 <NA> Mercia 113 Æthelred, Lord of the Mercians Mercia 114 Æthelflæd, Lady of the Mercians Mercia 115 Ælfwynn, Second Lady of the Mercians Mercia 116 Esa Bernicia 117 Eoppa Bernicia 118 Ida Bernicia 119 Glappa Bernicia 120 Adda Bernicia 121 Æthelric Bernicia 122 Theodric Bernicia 123 Frithuwald Bernicia 124 Hussa Bernicia 125 Æthelfrith Bernicia 126 Edwin Deira 127 Eanfrith Bernicia 128 Oswald Bernicia 129 Oswiu Bernicia 130 Oswiu Bernicia 131 Oswiu Bernicia 132 Oswiu Bernicia 133 Ælla Deira 134 Æthelric Deira 135 Osric Deira 136 Oswine Deira 137 Æthelwold Deira 138 Alchfrith Deira 139 Ælfwine Deira 140 Ecgfrith Northumbria 141 Aldfrith Northumbria 142 Eadwulf Northumbria 143 Osred I Northumbria 144 Coenred Northumbria 145 Osric Northumbria 146 Coelwulf Northumbria 147 Coelwulf Northumbria 148 Eadberht Northumbria 149 Oswulf Northumbria 150 Æthelwald Moll Northumbria 151 Alhred Northumbria 152 Æthelred I Northumbria 153 Æthelwald I Northumbria 154 Osred II Northumbria 155 Æthelred I Northumbria 156 Osbald Northumbria 157 Eardwulf Northumbria 158 Ælfwald II Northumbria 159 Eardwulf Northumbria 160 Eanred Northumbria 161 Æthelred II Northumbria 162 Rædwulf Northumbria 163 Æthelred II Northumbria 164 Osberht Northumbria 165 Ælle II Northumbria 166 Osberht Northumbria 167 Ecgberht I Norse 168 Ricsige Norse 169 Ecgberht II Norse 170 Guðroðr Norse 171 Sigfroðr Norse 172 Knútr Norse 173 Æthelwold Norse 174 Hálfdan and Eowils Norse 175 Eadwulf II Norse 176 Ealdred Norse 177 Ragnall ua Ímair Norse 178 Sigtrygg Caech Norse 179 Amlaíb mac Gofraid Norse 180 Amlaíb Cuarán Norse 181 Ragnall mac Gofraid Norse 182 Eric Norse 183 Amlaíb Cuarán Norse 184 Eric Norse 185 Eadred Norse 186 Ælle South Saxon 187 Cissa South Saxon 188 <NA> <NA> 189 Æðelwealh South Saxon 190 Eadwulf South Saxon 191 Ecgwald South Saxon 192 Berhthun South Saxon 193 Andhun South Saxon 194 Noðhelm and Watt Wessex 195 Noðhelm and Bryni Wessex 196 Noðhelm and Osric Wessex 197 Noðhelm and Æðelstan Wessex 198 Æðelstan Wessex 199 <NA> Wessex 200 <NA> <NA> 201 Æðelbehrt Wessex 202 Osmund Wessex 203 Ælfwald, Oslac and Osmund Wessex 204 Ælfwald, Ealdwulf, Oslac and Osmund Mercia 205 Ælfwald, Ealdwulf, Oslac, Osmund and Oswald Mercia 206 Ealdwulf Mercia 207 <NA> <NA> 208 Cerdic Gewissae 209 Cynric Gewissae 210 Ceawlin Gewissae 211 Ceol Gewissae 212 Ceolwulf Gewissae 213 Cynegils Gewissae 214 Cwichelm Gewissae 215 Cenwalh Gewissae 216 Penda Mercia 217 Cenwalh Cerdicing 218 Cenwalh and Seaxburh Cerdicing 219 Cenfus Cerdicing 220 Æscwine Cerdicing 221 Centwine Cerdicing 222 Caedwalla Cerdicing 223 Ine Cerdicing 224 Æthelheard Cerdicing 225 Cuthred Cerdicing 226 Sigeberht Cerdicing 227 Cynewulf Cerdicing 228 Beorhtric Cerdicing 229 Ecgbehrt Cerdicing 230 Æthelbald Cerdicing 231 Æthelbret Cerdicing 232 Alfred the Great Cerdicing 233 Edward the Elder Cerdicing 234 Ælfweard Cerdicing 235 Æthelstan Cerdicing 236 Æthelstan Wessex 237 Edmund I Wessex 238 Eadred Wessex 239 Eadwig Wessex 240 Edgar the Peaceful Wessex 241 Edward the Martyr Wessex 242 Æthelred the Unready Wessex 243 Sweyn Forkbeard Knýtlinga 244 Æthelred the Unready Wessex 245 Edmund Ironside Wessex 246 Cnut Knýtlinga 247 Harold Harefoot Knýtlinga 248 Harthacnut Knýtlinga 249 Edward the Confessor Wessex 250 Harold Godwinson Wessex 251 Edgar the Ætheling Wessex 252 William I Normandy 253 William II Normandy 254 Henry I Normandy 255 Stephen Normandy 256 Matilda Plantagenet/Angevin 257 Stephen Normandy 258 Henry II Plantagenet/Angevin 259 Richard I Plantagenet/Angevin start.of.reign end.of.reign domain length.of.reign.years 1 NA 571 East Anglia NA 2 571 578 East Anglia 7 3 578 616 East Anglia 38 4 616 627 East Anglia 11 5 627 627 East Anglia 0 6 627 630 East Anglia 3 7 630 634 East Anglia 4 8 634 NA East Anglia NA 9 NA 653 East Anglia NA 10 653 655 East Anglia 2 11 655 663 East Anglia 8 12 663 713 East Anglia 50 13 713 749 East Anglia 36 14 749 NA East Anglia NA 15 NA 779 East Anglia NA 16 779 794 East Anglia 15 17 757 785 East Anglia, Mercia 28 18 785 787 East Anglia, Kent, Mercia 2 19 787 796 East Anglia, Kent, Mercia 9 20 796 796 East Anglia, Kent, Mercia 0 21 796 800 East Anglia 4 22 796 798 East Anglia, Kent, Mercia 2 23 798 812 East Anglia, Kent, Mercia 14 24 812 821 East Anglia, Kent, Mercia 9 25 821 823 East Anglia, Kent, Mercia 2 26 823 826 East Anglia, Mercia 3 27 826 845 East Anglia 19 28 845 855 East Anglia 10 29 855 869 East Anglia 14 30 869 875 East Anglia 6 31 875 875 East Anglia 0 32 875 879 East Anglia 4 33 879 890 East Anglia 11 34 890 902 East Anglia 12 35 902 902 East Anglia 0 36 902 918 East Anglia 16 37 527 587 Essex 60 38 587 604 Essex 17 39 604 616 Essex 12 40 616 617 Essex 1 41 617 653 Essex 36 42 653 660 Essex 7 43 660 664 Essex 4 44 664 683 Essex 19 45 683 694 Essex 11 46 694 695 Essex 1 47 695 709 Essex 14 48 709 709 Essex 0 49 709 715 Essex 6 50 715 738 Essex 23 51 738 746 Essex 8 52 746 758 Essex 12 53 758 798 Essex 40 54 798 812 Essex 14 55 NA NA Kent NA 56 NA NA Kent NA 57 NA NA Kent NA 58 512 534 Kent 22 59 534 590 Kent 56 60 590 616 Kent 26 61 616 640 Kent 24 62 NA NA Kent NA 63 640 NA Kent NA 64 NA 664 Kent NA 65 664 673 Kent 9 66 674 685 Kent 11 67 685 686 Kent 1 68 686 687 Kent 1 69 687 689 Kent 2 70 689 690 Kent 1 71 689 692 Kent 3 72 692 725 Kent 33 73 725 748 Kent 23 74 748 762 Kent 14 75 762 764 Kent 2 76 764 765 Kent 1 77 765 779 Kent 14 78 NA 785 Kent NA 79 796 798 Kent 2 80 798 807 Kent 9 81 823 825 Kent 2 82 825 829 Kent, Wessex 4 83 829 830 Kent, Mercia, Wessex 1 84 830 839 Kent, Wessex 9 85 839 851 Kent, Wessex 12 86 851 855 Kent, Wessex 4 87 855 858 Kent, Wessex 3 88 858 866 Kent, Wessex 8 89 866 871 Kent, Wessex 5 90 606 NA Mercia NA 91 626 635 Mercia 9 92 635 642 Mercia 7 93 642 653 Mercia 11 94 653 655 Mercia 2 95 655 658 Mercia, Northumbria 3 96 658 675 Mercia 17 97 675 704 Mercia 29 98 704 709 Mercia 5 99 709 716 Mercia 7 100 716 716 Mercia 0 101 716 757 Mercia 41 102 757 757 Mercia 0 103 826 827 Mercia 1 104 827 829 Mercia 2 105 830 839 Mercia 9 106 839 840 Mercia 1 107 840 840 Mercia 0 108 840 840 Mercia 0 109 840 852 Mercia 12 110 852 874 Mercia 22 111 874 879 Mercia 5 112 879 883 Mercia 4 113 883 911 Mercia 28 114 911 918 Mercia 7 115 918 918 Mercia 0 116 500 520 Bernicia 20 117 520 547 Bernicia 27 118 547 559 Bernicia 12 119 559 560 Bernicia 1 120 560 568 Bernicia 8 121 568 572 Bernicia 4 122 572 579 Bernicia 7 123 579 585 Bernicia 6 124 585 593 Bernicia 8 125 593 616 Northumbria 23 126 616 632 Northumbria 16 127 632 633 Bernicia 1 128 634 642 Northumbria 8 129 642 644 Northumbria 2 130 644 654 Bernicia 10 131 654 655 Northumbria 1 132 658 670 Northumbria 12 133 559 589 Deira 30 134 589 604 Deira 15 135 633 634 Deira 1 136 644 651 Northumbria 7 137 651 654 Northumbria 3 138 656 664 Deira 8 139 670 679 Deira 9 140 670 685 Northumbria 15 141 685 704 Northumbria 19 142 704 705 Northumbria 1 143 705 716 Northumbria 11 144 716 718 Northumbria 2 145 718 729 Northumbria 11 146 729 731 Northumbria 2 147 731 737 Northumbria 6 148 737 758 Northumbria 21 149 758 759 Northumbria 1 150 759 765 Northumbria 6 151 765 774 Northumbria 9 152 774 779 Northumbria 5 153 779 788 Northumbria 9 154 788 790 Northumbria 2 155 790 796 Northumbria 6 156 796 796 Northumbria 0 157 796 806 Northumbria 10 158 806 808 Northumbria 2 159 808 810 Northumbria 2 160 810 841 Northumbria 31 161 841 844 Northumbria 3 162 844 844 Northumbria 0 163 844 849 Northumbria 5 164 849 862 Northumbria 13 165 862 867 Northumbria 5 166 867 867 Northumbria 0 167 867 872 Northumbria 5 168 872 876 Northumbria 4 169 876 878 Northumbria 2 170 878 895 Northumbria 17 171 NA NA Northumbria NA 172 NA NA Northumbria NA 173 900 902 Northumbria 2 174 902 910 Northumbria 8 175 910 913 Northumbria 3 176 913 918 Northumbria 5 177 914 921 Northumbria 7 178 921 927 Northumbria 6 179 939 941 Northumbria 2 180 941 944 Northumbria 3 181 943 944 Northumbria 1 182 947 948 Northumbria 1 183 949 952 Northumbria 3 184 952 954 Northumbria 2 185 954 954 Northumbria 0 186 477 514 Sussex 37 187 514 567 Sussex 53 188 567 660 Sussex 93 189 660 685 Sussex 25 190 683 683 Sussex 0 191 683 685 Sussex 2 192 685 NA Sussex NA 193 685 NA Sussex NA 194 692 700 Sussex 8 195 700 710 Sussex 10 196 710 717 Sussex 7 197 717 717 Sussex 0 198 717 724 Sussex 7 199 724 726 Sussex 2 200 726 740 Sussex 14 201 740 NA Sussex NA 202 760 765 Sussex 5 203 765 771 Sussex 6 204 771 772 Sussex 1 205 772 772 Sussex 0 206 772 791 Sussex 19 207 791 825 Sussex 34 208 519 534 Wessex 15 209 534 560 Wessex 26 210 560 591 Wessex 31 211 591 597 Wessex 6 212 597 611 Wessex 14 213 611 643 Wessex 32 214 626 636 Wessex 10 215 643 645 Wessex 2 216 645 648 Wessex 3 217 648 672 Wessex 24 218 672 674 Wessex 2 219 674 674 Wessex 0 220 674 676 Wessex 2 221 676 685 Wessex 9 222 685 688 Wessex 3 223 688 726 Wessex 38 224 726 740 Wessex 14 225 740 756 Wessex 16 226 756 757 Wessex 1 227 757 786 Wessex 29 228 786 802 Wessex 16 229 802 825 Wessex 23 230 858 860 Wessex 2 231 860 865 Wessex 5 232 871 899 Wessex 28 233 899 924 Wessex 25 234 924 924 Wessex 0 235 924 927 Wessex 3 236 927 939 England 12 237 939 946 England 7 238 946 955 England 9 239 955 959 England 4 240 959 975 England 16 241 975 978 England 3 242 978 1013 England 35 243 1013 1014 England 1 244 1014 1016 England 2 245 1016 1016 England 0 246 1016 1035 England 19 247 1035 1040 England 5 248 1040 1042 England 2 249 1042 1066 England 24 250 1066 1066 England 0 251 1066 1066 England 0 252 1066 1087 England 21 253 1087 1100 England 13 254 1100 1135 England 35 255 1135 1141 England 6 256 1141 1141 England 0 257 1141 1154 England 13 258 1154 1189 England 35 259 1189 1199 England 10 reign.was.more.than.30.years 1 NA 2 FALSE 3 TRUE 4 FALSE 5 FALSE 6 FALSE 7 FALSE 8 NA 9 NA 10 FALSE 11 FALSE 12 TRUE 13 TRUE 14 NA 15 NA 16 FALSE 17 FALSE 18 FALSE 19 FALSE 20 FALSE 21 FALSE 22 FALSE 23 FALSE 24 FALSE 25 FALSE 26 FALSE 27 FALSE 28 FALSE 29 FALSE 30 FALSE 31 FALSE 32 FALSE 33 FALSE 34 FALSE 35 FALSE 36 FALSE 37 TRUE 38 FALSE 39 FALSE 40 FALSE 41 TRUE 42 FALSE 43 FALSE 44 FALSE 45 FALSE 46 FALSE 47 FALSE 48 FALSE 49 FALSE 50 FALSE 51 FALSE 52 FALSE 53 TRUE 54 FALSE 55 NA 56 NA 57 NA 58 FALSE 59 TRUE 60 FALSE 61 FALSE 62 NA 63 NA 64 NA 65 FALSE 66 FALSE 67 FALSE 68 FALSE 69 FALSE 70 FALSE 71 FALSE 72 TRUE 73 FALSE 74 FALSE 75 FALSE 76 FALSE 77 FALSE 78 NA 79 FALSE 80 FALSE 81 FALSE 82 FALSE 83 FALSE 84 FALSE 85 FALSE 86 FALSE 87 FALSE 88 FALSE 89 FALSE 90 NA 91 FALSE 92 FALSE 93 FALSE 94 FALSE 95 FALSE 96 FALSE 97 FALSE 98 FALSE 99 FALSE 100 FALSE 101 TRUE 102 FALSE 103 FALSE 104 FALSE 105 FALSE 106 FALSE 107 FALSE 108 FALSE 109 FALSE 110 FALSE 111 FALSE 112 FALSE 113 FALSE 114 FALSE 115 FALSE 116 FALSE 117 FALSE 118 FALSE 119 FALSE 120 FALSE 121 FALSE 122 FALSE 123 FALSE 124 FALSE 125 FALSE 126 FALSE 127 FALSE 128 FALSE 129 FALSE 130 FALSE 131 FALSE 132 FALSE 133 FALSE 134 FALSE 135 FALSE 136 FALSE 137 FALSE 138 FALSE 139 FALSE 140 FALSE 141 FALSE 142 FALSE 143 FALSE 144 FALSE 145 FALSE 146 FALSE 147 FALSE 148 FALSE 149 FALSE 150 FALSE 151 FALSE 152 FALSE 153 FALSE 154 FALSE 155 FALSE 156 FALSE 157 FALSE 158 FALSE 159 FALSE 160 TRUE 161 FALSE 162 FALSE 163 FALSE 164 FALSE 165 FALSE 166 FALSE 167 FALSE 168 FALSE 169 FALSE 170 FALSE 171 NA 172 NA 173 FALSE 174 FALSE 175 FALSE 176 FALSE 177 FALSE 178 FALSE 179 FALSE 180 FALSE 181 FALSE 182 FALSE 183 FALSE 184 FALSE 185 FALSE 186 TRUE 187 TRUE 188 TRUE 189 FALSE 190 FALSE 191 FALSE 192 NA 193 NA 194 FALSE 195 FALSE 196 FALSE 197 FALSE 198 FALSE 199 FALSE 200 FALSE 201 NA 202 FALSE 203 FALSE 204 FALSE 205 FALSE 206 FALSE 207 TRUE 208 FALSE 209 FALSE 210 TRUE 211 FALSE 212 FALSE 213 TRUE 214 FALSE 215 FALSE 216 FALSE 217 FALSE 218 FALSE 219 FALSE 220 FALSE 221 FALSE 222 FALSE 223 TRUE 224 FALSE 225 FALSE 226 FALSE 227 FALSE 228 FALSE 229 FALSE 230 FALSE 231 FALSE 232 FALSE 233 FALSE 234 FALSE 235 FALSE 236 FALSE 237 FALSE 238 FALSE 239 FALSE 240 FALSE 241 FALSE 242 TRUE 243 FALSE 244 FALSE 245 FALSE 246 FALSE 247 FALSE 248 FALSE 249 FALSE 250 FALSE 251 FALSE 252 FALSE 253 FALSE 254 TRUE 255 FALSE 256 FALSE 257 FALSE 258 TRUE 259 FALSE","tags":"R Stats","url":"http://chrisalbon.com/r-stats/managing-data-frames.html"},{"title":"Manipulating Character Strings (i.e. text)","loc":"http://chrisalbon.com/r-stats/manipulate-character-strings.html","text":"R can handle a number of types of data, including numbers and text. In R, the text is refered to as character strings and is always wrapped in double quotation marks. In other words, if something is inside quotation marks, it is treated as text. R has a number of functions avaliable for manipulating text. # create simulated district crime name district <- factor ( c ( \"NORTH\" , \"NORTHWEST\" , \"CENTRAL\" , \"SOUTH\" , \"SOUTHWEST\" , \"WESTERN\" )) Count the number of characters in each individual character string in an object # display each district's name levels ( district ) [1] \"CENTRAL\" \"NORTH\" \"NORTHWEST\" \"SOUTH\" \"SOUTHWEST\" \"WESTERN\" You can also display text without quotes, although it is rare you would want to do so. # display each police district's name levels ( district ) [1] \"CENTRAL\" \"NORTH\" \"NORTHWEST\" \"SOUTH\" \"SOUTHWEST\" \"WESTERN\" # display each police district's name without quotes noquote ( levels ( district )) [1] CENTRAL NORTH NORTHWEST SOUTH SOUTHWEST WESTERN R can add characters to a vector of character strings using the paste function # Add \"DISTRICT\" to the name of each police district paste ( levels ( district ), \"DISTRICT\" ) [1] \"CENTRAL DISTRICT\" \"NORTH DISTRICT\" \"NORTHWEST DISTRICT\" [4] \"SOUTH DISTRICT\" \"SOUTHWEST DISTRICT\" \"WESTERN DISTRICT\" Extracting Segments Of Character Strings using Substring Function # Extract characters from district names, starting at the 1st character and ending at the 5th character substr ( district , 1 , 5 ) [1] \"NORTH\" \"NORTH\" \"CENTR\" \"SOUTH\" \"SOUTH\" \"WESTE\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/manipulate-character-strings.html"},{"title":"Match A Symbol","loc":"http://chrisalbon.com/regex/match_a_symbol.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '$100' Apply regex # Find all instances of the exact match '$' re . findall ( r'\\$' , text ) ['$']","tags":"Regex","url":"http://chrisalbon.com/regex/match_a_symbol.html"},{"title":"Match A Unicode Character","loc":"http://chrisalbon.com/regex/match_a_unicode_character.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'Microsoft™.' Apply regex # Find any unicode character for a trademark re . findall ( r'\\u2122' , text ) ['™']","tags":"Regex","url":"http://chrisalbon.com/regex/match_a_unicode_character.html"},{"title":"Match A Word","loc":"http://chrisalbon.com/regex/match_a_word.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find any word of three letters re . findall ( r'\\b...\\b' , text ) ['The', 'fox', 'the']","tags":"Regex","url":"http://chrisalbon.com/regex/match_a_word.html"},{"title":"Match Any Character","loc":"http://chrisalbon.com/regex/match_any_character.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find anything with a 'T' and then the next two characters re . findall ( r'T..' , text ) ['The']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_character.html"},{"title":"Match Any Of A List Of Characters","loc":"http://chrisalbon.com/regex/match_any_of_a_list_of_symbols.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find all instances of any vowel re . findall ( r'[aeiou]' , text ) ['e', 'u', 'i', 'o', 'o', 'u', 'e', 'o', 'e', 'e', 'a', 'o', 'e', 'a']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_of_a_list_of_symbols.html"},{"title":"Match Any Of A Series Of Options","loc":"http://chrisalbon.com/regex/match_any_of_series_of_characters.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find any of fox, snake, or bear re . findall ( r'fox|snake|bear' , text ) ['fox', 'bear']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_of_series_of_characters.html"},{"title":"Match Any Of A Series Of Words","loc":"http://chrisalbon.com/regex/match_any_of_series_of_words.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find any of fox, snake, or bear re . findall ( r'\\b(fox|snake|bear)\\b' , text ) ['fox', 'bear']","tags":"Regex","url":"http://chrisalbon.com/regex/match_any_of_series_of_words.html"},{"title":"Match Dates","loc":"http://chrisalbon.com/regex/match_dates.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My birthday is 09/15/1983. My brother \\' s birthday is 01/01/01. My other two brothers have birthdays of 9/3/2001 and 09/1/83.' Apply regex # Find any text that fits the regex re . findall ( r'\\b[0-3]?[0-9]/[0-3]?[0-9]/(?:[0-9]{2})?[0-9]{2}\\b' , text ) ['09/15/1983', '01/01/01', '9/3/2001', '09/1/83']","tags":"Regex","url":"http://chrisalbon.com/regex/match_dates.html"},{"title":"Match Email Addresses","loc":"http://chrisalbon.com/regex/match_email_addresses.html","text":"Based on: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My email is chris@hotmail.com, thanks! No, I am at bob@data.ninja.' Apply regex # Find all email addresses re . findall ( r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9]+' , text ) # Explanation: # This regex has three parts # [a-zA-Z0-9_.+-]+ Matches a word (the username) of any length # @[a-zA-Z0-9-]+ Matches a word (the domain name) of any length # \\.[a-zA-Z0-9-.]+ Matches a word (the TLD) of any length ['chris@hotmail.com', 'bob@data.ninja']","tags":"Regex","url":"http://chrisalbon.com/regex/match_email_addresses.html"},{"title":"Match Exact Text","loc":"http://chrisalbon.com/regex/match_exact_text.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'The quick brown fox jumped over the lazy brown bear.' Apply regex # Find all instances of the exact match 'The' re . findall ( r'The' , text ) ['The']","tags":"Regex","url":"http://chrisalbon.com/regex/match_exact_text.html"},{"title":"Match Integers Of Any Length","loc":"http://chrisalbon.com/regex/match_integers_of_any_length.html","text":"Based on: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '21 scouts and 3 tanks fought against 4,003 protestors.' Apply regex # Find any character block that is a integer of any length re . findall ( r'[1-9](?:\\d{0,2})(?:,\\d{3})*(?:\\.\\d*[1-9])?|0?\\.\\d*[1-9]|0' , text ) ['21', '3', '4,003'] Explanation from Justin Morgan [1-9](?:\\d{0,2}) #A sequence of 1-3 numerals not starting with 0 (?:,\\d{3})* #Any number of three-digit groups, each preceded by a comma (?:\\.\\d*[1-9])? #Optionally, a decimal point followed by any number of digits not ending in 0 | #OR... 0?\\.\\d*[1-9] #Only the decimal portion, optionally preceded by a 0 | #OR... 0 #Zero.","tags":"Regex","url":"http://chrisalbon.com/regex/match_integers_of_any_length.html"},{"title":"Match Text Between HTML Tags","loc":"http://chrisalbon.com/regex/match_text_between_html_tags.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '<p>The quick brown fox.</p><p>The lazy brown bear.</p>' Apply regex # Find any text between '<p>' and '</p>' re . findall ( r'<p>(.*?)</p>' , text ) ['The quick brown fox.', 'The lazy brown bear.']","tags":"Regex","url":"http://chrisalbon.com/regex/match_text_between_html_tags.html"},{"title":"Match Times","loc":"http://chrisalbon.com/regex/match_times.html","text":"Based on: StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'Chris: 12:34am. Steve: 16:30' Apply regex # Find any text that fits the regex re . findall ( r'([0-1]\\d:[0-5]\\d)\\s*(?:AM|PM)?' , text ) ['12:34', '16:30']","tags":"Regex","url":"http://chrisalbon.com/regex/match_times.html"},{"title":"Match URLs","loc":"http://chrisalbon.com/regex/match_urls.html","text":"StackOverflow Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My blog is http://www.chrisalbon.com and not http://chrisalbon.com' Apply regex # Find any ISBN-10 or ISBN-13 number re . findall ( r'(http|ftp|https):\\/\\/([\\w\\-_]+(?:(?:\\.[\\w\\-_]+)+))([\\w\\-\\.,@?&#94;=%&amp;:/~\\+#]*[\\w\\-\\@?&#94;=%&amp;/~\\+#])?' , text ) [('http', 'www.chrisalbon.com', ''), ('http', 'chrisalbon.com', '')]","tags":"Regex","url":"http://chrisalbon.com/regex/match_urls.html"},{"title":"Match US Phone Numbers","loc":"http://chrisalbon.com/regex/match_us_phone_numbers.html","text":"Based on: Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'My phone number is 415-333-3922. His phone number is 4239389283' Apply regex # Find any text that fits the regex re . findall ( r'\\(?([2-9][0-8][0-9])\\)?[-.●]?([2-9][0-9]{2})[-.●]?([0-9]{4})' , text ) [('415', '333', '3922'), ('423', '938', '9283')]","tags":"Regex","url":"http://chrisalbon.com/regex/match_us_phone_numbers.html"},{"title":"Match US and UK Spellings","loc":"http://chrisalbon.com/regex/match_us_uk_spellings.html","text":"Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'It\\s center and not centre.' Apply regex # Find any ISBN-10 or ISBN-13 number re . findall ( r'\\bcent(?:er|re)\\b' , text ) ['center', 'centre']","tags":"Regex","url":"http://chrisalbon.com/regex/match_us_uk_spellings.html"},{"title":"Match Words With A Certain Ending","loc":"http://chrisalbon.com/regex/match_words_with_certain_ending.html","text":"Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = 'Capitalism, Communism, Neorealism, Liberalism' Apply regex # Find any word ending in 'ism' re . findall ( r'\\b\\w*ism\\b' , text ) # Specific: # \\b - start of the word # \\w* - a word of any length # ism\\b - with 'ism'at the end ['Capitalism', 'Communism', 'Neorealism', 'Liberalism']","tags":"Regex","url":"http://chrisalbon.com/regex/match_words_with_certain_ending.html"},{"title":"Match ZIP Codes","loc":"http://chrisalbon.com/regex/match_zip_codes.html","text":"Regular Expressions Cookbook Preliminaries # Load regex package import re Create some text # Create a variable containing a text string text = '3829 South Ave Street, Pheonix, AZ 34923' Apply regex # Find any ISBN-10 or ISBN-13 number re . findall ( r'[0-9]{5}(?:-[0-9]{4})?' , text ) ['34923']","tags":"Regex","url":"http://chrisalbon.com/regex/match_zip_codes.html"},{"title":"Mathematical Operations","loc":"http://chrisalbon.com/python/math_operations.html","text":"Import the math module import math Display the value of pi. math . pi 3.141592653589793 Display the value of e. math . e 2.718281828459045 Sine, cosine, and tangent math . sin ( 2 * math . pi / 180 ) 0.03489949670250097 Exponent 2 ** 4 , pow ( 2 , 4 ) (16, 16) Absolute value abs ( - 20 ) 20 Summation sum (( 1 , 2 , 3 , 4 )) 10 Minimum min ( 3 , 9 , 10 , 12 ) 3 Maximum max ( 3 , 5 , 10 , 15 ) 15 Floor math . floor ( 2.949 ) 2 Truncate (drop decimal digits) math . trunc ( 32.09292 ) 32 Truncate (integer conversion) int ( 3.292838 ) 3 Round to an integrer round ( 2.943 ), round ( 2.499 ) (3, 2) Round by 2 digits round ( 2.569 , 2 ) 2.57","tags":"Python","url":"http://chrisalbon.com/python/math_operations.html"},{"title":"Back To Back Bar Plot In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_back_to_back_bar_plot.html","text":"Note: Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # input data, specifically the second and # third rows, skipping the first column x1 = df . ix [ 1 , 1 :] x2 = df . ix [ 2 , 1 :] # Create the bar labels bar_labels = [ 'Pre Score' , 'Mid Score' , 'Post Score' ] # Create a figure fig = plt . figure ( figsize = ( 8 , 6 )) # Set the y position y_pos = np . arange ( len ( x1 )) y_pos = [ x for x in y_pos ] plt . yticks ( y_pos , bar_labels , fontsize = 10 ) # Create a horizontal bar in the position y_pos plt . barh ( y_pos , # using x1 data x1 , # that is centered align = 'center' , # with alpha 0.4 alpha = 0.4 , # and color green color = '#263F13' ) # Create a horizontal bar in the position y_pos plt . barh ( y_pos , # using NEGATIVE x2 data - x2 , # that is centered align = 'center' , # with alpha 0.4 alpha = 0.4 , # and color green color = '#77A61D' ) # annotation and labels plt . xlabel ( 'Tina \\' s Score: Light Green. Molly \\' s Score: Dark Green' ) t = plt . title ( 'Comparison of Molly and Tina \\' s Score' ) plt . ylim ([ - 1 , len ( x1 ) + 0.1 ]) plt . xlim ([ - max ( x2 ) - 10 , max ( x1 ) + 10 ]) plt . grid () plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_back_to_back_bar_plot.html"},{"title":"Bar Plot In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_bar_plot.html","text":"Note: Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Create a list of the mean scores for each variable mean_values = [ df [ 'pre_score' ] . mean (), df [ 'mid_score' ] . mean (), df [ 'post_score' ] . mean ()] # Create a list of variances, which are set at .25 above and below the score variance = [ df [ 'pre_score' ] . mean () * 0.25 , df [ 'pre_score' ] . mean () * 0.25 , df [ 'pre_score' ] . mean () * 0.25 ] # Set the bar labels bar_labels = [ 'Pre Score' , 'Mid Score' , 'Post Score' ] # Create the x position of the bars x_pos = list ( range ( len ( bar_labels ))) # Create the plot bars # In x position plt . bar ( x_pos , # using the data from the mean_values mean_values , # with a y-error lines set at variance yerr = variance , # aligned in the center align = 'center' , # with color color = '#FFC222' , # alpha 0.5 alpha = 0.5 ) # add a grid plt . grid () # set height of the y-axis max_y = max ( zip ( mean_values , variance )) # returns a tuple, here: (3, 5) plt . ylim ([ 0 , ( max_y [ 0 ] + max_y [ 1 ]) * 1.1 ]) # set axes labels and title plt . ylabel ( 'Score' ) plt . xticks ( x_pos , bar_labels ) plt . title ( 'Mean Scores For Each Test' ) plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_bar_plot.html"},{"title":"Group Bar Plot In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_grouped_bar_plot.html","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Setting the positions and width for the bars pos = list ( range ( len ( df [ 'pre_score' ]))) width = 0.25 # Plotting the bars fig , ax = plt . subplots ( figsize = ( 10 , 5 )) # Create a bar with pre_score data, # in position pos, plt . bar ( pos , #using df['pre_score'] data, df [ 'pre_score' ], # of width width , # with alpha 0.5 alpha = 0.5 , # with color color = '#EE3224' , # with label the first value in first_name label = df [ 'first_name' ][ 0 ]) # Create a bar with mid_score data, # in position pos + some width buffer, plt . bar ([ p + width for p in pos ], #using df['mid_score'] data, df [ 'mid_score' ], # of width width , # with alpha 0.5 alpha = 0.5 , # with color color = '#F78F1E' , # with label the second value in first_name label = df [ 'first_name' ][ 1 ]) # Create a bar with post_score data, # in position pos + some width buffer, plt . bar ([ p + width * 2 for p in pos ], #using df['post_score'] data, df [ 'post_score' ], # of width width , # with alpha 0.5 alpha = 0.5 , # with color color = '#FFC222' , # with label the third value in first_name label = df [ 'first_name' ][ 2 ]) # Set the y axis label ax . set_ylabel ( 'Score' ) # Set the chart's title ax . set_title ( 'Test Subject Scores' ) # Set the position of the x ticks ax . set_xticks ([ p + 1.5 * width for p in pos ]) # Set the labels for the x ticks ax . set_xticklabels ( df [ 'first_name' ]) # Setting the x-axis and y-axis limits plt . xlim ( min ( pos ) - width , max ( pos ) + width * 4 ) plt . ylim ([ 0 , max ( df [ 'pre_score' ] + df [ 'mid_score' ] + df [ 'post_score' ])] ) # Adding the legend and showing the plot plt . legend ([ 'Pre Score' , 'Mid Score' , 'Post Score' ], loc = 'upper left' ) plt . grid () plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_grouped_bar_plot.html"},{"title":"Histograms In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_histogram.html","text":"Note: Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np import math # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create dataframe df = pd . read_csv ( 'https://www.dropbox.com/s/52cb7kcflr8qm2u/5kings_battles_v1.csv?dl=1' ) df . head () name year battle_number attacker_king defender_king attacker_1 attacker_2 attacker_3 attacker_4 defender_1 defender_2 defender_3 defender_4 attacker_outcome battle_type major_death major_capture attacker_size defender_size attacker_commander defender_commander summer location region note 0 Battle of the Golden Tooth 298 1 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 1 0 15000 4000 Jaime Lannister Clement Piper, Vance 1 Golden Tooth The Westerlands NaN 1 Battle at the Mummer's Ford 298 2 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Baratheon NaN NaN NaN win ambush 1 0 NaN 120 Gregor Clegane Beric Dondarrion 1 Mummer's Ford The Riverlands NaN 2 Battle of Riverrun 298 3 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 0 1 15000 10000 Jaime Lannister, Andros Brax Edmure Tully, Tytos Blackwood 1 Riverrun The Riverlands NaN 3 Battle of the Green Fork 298 4 Robb Stark Joffrey/Tommen Baratheon Stark NaN NaN NaN Lannister NaN NaN NaN loss pitched battle 1 1 18000 20000 Roose Bolton, Wylis Manderly, Medger Cerwyn, H... Tywin Lannister, Gregor Clegane, Kevan Lannist... 1 Green Fork The Riverlands NaN 4 Battle of the Whispering Wood 298 5 Robb Stark Joffrey/Tommen Baratheon Stark Tully NaN NaN Lannister NaN NaN NaN win ambush 1 1 1875 6000 Robb Stark, Brynden Tully Jaime Lannister 1 Whispering Wood The Riverlands NaN Make plot with bins of fixed size # Make two variables of the attacker and defender size, but leaving out # cases when there are over 10000 attackers data1 = df [ 'attacker_size' ][ df [ 'attacker_size' ] < 90000 ] data2 = df [ 'defender_size' ][ df [ 'attacker_size' ] < 90000 ] # Create bins of 2000 each bins = np . arange ( data1 . min (), data2 . max (), 2000 ) # fixed bin size # Plot a histogram of attacker size plt . hist ( data1 , bins = bins , alpha = 0.5 , color = '#EDD834' , label = 'Attacker' ) # Plot a histogram of defender size plt . hist ( data2 , bins = bins , alpha = 0.5 , color = '#887E43' , label = 'Defender' ) # Set the x and y boundaries of the figure plt . ylim ([ 0 , 10 ]) # Set the title and labels plt . title ( 'Histogram of Attacker and Defender Size' ) plt . xlabel ( 'Number of troops' ) plt . ylabel ( 'Number of battles' ) plt . legend ( loc = 'upper right' ) plt . show () Make plot with fixed number of bins # Make two variables of the attacker and defender size, but leaving out # cases when there are over 10000 attackers data1 = df [ 'attacker_size' ][ df [ 'attacker_size' ] < 90000 ] data2 = df [ 'defender_size' ][ df [ 'attacker_size' ] < 90000 ] # Create 10 bins with the minimum # being the smallest value of data1 and data2 bins = np . linspace ( min ( data1 + data2 ), # the max being the highest value max ( data1 + data2 ), # and divided into 10 bins 10 ) # Plot a histogram of attacker size plt . hist ( data1 , # with bins defined as bins = bins , # with alpha alpha = 0.5 , # with color color = '#EDD834' , # labelled attacker label = 'Attacker' ) # Plot a histogram of defender size plt . hist ( data2 , # with bins defined as bins = bins , # with alpha alpha = 0.5 , # with color color = '#887E43' , # labeled defender label = 'Defender' ) # Set the x and y boundaries of the figure plt . ylim ([ 0 , 10 ]) # Set the title and labels plt . title ( 'Histogram of Attacker and Defender Size' ) plt . xlabel ( 'Number of troops' ) plt . ylabel ( 'Number of battles' ) plt . legend ( loc = 'upper right' ) plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_histogram.html"},{"title":"Stacked Percentage Bar Plot In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html","text":"Note: Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Create a figure with a single subplot f , ax = plt . subplots ( 1 , figsize = ( 10 , 5 )) # Set bar width at 1 bar_width = 1 # positions of the left bar-boundaries bar_l = [ i for i in range ( len ( df [ 'pre_score' ]))] # positions of the x-axis ticks (center of the bars as bar labels) tick_pos = [ i + ( bar_width / 2 ) for i in bar_l ] # Create the total score for each participant totals = [ i + j + k for i , j , k in zip ( df [ 'pre_score' ], df [ 'mid_score' ], df [ 'post_score' ])] # Create the percentage of the total score the pre_score value for each participant was pre_rel = [ i / j * 100 for i , j in zip ( df [ 'pre_score' ], totals )] # Create the percentage of the total score the mid_score value for each participant was mid_rel = [ i / j * 100 for i , j in zip ( df [ 'mid_score' ], totals )] # Create the percentage of the total score the post_score value for each participant was post_rel = [ i / j * 100 for i , j in zip ( df [ 'post_score' ], totals )] # Create a bar chart in position bar_1 ax . bar ( bar_l , # using pre_rel data pre_rel , # labeled label = 'Pre Score' , # with alpha alpha = 0.9 , # with color color = '#019600' , # with bar width width = bar_width , # with border color edgecolor = 'white' ) # Create a bar chart in position bar_1 ax . bar ( bar_l , # using mid_rel data mid_rel , # with pre_rel bottom = pre_rel , # labeled label = 'Mid Score' , # with alpha alpha = 0.9 , # with color color = '#3C5F5A' , # with bar width width = bar_width , # with border color edgecolor = 'white' ) # Create a bar chart in position bar_1 ax . bar ( bar_l , # using post_rel data post_rel , # with pre_rel and mid_rel on bottom bottom = [ i + j for i , j in zip ( pre_rel , mid_rel )], # labeled label = 'Post Score' , # with alpha alpha = 0.9 , # with color color = '#219AD8' , # with bar width width = bar_width , # with border color edgecolor = 'white' ) # Set the ticks to be first names plt . xticks ( tick_pos , df [ 'first_name' ]) ax . set_ylabel ( \"Percentage\" ) ax . set_xlabel ( \"\" ) # Let the borders of the graphic plt . xlim ([ min ( tick_pos ) - bar_width , max ( tick_pos ) + bar_width ]) plt . ylim ( - 10 , 110 ) # rotate axis labels plt . setp ( plt . gca () . get_xticklabels (), rotation = 45 , horizontalalignment = 'right' ) # shot plot plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_percentage_stacked_bar_plot.html"},{"title":"Pie Chart In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_pie_chart.html","text":"Note: Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt Create dataframe raw_data = { 'officer_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'jan_arrests' : [ 4 , 24 , 31 , 2 , 3 ], 'feb_arrests' : [ 25 , 94 , 57 , 62 , 70 ], 'march_arrests' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'officer_name' , 'jan_arrests' , 'feb_arrests' , 'march_arrests' ]) df officer_name jan_arrests feb_arrests march_arrests 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 # Create a column with the total arrests for each officer df [ 'total_arrests' ] = df [ 'jan_arrests' ] + df [ 'feb_arrests' ] + df [ 'march_arrests' ] df officer_name jan_arrests feb_arrests march_arrests total_arrests 0 Jason 4 25 5 34 1 Molly 24 94 43 161 2 Tina 31 57 23 111 3 Jake 2 62 23 87 4 Amy 3 70 51 124 Make plot # Create a list of colors (from iWantHue) colors = [ \"#E13F29\" , \"#D69A80\" , \"#D63B59\" , \"#AE5552\" , \"#CB5C3B\" , \"#EB8076\" , \"#96624E\" ] # Create a pie chart plt . pie ( # using data total)arrests df [ 'total_arrests' ], # with the labels being officer names labels = df [ 'officer_name' ], # with no shadows shadow = False , # with colors colors = colors , # with one slide exploded out explode = ( 0 , 0 , 0 , 0 , 0.15 ), # with the start angle at 90% startangle = 90 , # with the percent listed as a fraction autopct = ' %1.1f%% ' , ) # View the plot drop above plt . axis ( 'equal' ) # View the plot plt . tight_layout () plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_pie_chart.html"},{"title":"Making A Matplotlib Scatterplot From A Pandas Dataframe","loc":"http://chrisalbon.com/python/matplotlib_scatterplot_from_pandas.html","text":"Note: Based on: StackOverflow . import modules % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'female' : [ 0 , 1 , 1 , 0 , 1 ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'female' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age female preTestScore postTestScore 0 Jason Miller 42 0 4 25 1 Molly Jacobson 52 1 24 94 2 Tina Ali 36 1 31 57 3 Jake Milner 24 0 2 62 4 Amy Cooze 73 1 3 70 Scatterplot of preTestScore and postTestScore, with the size of each point determined by age plt . scatter ( df . preTestScore , df . postTestScore , s = df . age ) <matplotlib.collections.PathCollection at 0x10841df50> Scatterplot of preTestScore and postTestScore with the size = 300 and the color determined by sex plt . scatter ( df . preTestScore , df . postTestScore , s = 300 , c = 'm' ) <matplotlib.collections.PathCollection at 0x1086dcdd0>","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_scatterplot_from_pandas.html"},{"title":"Matplotlib, A Simple Example","loc":"http://chrisalbon.com/python/matplotlib_simple_example.html","text":"Tell iPython to load matplotlib and display all visuals created inline (that is, on this page) % matplotlib inline Import matplotlib's pyplot module import matplotlib.pyplot as pyplot Create a simple plot pyplot . plot ([ 1.6 , 2.7 ]) [<matplotlib.lines.Line2D at 0x10870d2d0>]","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_simple_example.html"},{"title":"Scatterplot In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_simple_scatterplot.html","text":"Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create dataframe df = pd . read_csv ( 'https://raw.githubusercontent.com/chrisalbon/war_of_the_five_kings_dataset/master/5kings_battles_v1.csv' ) df . head () name year battle_number attacker_king defender_king attacker_1 attacker_2 attacker_3 attacker_4 defender_1 defender_2 defender_3 defender_4 attacker_outcome battle_type major_death major_capture attacker_size defender_size attacker_commander defender_commander summer location region note 0 Battle of the Golden Tooth 298 1 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 1.0 0.0 15000.0 4000.0 Jaime Lannister Clement Piper, Vance 1.0 Golden Tooth The Westerlands NaN 1 Battle at the Mummer's Ford 298 2 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Baratheon NaN NaN NaN win ambush 1.0 0.0 NaN 120.0 Gregor Clegane Beric Dondarrion 1.0 Mummer's Ford The Riverlands NaN 2 Battle of Riverrun 298 3 Joffrey/Tommen Baratheon Robb Stark Lannister NaN NaN NaN Tully NaN NaN NaN win pitched battle 0.0 1.0 15000.0 10000.0 Jaime Lannister, Andros Brax Edmure Tully, Tytos Blackwood 1.0 Riverrun The Riverlands NaN 3 Battle of the Green Fork 298 4 Robb Stark Joffrey/Tommen Baratheon Stark NaN NaN NaN Lannister NaN NaN NaN loss pitched battle 1.0 1.0 18000.0 20000.0 Roose Bolton, Wylis Manderly, Medger Cerwyn, H... Tywin Lannister, Gregor Clegane, Kevan Lannist... 1.0 Green Fork The Riverlands NaN 4 Battle of the Whispering Wood 298 5 Robb Stark Joffrey/Tommen Baratheon Stark Tully NaN NaN Lannister NaN NaN NaN win ambush 1.0 1.0 1875.0 6000.0 Robb Stark, Brynden Tully Jaime Lannister 1.0 Whispering Wood The Riverlands NaN Make plot # Create a figure plt . figure ( figsize = ( 10 , 8 )) # Create a scatterplot of, # attacker size in year 298 as the x axis plt . scatter ( df [ 'attacker_size' ][ df [ 'year' ] == 298 ], # attacker size in year 298 as the y axis df [ 'defender_size' ][ df [ 'year' ] == 298 ], # the marker as marker = 'x' , # the color color = 'b' , # the alpha alpha = 0.7 , # with size s = 124 , # labelled this label = 'Year 298' ) # attacker size in year 299 as the x axis plt . scatter ( df [ 'attacker_size' ][ df [ 'year' ] == 299 ], # defender size in year 299 as the y axis df [ 'defender_size' ][ df [ 'year' ] == 299 ], # the marker as marker = 'o' , # the color color = 'r' , # the alpha alpha = 0.7 , # with size s = 124 , # labelled this label = 'Year 299' ) # attacker size in year 300 as the x axis plt . scatter ( df [ 'attacker_size' ][ df [ 'year' ] == 300 ], # defender size in year 300 as the y axis df [ 'defender_size' ][ df [ 'year' ] == 300 ], # the marker as marker = '&#94;' , # the color color = 'g' , # the alpha alpha = 0.7 , # with size s = 124 , # labelled this label = 'Year 300' ) # Chart title plt . title ( 'Battles Of The War Of The Five Kings' ) # y label plt . ylabel ( 'Defender Size' ) # x label plt . xlabel ( 'Attacker Size' ) # and a legend plt . legend ( loc = 'upper right' ) # set the figure boundaries plt . xlim ([ min ( df [ 'attacker_size' ]) - 1000 , max ( df [ 'attacker_size' ]) + 1000 ]) plt . ylim ([ min ( df [ 'defender_size' ]) - 1000 , max ( df [ 'defender_size' ]) + 1000 ]) plt . show ()","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_simple_scatterplot.html"},{"title":"Stacked Bar Plot In MatPlotLib","loc":"http://chrisalbon.com/python/matplotlib_stacked_bar_plot.html","text":"Note: Based on: Sebastian Raschka . Preliminaries % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'pre_score' : [ 4 , 24 , 31 , 2 , 3 ], 'mid_score' : [ 25 , 94 , 57 , 62 , 70 ], 'post_score' : [ 5 , 43 , 23 , 23 , 51 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'pre_score' , 'mid_score' , 'post_score' ]) df first_name pre_score mid_score post_score 0 Jason 4 25 5 1 Molly 24 94 43 2 Tina 31 57 23 3 Jake 2 62 23 4 Amy 3 70 51 Make plot # Create the general blog and the \"subplots\" i.e. the bars f , ax1 = plt . subplots ( 1 , figsize = ( 10 , 5 )) # Set the bar width bar_width = 0.75 # positions of the left bar-boundaries bar_l = [ i + 1 for i in range ( len ( df [ 'pre_score' ]))] # positions of the x-axis ticks (center of the bars as bar labels) tick_pos = [ i + ( bar_width / 2 ) for i in bar_l ] # Create a bar plot, in position bar_1 ax1 . bar ( bar_l , # using the pre_score data df [ 'pre_score' ], # set the width width = bar_width , # with the label pre score label = 'Pre Score' , # with alpha 0.5 alpha = 0.5 , # with color color = '#F4561D' ) # Create a bar plot, in position bar_1 ax1 . bar ( bar_l , # using the mid_score data df [ 'mid_score' ], # set the width width = bar_width , # with pre_score on the bottom bottom = df [ 'pre_score' ], # with the label mid score label = 'Mid Score' , # with alpha 0.5 alpha = 0.5 , # with color color = '#F1911E' ) # Create a bar plot, in position bar_1 ax1 . bar ( bar_l , # using the post_score data df [ 'post_score' ], # set the width width = bar_width , # with pre_score and mid_score on the bottom bottom = [ i + j for i , j in zip ( df [ 'pre_score' ], df [ 'mid_score' ])], # with the label post score label = 'Post Score' , # with alpha 0.5 alpha = 0.5 , # with color color = '#F1BD1A' ) # set the x ticks with names plt . xticks ( tick_pos , df [ 'first_name' ]) # Set the label and legends ax1 . set_ylabel ( \"Total Score\" ) ax1 . set_xlabel ( \"Test Subject\" ) plt . legend ( loc = 'upper left' ) # Set a buffer around the edge plt . xlim ([ min ( tick_pos ) - bar_width , max ( tick_pos ) + bar_width ]) (0.625, 6.125)","tags":"Python","url":"http://chrisalbon.com/python/matplotlib_stacked_bar_plot.html"},{"title":"Find Which Column Has The Greatest Value In Each Row","loc":"http://chrisalbon.com/r-stats/max-columns-value.html","text":"Original source: the r book # create a dataframe with simulated values x <- runif ( 50 ) y <- runif ( 50 ) z <- runif ( 50 ) a <- runif ( 50 ) df <- data.frame ( x , y , z , a ) # go through each row, and find the column index with the maximum value max.col ( df ) [1] 1 2 4 4 2 1 3 2 2 3 3 2 2 3 3 3 2 2 3 2 3 3 2 2 2 1 4 1 3 4 2 3 4 3 2 4 4 1 [39] 1 4 1 4 4 3 2 4 4 2 3 1 # go through each row, and find the column index with the minimum value max.col ( - df ) [1] 2 4 3 2 4 3 2 1 3 2 4 4 1 4 2 4 3 4 1 3 4 4 1 4 4 4 2 3 1 1 4 4 1 1 3 3 2 4 [39] 4 3 3 3 2 2 4 2 2 3 1 4","tags":"R Stats","url":"http://chrisalbon.com/r-stats/max-columns-value.html"},{"title":"Merge Data Frames","loc":"http://chrisalbon.com/r-stats/merge-data-frame.html","text":"# Create two variables of 50 observations, note that we only use 10 month names, because to be combined into a dataset all variables must have the same number of lengths OR be a multiple of the longest length. percent.sms.85 <- runif ( 50 ) state <- state.name month.85 <- month.name [ 1 : 10 ] # Create a dataframe of those two variables usa1985 <- data.frame ( state , percent.sms.85 , month.85 ) percent.sms.95 <- runif ( 50 ) state <- state.name month.95 <- month.name [ 1 : 10 ] # Create a dataframe of those two variables usa1995 <- data.frame ( state , percent.sms.95 , month.95 ) # Merge the 1995 and 1985 data frames by the variable \"state\" usa <- merge ( usa1985 , usa1995 , by = \"state\" ) If some observations don't have a match, we can force the merge with the all = TRUE argument # Drop the last 10 observations in usa1985 usa1985a <- usa1985 [ 1 : 40 ,] # Merge 1985 and 1995 data frames usa.a <- merge ( usa1985a , usa1995 , by = \"state\" , all = TRUE ) usa.a state percent.sms.85 month.85 percent.sms.95 month.95 1 Alabama 0.911839169 January 0.78692563 January 2 Alaska 0.962579448 February 0.70895643 February 3 Arizona 0.026227813 March 0.72485492 March 4 Arkansas 0.670236194 April 0.01125962 April 5 California 0.020827639 May 0.33374079 May 6 Colorado 0.863195854 June 0.68766522 June 7 Connecticut 0.219795803 July 0.10379131 July 8 Delaware 0.184099688 August 0.49875863 August 9 Florida 0.454246269 September 0.73110314 September 10 Georgia 0.765207091 October 0.61619085 October 11 Hawaii 0.877940437 January 0.34551863 January 12 Idaho 0.443048854 February 0.74052659 February 13 Illinois 0.772117489 March 0.19904699 March 14 Indiana 0.004526970 April 0.85266998 April 15 Iowa 0.537924783 May 0.98608913 May 16 Kansas 0.517531423 June 0.35238215 June 17 Kentucky 0.048399794 July 0.96977605 July 18 Louisiana 0.127585422 August 0.87639278 August 19 Maine 0.693070376 September 0.93714056 September 20 Maryland 0.446622736 October 0.47403974 October 21 Massachusetts 0.996398812 January 0.11717452 January 22 Michigan 0.598997187 February 0.85384948 February 23 Minnesota 0.453290568 March 0.46549906 March 24 Mississippi 0.501605880 April 0.54499399 April 25 Missouri 0.344708045 May 0.99621523 May 26 Montana 0.199458824 June 0.26662323 June 27 Nebraska 0.661483499 July 0.30106483 July 28 Nevada 0.403032863 August 0.99457227 August 29 New Hampshire 0.006592093 September 0.29969927 September 30 New Jersey 0.080242319 October 0.68534574 October 31 New Mexico 0.157568631 January 0.35347240 January 32 New York 0.425534808 February 0.30905687 February 33 North Carolina 0.780164812 March 0.29777418 March 34 North Dakota 0.338809068 April 0.31317768 April 35 Ohio 0.969465860 May 0.70756109 May 36 Oklahoma 0.869238595 June 0.63593124 June 37 Oregon 0.069332679 July 0.99365654 July 38 Pennsylvania 0.067476764 August 0.38521444 August 39 Rhode Island 0.264621076 September 0.27603005 September 40 South Carolina 0.843611176 October 0.22785430 October 41 South Dakota NA <NA> 0.06466419 January 42 Tennessee NA <NA> 0.56589275 February 43 Texas NA <NA> 0.75391787 March 44 Utah NA <NA> 0.82641243 April 45 Vermont NA <NA> 0.99062278 May 46 Virginia NA <NA> 0.38918234 June 47 Washington NA <NA> 0.82666687 July 48 West Virginia NA <NA> 0.69003692 August 49 Wisconsin NA <NA> 0.03635303 September 50 Wyoming NA <NA> 0.90969228 October","tags":"R Stats","url":"http://chrisalbon.com/r-stats/merge-data-frame.html"},{"title":"Missing values","loc":"http://chrisalbon.com/r-stats/missing-data.html","text":"# Sometimes we do not have all the data we want. When that happens, we can represent that in R as missing value, represented as NA. number.of.pets <- c ( 2 , 5 , NA , NA ) # create a vector that with 2, 5, and two missing values. # We can use the is.na() function to identify missing data. unknown.number.of.pets <- is.na ( number.of.pets ) # Create a variable called unknown.number.of.pets that finds missing data in the variable number.of.pets. unknown.number.of.pets [1] FALSE FALSE TRUE TRUE","tags":"R Stats","url":"http://chrisalbon.com/r-stats/missing-data.html"},{"title":"Removing Missing Observations","loc":"http://chrisalbon.com/r-stats/missing-obs.html","text":"# Create some data with missing observations ages <- c ( 3 , 4 , 9 , NA , 93 , 2 , NA , NA , 2 , 0 , 2 , 9 ) # Create a new variable that is a subset that doesn't include the missing observations ages.no.na <- subset ( ages , ! is.na ( ages )) ages.no.na [1] 3 4 9 93 2 2 0 2 9","tags":"R Stats","url":"http://chrisalbon.com/r-stats/missing-obs.html"},{"title":"Extracting Information From Objects Using Names()","loc":"http://chrisalbon.com/r-stats/names.html","text":"Original source: http://rforpublichealth.blogspot.com/2013/03/extracting-information-from-objects.html # create some simulated data ID <- 1 : 10 Age <- c ( 26 , 65 , 15 , 7 , 88 , 43 , 28 , 66 , 45 , 12 ) Sex <- c ( 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ) Weight <- c ( 132 , 122 , 184 , 145 , 118 , NA , 128 , 154 , 166 , 164 ) Height <- c ( 60 , 63 , 57 , 59 , 64 , NA , 67 , 65 , NA , 60 ) Married <- c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ) # create a dataframe of the simulated data mydata <- data.frame ( ID , Age , Sex , Weight , Height , Married ) names() shows us everything stored under an object # view everything under mydata names ( mydata ) [1] \"ID\" \"Age\" \"Sex\" \"Weight\" \"Height\" \"Married\" we can use names() to change a column header # change the name of column 4 to Weight_lbs names ( mydata )[ 4 ] <- \"Weight_lbs\" # run a regression reg.object <- lm ( Weight_lbs ~ Height + Age , data = mydata ) # display all the objects under the regression names ( reg.object ) [1] \"coefficients\" \"residuals\" \"effects\" \"rank\" [5] \"fitted.values\" \"assign\" \"qr\" \"df.residual\" [9] \"na.action\" \"xlevels\" \"call\" \"terms\" [13] \"model\" # print the residuals of the regression reg.object $ residuals 1 2 3 4 5 7 8 -19.799464 -12.030679 20.687479 -14.062906 -7.873562 -2.223245 26.229706 10 9.072671 # print a histogram of the residuals hist ( reg.object $ residuals , main = \"Distribution of Residuals\" , xlab = \"Residuals\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/names.html"},{"title":"Nesting Lists","loc":"http://chrisalbon.com/python/nesting_lists.html","text":"# Create a list of three nested lists, each with three items state_regions = [[ 'California' , 'Oregon' , 'Washington' ], [ 'Texas' , 'Georgia' , 'Alabama' ], [ 'Maine' , 'Vermont' , 'New York' ]] # View the list state_regions [['California', 'Oregon', 'Washington'], ['Texas', 'Georgia', 'Alabama'], ['Maine', 'Vermont', 'New York']] # Print the second list's third item state_regions [ 1 ][ 2 ] 'Alabama'","tags":"Python","url":"http://chrisalbon.com/python/nesting_lists.html"},{"title":"Normality Test","loc":"http://chrisalbon.com/r-stats/normality-test.html","text":"One simple test for normality is a quantile-quantile plot. It plots the sample's quantiles against a set of quantiles taken from a normal distribution. If the points follow the line drawn, they are roughly normally distributed. If the points create a S-shape or other shape, they are not normally distributed # create simulated data that is not normal y <- runif ( 1000 ) # create simulated data that is normal y.norm <- rnorm ( 1000 ) # create a qq-plot for the non-normal data qqnorm ( y ) qqline ( y , lty = 2 ) # create a qq-plot for the normal data qqnorm ( y.norm ) qqline ( y.norm , lty = 2 ) Shapiro test The null hypothesis is that the data is normally distributed. We want a large p-value, meaning that we cannot reject the null hypothesis (that the data is normally distributed) # shapiro test on non-normal data (results show that we can reject the null that the data is normally distributed) shapiro.test ( y ) Shapiro-Wilk normality test data: y W = 0.9538, p-value < 2.2e-16 # shapiro test on normal data (results show that we cannot reject the null hypothesis that the data is normally distributed) shapiro.test ( y.norm ) Shapiro-Wilk normality test data: y.norm W = 0.9981, p-value = 0.3256","tags":"R Stats","url":"http://chrisalbon.com/r-stats/normality-test.html"},{"title":"Find Approximate Number Of Google Search Results For A String","loc":"http://chrisalbon.com/r-stats/number-of-google-search-results.html","text":"Original source: https://gist.github.com/drewconway/791559 # load the RCurl package require ( RCurl ) # load the xml package require ( XML ) Loading required package: RCurl Loading required package: bitops Loading required package: XML # create a google.counts functions google.counts <- function ( s ){ # take the variable \"s\" and paste it into a google search url search.url <- paste ( \"http://www.google.com/search?q=\" , gsub ( \" \" , \"+\" , s ), sep = \"\" ) # grab the html contents of the search results page search.html <- getURL ( search.url ) # format the html contents parse.search <- htmlTreeParse ( search.html , useInternalNodes = TRUE ) # find a div with the id \"resultStats\" search.nodes <- getNodeSet ( parse.search , \"//div[@id='resultStats']\" ) # Take the entire tag, remove tags themselves (xmlValue), seperate every string by the spaces (strsplit), and take the second string (strsplit()[[1]][2]). search.value <- strsplit ( xmlValue ( search.nodes [[ 1 ]]), \" \" , fixed = TRUE )[[ 1 ]][ 2 ] # display, as numeric, the number of search results return ( as.numeric ( gsub ( \",\" , \"\" , search.value , fixed = TRUE ))) } # an example of using the library google.counts ( \"frogs\" ) [1] 44200000","tags":"R Stats","url":"http://chrisalbon.com/r-stats/number-of-google-search-results.html"},{"title":"Basic Operations With Numpy Array","loc":"http://chrisalbon.com/python/numpy_array_basic_operations.html","text":"# Import modules import numpy as np # Create an array civilian_deaths = np . array ([ 4352 , 233 , 3245 , 256 , 2394 ]) civilian_deaths array([4352, 233, 3245, 256, 2394]) # Mean value of the array civilian_deaths . mean () 2096.0 # Total amount of deaths civilian_deaths . sum () 10480 # Smallest value in the array civilian_deaths . min () 233 # Largest value in the array civilian_deaths . max () 4352","tags":"Python","url":"http://chrisalbon.com/python/numpy_array_basic_operations.html"},{"title":"Numpy Array Basics","loc":"http://chrisalbon.com/python/numpy_array_basics.html","text":"# Import modules import numpy as np # Create a list battle_deaths = [ 3246 , 326 , 2754 , 2547 , 2457 , 3456 ] battle_deaths [3246, 326, 2754, 2547, 2457, 3456] # Create an array from numpy deaths = np . array ( battle_deaths ) deaths array([3246, 326, 2754, 2547, 2457, 3456]) # Create an array of zeros defectors = np . zeros ( 6 ) defectors array([ 0., 0., 0., 0., 0., 0.]) # Create a range from 0 to 100 zero_to_99 = np . arange ( 0 , 100 ) zero_to_99 array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]) # Create 100 ticks between 0 and 1 zero_to_1 = np . linspace ( 0 , 1 , 100 ) zero_to_1 array([ 0. , 0.01010101, 0.02020202, 0.03030303, 0.04040404, 0.05050505, 0.06060606, 0.07070707, 0.08080808, 0.09090909, 0.1010101 , 0.11111111, 0.12121212, 0.13131313, 0.14141414, 0.15151515, 0.16161616, 0.17171717, 0.18181818, 0.19191919, 0.2020202 , 0.21212121, 0.22222222, 0.23232323, 0.24242424, 0.25252525, 0.26262626, 0.27272727, 0.28282828, 0.29292929, 0.3030303 , 0.31313131, 0.32323232, 0.33333333, 0.34343434, 0.35353535, 0.36363636, 0.37373737, 0.38383838, 0.39393939, 0.4040404 , 0.41414141, 0.42424242, 0.43434343, 0.44444444, 0.45454545, 0.46464646, 0.47474747, 0.48484848, 0.49494949, 0.50505051, 0.51515152, 0.52525253, 0.53535354, 0.54545455, 0.55555556, 0.56565657, 0.57575758, 0.58585859, 0.5959596 , 0.60606061, 0.61616162, 0.62626263, 0.63636364, 0.64646465, 0.65656566, 0.66666667, 0.67676768, 0.68686869, 0.6969697 , 0.70707071, 0.71717172, 0.72727273, 0.73737374, 0.74747475, 0.75757576, 0.76767677, 0.77777778, 0.78787879, 0.7979798 , 0.80808081, 0.81818182, 0.82828283, 0.83838384, 0.84848485, 0.85858586, 0.86868687, 0.87878788, 0.88888889, 0.8989899 , 0.90909091, 0.91919192, 0.92929293, 0.93939394, 0.94949495, 0.95959596, 0.96969697, 0.97979798, 0.98989899, 1. ])","tags":"Python","url":"http://chrisalbon.com/python/numpy_array_basics.html"},{"title":"Indexing And Slicing Numpy Arrays","loc":"http://chrisalbon.com/python/numpy_indexing_and_slicing.html","text":"# Import modules import numpy as np # Create a 2x2 array battle_deaths = [[ 344 , 2345 ], [ 253 , 4345 ]] deaths = np . array ( battle_deaths ) deaths array([[5341, 2345], [ 253, 4345]]) # Select the top row, second item deaths [ 0 , 1 ] 2345 # Select the second column deaths [:, 1 ] array([2345, 4345]) # Select the second row deaths [ 1 , :] array([ 253, 4345]) # Create an array of civilian deaths civilian_deaths = np . array ([ 4352 , 233 , 3245 , 256 , 2394 ]) civilian_deaths array([4352, 233, 3245, 256, 2394]) # Find the index of battles with less than 500 deaths few_civ_deaths = np . where ( civilian_deaths < 500 ) few_civ_deaths (array([1, 3]),) # Find the number of civilian deaths in battles with less than 500 deaths civ_deaths = civilian_deaths [ few_civ_deaths ] civ_deaths array([233, 256])","tags":"Python","url":"http://chrisalbon.com/python/numpy_indexing_and_slicing.html"},{"title":"Object Basics","loc":"http://chrisalbon.com/r-stats/objects-101.html","text":"R treats almost everything in the software as an object. Objects can be individual numbers to variable names to strings of plain text. Working in R meaning manipulating these objects to make them how you want them. In R, a collection of one or more values is called a vector. Think of vectors as a collection of numbers. # We can create a vector by creating a variable my.age <- 29 # create an vector variable called my.age that contains the value \"29\" my.age # view the contents of our.ages [1] 29 our.ages <- c ( 29 , 29 , 43 , 4 ) # create a vector variable called our.ages containing the values 29, 29, 43, and 4 our.ages # view the contents of our.ages [1] 29 29 43 4","tags":"R Stats","url":"http://chrisalbon.com/r-stats/objects-101.html"},{"title":"Load Jupyter Notebook In Non-Default Browser","loc":"http://chrisalbon.com/command-line/open_ipython_nb_in_nondefault_browser.html","text":"By default Jupyter Notebooks loads in your default browser. The following bash script opens Jupyter Notebook in a specific browser (in this example, Chrome). Note: I have commented out all the commands so it doesn't run while in Jupyter # %%bash # Set the bash hashbang # #!/bin/bash # Open IPython Notebook using Chrome # BROWSER=/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome jupyter notebook","tags":"Command Line","url":"http://chrisalbon.com/command-line/open_ipython_nb_in_nondefault_browser.html"},{"title":"Ordered Factors","loc":"http://chrisalbon.com/r-stats/ordered-factors.html","text":"# Generate some fake data about education in 1000 observations education <- c ( \"some high school\" , \"high school\" , \"college\" , \"masters\" , \"phd\" ) obs <- sample ( education , 1000 , replace = TRUE ) obs [1] \"college\" \"college\" \"college\" [4] \"phd\" \"high school\" \"college\" [7] \"phd\" \"masters\" \"high school\" [10] \"some high school\" \"high school\" \"masters\" [13] \"high school\" \"phd\" \"some high school\" [16] \"masters\" \"some high school\" \"high school\" [19] \"phd\" \"college\" \"college\" [22] \"phd\" \"college\" \"high school\" [25] \"high school\" \"college\" \"high school\" [28] \"college\" \"college\" \"high school\" [31] \"high school\" \"phd\" \"some high school\" [34] \"some high school\" \"high school\" \"high school\" [37] \"phd\" \"phd\" \"college\" [40] \"some high school\" \"masters\" \"phd\" [43] \"college\" \"some high school\" \"phd\" [46] \"phd\" \"some high school\" \"high school\" [49] \"high school\" \"masters\" \"some high school\" [52] \"masters\" \"some high school\" \"masters\" [55] \"high school\" \"some high school\" \"some high school\" [58] \"masters\" \"masters\" \"masters\" [61] \"phd\" \"phd\" \"some high school\" [64] \"masters\" \"masters\" \"high school\" [67] \"college\" \"college\" \"phd\" [70] \"some high school\" \"high school\" \"college\" [73] \"phd\" \"phd\" \"high school\" [76] \"college\" \"some high school\" \"phd\" [79] \"college\" \"phd\" \"masters\" [82] \"phd\" \"masters\" \"masters\" [85] \"college\" \"college\" \"college\" [88] \"high school\" \"college\" \"phd\" [91] \"some high school\" \"high school\" \"masters\" [94] \"masters\" \"phd\" \"some high school\" [97] \"masters\" \"phd\" \"some high school\" [100] \"masters\" \"some high school\" \"phd\" [103] \"masters\" \"masters\" \"masters\" [106] \"some high school\" \"high school\" \"phd\" [109] \"phd\" \"some high school\" \"masters\" [112] \"phd\" \"phd\" \"college\" [115] \"masters\" \"college\" \"college\" [118] \"college\" \"college\" \"some high school\" [121] \"phd\" \"high school\" \"high school\" [124] \"high school\" \"phd\" \"phd\" [127] \"college\" \"high school\" \"phd\" [130] \"high school\" \"phd\" \"high school\" [133] \"phd\" \"phd\" \"college\" [136] \"high school\" \"high school\" \"high school\" [139] \"masters\" \"masters\" \"phd\" [142] \"masters\" \"college\" \"masters\" [145] \"college\" \"masters\" \"phd\" [148] \"high school\" \"masters\" \"phd\" [151] \"masters\" \"college\" \"high school\" [154] \"college\" \"masters\" \"some high school\" [157] \"college\" \"college\" \"phd\" [160] \"high school\" \"masters\" \"phd\" [163] \"masters\" \"some high school\" \"college\" [166] \"some high school\" \"high school\" \"some high school\" [169] \"college\" \"high school\" \"phd\" [172] \"masters\" \"phd\" \"some high school\" [175] \"some high school\" \"phd\" \"phd\" [178] \"masters\" \"phd\" \"college\" [181] \"high school\" \"some high school\" \"some high school\" [184] \"college\" \"high school\" \"high school\" [187] \"phd\" \"high school\" \"masters\" [190] \"some high school\" \"high school\" \"masters\" [193] \"high school\" \"college\" \"college\" [196] \"masters\" \"some high school\" \"masters\" [199] \"some high school\" \"college\" \"phd\" [202] \"some high school\" \"college\" \"high school\" [205] \"phd\" \"masters\" \"phd\" [208] \"college\" \"some high school\" \"some high school\" [211] \"high school\" \"some high school\" \"some high school\" [214] \"phd\" \"masters\" \"some high school\" [217] \"some high school\" \"college\" \"masters\" [220] \"college\" \"masters\" \"some high school\" [223] \"phd\" \"phd\" \"high school\" [226] \"masters\" \"college\" \"some high school\" [229] \"phd\" \"college\" \"some high school\" [232] \"college\" \"some high school\" \"phd\" [235] \"high school\" \"phd\" \"masters\" [238] \"high school\" \"phd\" \"masters\" [241] \"high school\" \"masters\" \"masters\" [244] \"masters\" \"college\" \"some high school\" [247] \"phd\" \"masters\" \"masters\" [250] \"some high school\" \"phd\" \"high school\" [253] \"some high school\" \"some high school\" \"college\" [256] \"some high school\" \"masters\" \"high school\" [259] \"high school\" \"some high school\" \"high school\" [262] \"high school\" \"high school\" \"phd\" [265] \"masters\" \"high school\" \"college\" [268] \"some high school\" \"masters\" \"high school\" [271] \"phd\" \"some high school\" \"high school\" [274] \"high school\" \"masters\" \"high school\" [277] \"masters\" \"phd\" \"masters\" [280] \"phd\" \"phd\" \"college\" [283] \"masters\" \"some high school\" \"masters\" [286] \"phd\" \"masters\" \"college\" [289] \"masters\" \"high school\" \"some high school\" [292] \"phd\" \"phd\" \"masters\" [295] \"phd\" \"high school\" \"college\" [298] \"masters\" \"masters\" \"masters\" [301] \"phd\" \"college\" \"college\" [304] \"phd\" \"high school\" \"masters\" [307] \"high school\" \"masters\" \"some high school\" [310] \"college\" \"phd\" \"some high school\" [313] \"college\" \"some high school\" \"college\" [316] \"high school\" \"masters\" \"college\" [319] \"masters\" \"college\" \"college\" [322] \"college\" \"masters\" \"some high school\" [325] \"masters\" \"masters\" \"masters\" [328] \"masters\" \"masters\" \"phd\" [331] \"college\" \"some high school\" \"some high school\" [334] \"masters\" \"high school\" \"high school\" [337] \"some high school\" \"phd\" \"masters\" [340] \"some high school\" \"phd\" \"some high school\" [343] \"masters\" \"phd\" \"college\" [346] \"some high school\" \"phd\" \"high school\" [349] \"masters\" \"college\" \"college\" [352] \"phd\" \"masters\" \"high school\" [355] \"high school\" \"high school\" \"masters\" [358] \"phd\" \"phd\" \"masters\" [361] \"phd\" \"masters\" \"high school\" [364] \"some high school\" \"college\" \"high school\" [367] \"some high school\" \"some high school\" \"phd\" [370] \"some high school\" \"phd\" \"phd\" [373] \"some high school\" \"masters\" \"high school\" [376] \"college\" \"masters\" \"some high school\" [379] \"some high school\" \"some high school\" \"some high school\" [382] \"masters\" \"phd\" \"masters\" [385] \"masters\" \"high school\" \"masters\" [388] \"masters\" \"college\" \"phd\" [391] \"high school\" \"some high school\" \"masters\" [394] \"masters\" \"masters\" \"masters\" [397] \"college\" \"phd\" \"high school\" [400] \"masters\" \"high school\" \"masters\" [403] \"college\" \"high school\" \"some high school\" [406] \"phd\" \"phd\" \"masters\" [409] \"phd\" \"masters\" \"masters\" [412] \"phd\" \"masters\" \"high school\" [415] \"college\" \"masters\" \"college\" [418] \"masters\" \"phd\" \"phd\" [421] \"college\" \"college\" \"some high school\" [424] \"masters\" \"some high school\" \"phd\" [427] \"college\" \"masters\" \"some high school\" [430] \"college\" \"some high school\" \"college\" [433] \"masters\" \"masters\" \"some high school\" [436] \"masters\" \"high school\" \"some high school\" [439] \"phd\" \"phd\" \"college\" [442] \"phd\" \"high school\" \"some high school\" [445] \"some high school\" \"some high school\" \"phd\" [448] \"high school\" \"masters\" \"masters\" [451] \"some high school\" \"masters\" \"some high school\" [454] \"college\" \"masters\" \"masters\" [457] \"some high school\" \"phd\" \"college\" [460] \"masters\" \"high school\" \"some high school\" [463] \"some high school\" \"college\" \"phd\" [466] \"high school\" \"masters\" \"high school\" [469] \"high school\" \"some high school\" \"phd\" [472] \"high school\" \"some high school\" \"some high school\" [475] \"college\" \"college\" \"masters\" [478] \"phd\" \"masters\" \"masters\" [481] \"high school\" \"high school\" \"phd\" [484] \"college\" \"some high school\" \"phd\" [487] \"masters\" \"phd\" \"some high school\" [490] \"high school\" \"college\" \"some high school\" [493] \"high school\" \"phd\" \"college\" [496] \"phd\" \"masters\" \"phd\" [499] \"some high school\" \"phd\" \"high school\" [502] \"phd\" \"college\" \"some high school\" [505] \"masters\" \"phd\" \"some high school\" [508] \"high school\" \"phd\" \"masters\" [511] \"masters\" \"some high school\" \"high school\" [514] \"some high school\" \"masters\" \"some high school\" [517] \"some high school\" \"phd\" \"phd\" [520] \"some high school\" \"phd\" \"high school\" [523] \"some high school\" \"masters\" \"college\" [526] \"high school\" \"high school\" \"phd\" [529] \"high school\" \"college\" \"phd\" [532] \"masters\" \"high school\" \"college\" [535] \"masters\" \"college\" \"college\" [538] \"college\" \"some high school\" \"college\" [541] \"phd\" \"phd\" \"college\" [544] \"college\" \"phd\" \"college\" [547] \"masters\" \"college\" \"masters\" [550] \"phd\" \"college\" \"college\" [553] \"college\" \"some high school\" \"high school\" [556] \"masters\" \"phd\" \"some high school\" [559] \"masters\" \"masters\" \"high school\" [562] \"high school\" \"phd\" \"phd\" [565] \"college\" \"college\" \"some high school\" [568] \"college\" \"college\" \"masters\" [571] \"high school\" \"phd\" \"phd\" [574] \"phd\" \"masters\" \"some high school\" [577] \"college\" \"masters\" \"college\" [580] \"phd\" \"masters\" \"college\" [583] \"college\" \"phd\" \"phd\" [586] \"some high school\" \"phd\" \"college\" [589] \"high school\" \"phd\" \"some high school\" [592] \"masters\" \"masters\" \"masters\" [595] \"college\" \"masters\" \"college\" [598] \"college\" \"college\" \"high school\" [601] \"masters\" \"masters\" \"phd\" [604] \"phd\" \"phd\" \"college\" [607] \"college\" \"phd\" \"phd\" [610] \"phd\" \"college\" \"college\" [613] \"masters\" \"phd\" \"masters\" [616] \"high school\" \"phd\" \"college\" [619] \"masters\" \"high school\" \"masters\" [622] \"masters\" \"masters\" \"college\" [625] \"phd\" \"phd\" \"masters\" [628] \"some high school\" \"college\" \"masters\" [631] \"masters\" \"college\" \"high school\" [634] \"some high school\" \"college\" \"college\" [637] \"college\" \"phd\" \"college\" [640] \"masters\" \"college\" \"some high school\" [643] \"some high school\" \"phd\" \"high school\" [646] \"masters\" \"college\" \"high school\" [649] \"high school\" \"college\" \"some high school\" [652] \"high school\" \"phd\" \"high school\" [655] \"masters\" \"masters\" \"high school\" [658] \"high school\" \"some high school\" \"masters\" [661] \"phd\" \"high school\" \"phd\" [664] \"masters\" \"masters\" \"high school\" [667] \"high school\" \"masters\" \"masters\" [670] \"college\" \"phd\" \"some high school\" [673] \"masters\" \"phd\" \"some high school\" [676] \"high school\" \"college\" \"some high school\" [679] \"some high school\" \"high school\" \"phd\" [682] \"phd\" \"college\" \"phd\" [685] \"some high school\" \"masters\" \"high school\" [688] \"college\" \"masters\" \"college\" [691] \"college\" \"masters\" \"some high school\" [694] \"some high school\" \"college\" \"some high school\" [697] \"high school\" \"high school\" \"phd\" [700] \"some high school\" \"masters\" \"phd\" [703] \"high school\" \"some high school\" \"college\" [706] \"college\" \"college\" \"college\" [709] \"some high school\" \"college\" \"college\" [712] \"some high school\" \"college\" \"masters\" [715] \"phd\" \"college\" \"phd\" [718] \"high school\" \"college\" \"masters\" [721] \"phd\" \"some high school\" \"some high school\" [724] \"some high school\" \"masters\" \"college\" [727] \"college\" \"some high school\" \"college\" [730] \"masters\" \"masters\" \"high school\" [733] \"phd\" \"phd\" \"college\" [736] \"phd\" \"college\" \"college\" [739] \"masters\" \"some high school\" \"masters\" [742] \"masters\" \"high school\" \"some high school\" [745] \"high school\" \"phd\" \"high school\" [748] \"some high school\" \"masters\" \"high school\" [751] \"some high school\" \"phd\" \"phd\" [754] \"high school\" \"college\" \"high school\" [757] \"college\" \"college\" \"masters\" [760] \"phd\" \"masters\" \"phd\" [763] \"high school\" \"high school\" \"high school\" [766] \"college\" \"masters\" \"some high school\" [769] \"high school\" \"phd\" \"some high school\" [772] \"masters\" \"some high school\" \"masters\" [775] \"phd\" \"some high school\" \"some high school\" [778] \"masters\" \"high school\" \"phd\" [781] \"phd\" \"some high school\" \"college\" [784] \"some high school\" \"phd\" \"some high school\" [787] \"masters\" \"masters\" \"masters\" [790] \"college\" \"high school\" \"phd\" [793] \"high school\" \"college\" \"some high school\" [796] \"high school\" \"masters\" \"phd\" [799] \"college\" \"some high school\" \"high school\" [802] \"phd\" \"college\" \"high school\" [805] \"masters\" \"some high school\" \"college\" [808] \"some high school\" \"high school\" \"phd\" [811] \"high school\" \"high school\" \"masters\" [814] \"some high school\" \"some high school\" \"phd\" [817] \"masters\" \"college\" \"high school\" [820] \"masters\" \"college\" \"some high school\" [823] \"high school\" \"high school\" \"phd\" [826] \"college\" \"college\" \"college\" [829] \"college\" \"masters\" \"masters\" [832] \"high school\" \"some high school\" \"phd\" [835] \"college\" \"some high school\" \"some high school\" [838] \"high school\" \"high school\" \"college\" [841] \"phd\" \"high school\" \"college\" [844] \"phd\" \"high school\" \"college\" [847] \"masters\" \"college\" \"college\" [850] \"some high school\" \"some high school\" \"high school\" [853] \"high school\" \"high school\" \"high school\" [856] \"masters\" \"high school\" \"high school\" [859] \"phd\" \"some high school\" \"phd\" [862] \"college\" \"some high school\" \"college\" [865] \"masters\" \"phd\" \"college\" [868] \"phd\" \"phd\" \"some high school\" [871] \"high school\" \"masters\" \"some high school\" [874] \"masters\" \"college\" \"high school\" [877] \"phd\" \"high school\" \"masters\" [880] \"masters\" \"some high school\" \"college\" [883] \"some high school\" \"college\" \"masters\" [886] \"college\" \"college\" \"some high school\" [889] \"phd\" \"phd\" \"college\" [892] \"phd\" \"phd\" \"masters\" [895] \"college\" \"phd\" \"masters\" [898] \"some high school\" \"high school\" \"college\" [901] \"high school\" \"college\" \"some high school\" [904] \"high school\" \"masters\" \"college\" [907] \"masters\" \"college\" \"college\" [910] \"some high school\" \"college\" \"some high school\" [913] \"masters\" \"phd\" \"high school\" [916] \"college\" \"some high school\" \"phd\" [919] \"high school\" \"masters\" \"some high school\" [922] \"phd\" \"high school\" \"some high school\" [925] \"phd\" \"high school\" \"phd\" [928] \"high school\" \"some high school\" \"some high school\" [931] \"high school\" \"phd\" \"masters\" [934] \"some high school\" \"high school\" \"some high school\" [937] \"phd\" \"masters\" \"high school\" [940] \"college\" \"some high school\" \"some high school\" [943] \"masters\" \"some high school\" \"phd\" [946] \"some high school\" \"phd\" \"some high school\" [949] \"college\" \"phd\" \"phd\" [952] \"masters\" \"college\" \"high school\" [955] \"high school\" \"masters\" \"phd\" [958] \"high school\" \"phd\" \"college\" [961] \"masters\" \"masters\" \"phd\" [964] \"college\" \"masters\" \"college\" [967] \"college\" \"some high school\" \"phd\" [970] \"college\" \"some high school\" \"some high school\" [973] \"high school\" \"masters\" \"some high school\" [976] \"phd\" \"college\" \"high school\" [979] \"phd\" \"high school\" \"high school\" [982] \"phd\" \"masters\" \"masters\" [985] \"some high school\" \"high school\" \"phd\" [988] \"phd\" \"college\" \"phd\" [991] \"college\" \"high school\" \"some high school\" [994] \"some high school\" \"college\" \"some high school\" [997] \"high school\" \"masters\" \"masters\" [1000] \"phd\" # Turn the same into a ordered factor obs.ordered <- ordered ( obs ) obs.ordered [1] college college college phd [5] high school college phd masters [9] high school some high school high school masters [13] high school phd some high school masters [17] some high school high school phd college [21] college phd college high school [25] high school college high school college [29] college high school high school phd [33] some high school some high school high school high school [37] phd phd college some high school [41] masters phd college some high school [45] phd phd some high school high school [49] high school masters some high school masters [53] some high school masters high school some high school [57] some high school masters masters masters [61] phd phd some high school masters [65] masters high school college college [69] phd some high school high school college [73] phd phd high school college [77] some high school phd college phd [81] masters phd masters masters [85] college college college high school [89] college phd some high school high school [93] masters masters phd some high school [97] masters phd some high school masters [101] some high school phd masters masters [105] masters some high school high school phd [109] phd some high school masters phd [113] phd college masters college [117] college college college some high school [121] phd high school high school high school [125] phd phd college high school [129] phd high school phd high school [133] phd phd college high school [137] high school high school masters masters [141] phd masters college masters [145] college masters phd high school [149] masters phd masters college [153] high school college masters some high school [157] college college phd high school [161] masters phd masters some high school [165] college some high school high school some high school [169] college high school phd masters [173] phd some high school some high school phd [177] phd masters phd college [181] high school some high school some high school college [185] high school high school phd high school [189] masters some high school high school masters [193] high school college college masters [197] some high school masters some high school college [201] phd some high school college high school [205] phd masters phd college [209] some high school some high school high school some high school [213] some high school phd masters some high school [217] some high school college masters college [221] masters some high school phd phd [225] high school masters college some high school [229] phd college some high school college [233] some high school phd high school phd [237] masters high school phd masters [241] high school masters masters masters [245] college some high school phd masters [249] masters some high school phd high school [253] some high school some high school college some high school [257] masters high school high school some high school [261] high school high school high school phd [265] masters high school college some high school [269] masters high school phd some high school [273] high school high school masters high school [277] masters phd masters phd [281] phd college masters some high school [285] masters phd masters college [289] masters high school some high school phd [293] phd masters phd high school [297] college masters masters masters [301] phd college college phd [305] high school masters high school masters [309] some high school college phd some high school [313] college some high school college high school [317] masters college masters college [321] college college masters some high school [325] masters masters masters masters [329] masters phd college some high school [333] some high school masters high school high school [337] some high school phd masters some high school [341] phd some high school masters phd [345] college some high school phd high school [349] masters college college phd [353] masters high school high school high school [357] masters phd phd masters [361] phd masters high school some high school [365] college high school some high school some high school [369] phd some high school phd phd [373] some high school masters high school college [377] masters some high school some high school some high school [381] some high school masters phd masters [385] masters high school masters masters [389] college phd high school some high school [393] masters masters masters masters [397] college phd high school masters [401] high school masters college high school [405] some high school phd phd masters [409] phd masters masters phd [413] masters high school college masters [417] college masters phd phd [421] college college some high school masters [425] some high school phd college masters [429] some high school college some high school college [433] masters masters some high school masters [437] high school some high school phd phd [441] college phd high school some high school [445] some high school some high school phd high school [449] masters masters some high school masters [453] some high school college masters masters [457] some high school phd college masters [461] high school some high school some high school college [465] phd high school masters high school [469] high school some high school phd high school [473] some high school some high school college college [477] masters phd masters masters [481] high school high school phd college [485] some high school phd masters phd [489] some high school high school college some high school [493] high school phd college phd [497] masters phd some high school phd [501] high school phd college some high school [505] masters phd some high school high school [509] phd masters masters some high school [513] high school some high school masters some high school [517] some high school phd phd some high school [521] phd high school some high school masters [525] college high school high school phd [529] high school college phd masters [533] high school college masters college [537] college college some high school college [541] phd phd college college [545] phd college masters college [549] masters phd college college [553] college some high school high school masters [557] phd some high school masters masters [561] high school high school phd phd [565] college college some high school college [569] college masters high school phd [573] phd phd masters some high school [577] college masters college phd [581] masters college college phd [585] phd some high school phd college [589] high school phd some high school masters [593] masters masters college masters [597] college college college high school [601] masters masters phd phd [605] phd college college phd [609] phd phd college college [613] masters phd masters high school [617] phd college masters high school [621] masters masters masters college [625] phd phd masters some high school [629] college masters masters college [633] high school some high school college college [637] college phd college masters [641] college some high school some high school phd [645] high school masters college high school [649] high school college some high school high school [653] phd high school masters masters [657] high school high school some high school masters [661] phd high school phd masters [665] masters high school high school masters [669] masters college phd some high school [673] masters phd some high school high school [677] college some high school some high school high school [681] phd phd college phd [685] some high school masters high school college [689] masters college college masters [693] some high school some high school college some high school [697] high school high school phd some high school [701] masters phd high school some high school [705] college college college college [709] some high school college college some high school [713] college masters phd college [717] phd high school college masters [721] phd some high school some high school some high school [725] masters college college some high school [729] college masters masters high school [733] phd phd college phd [737] college college masters some high school [741] masters masters high school some high school [745] high school phd high school some high school [749] masters high school some high school phd [753] phd high school college high school [757] college college masters phd [761] masters phd high school high school [765] high school college masters some high school [769] high school phd some high school masters [773] some high school masters phd some high school [777] some high school masters high school phd [781] phd some high school college some high school [785] phd some high school masters masters [789] masters college high school phd [793] high school college some high school high school [797] masters phd college some high school [801] high school phd college high school [805] masters some high school college some high school [809] high school phd high school high school [813] masters some high school some high school phd [817] masters college high school masters [821] college some high school high school high school [825] phd college college college [829] college masters masters high school [833] some high school phd college some high school [837] some high school high school high school college [841] phd high school college phd [845] high school college masters college [849] college some high school some high school high school [853] high school high school high school masters [857] high school high school phd some high school [861] phd college some high school college [865] masters phd college phd [869] phd some high school high school masters [873] some high school masters college high school [877] phd high school masters masters [881] some high school college some high school college [885] masters college college some high school [889] phd phd college phd [893] phd masters college phd [897] masters some high school high school college [901] high school college some high school high school [905] masters college masters college [909] college some high school college some high school [913] masters phd high school college [917] some high school phd high school masters [921] some high school phd high school some high school [925] phd high school phd high school [929] some high school some high school high school phd [933] masters some high school high school some high school [937] phd masters high school college [941] some high school some high school masters some high school [945] phd some high school phd some high school [949] college phd phd masters [953] college high school high school masters [957] phd high school phd college [961] masters masters phd college [965] masters college college some high school [969] phd college some high school some high school [973] high school masters some high school phd [977] college high school phd high school [981] high school phd masters masters [985] some high school high school phd phd [989] college phd college high school [993] some high school some high school college some high school [997] high school masters masters phd Levels: college < high school < masters < phd < some high school","tags":"R Stats","url":"http://chrisalbon.com/r-stats/ordered-factors.html"},{"title":"Overlapping Histrograms","loc":"http://chrisalbon.com/r-stats/overlapping-histograms.html","text":"Original Source: http://r-nold.blogspot.com/2013/03/overlapping-histogram-in-r.html Create two fictional histographs # A histograph of 1000 normally distributed points with mean 4 h2 <- rnorm ( 1000 , 4 ) # A histograph of 1000 normally distributed points with mean 6 h1 <- rnorm ( 1000 , 6 ) Histogram Colored (blue and red) # Create a histogram of h1, with colors, at .5 opacity hist ( h1 , col = rgb ( 1 , 0 , 0 , 0.5 ), xlim = c ( 0 , 10 ), ylim = c ( 0 , 200 ), main = \"Overlapping Histogram\" , xlab = \"Variable\" ) # Create a histogram of h1, with colors, at .5 opacity hist ( h2 , col = rgb ( 0 , 0 , 1 , 0.5 ), add = T ) # Combine the two histograms box ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/overlapping-histograms.html"},{"title":"Apply Functions By Group In Pandas","loc":"http://chrisalbon.com/python/pandas_apply_function_by_group.html","text":"Preliminaries # import pandas as pd import pandas as pd Create a simulated dataset # Create an example dataframe data = { 'Platoon' : [ 'A' , 'A' , 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' , 'B' , 'C' , 'C' , 'C' , 'C' , 'C' ], 'Casualties' : [ 1 , 4 , 5 , 7 , 5 , 5 , 6 , 1 , 4 , 5 , 6 , 7 , 4 , 6 , 4 , 6 ]} df = pd . DataFrame ( data ) df Casualties Platoon 0 1 A 1 4 A 2 5 A 3 7 A 4 5 A 5 5 A 6 6 B 7 1 B 8 4 B 9 5 B 10 6 B 11 7 C 12 4 C 13 6 C 14 4 C 15 6 C Apply A Function (Rolling Mean) To The DataFrame, By Group # Group df by df.platoon, then apply a rolling mean lambda function to df.casualties df . groupby ( 'Platoon' )[ 'Casualties' ] . apply ( lambda x : x . rolling ( center = False , window = 2 ) . mean ()) 0 NaN 1 2.5 2 4.5 3 6.0 4 6.0 5 5.0 6 NaN 7 3.5 8 2.5 9 4.5 10 5.5 11 NaN 12 5.5 13 5.0 14 5.0 15 5.0 dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/pandas_apply_function_by_group.html"},{"title":"Applying Operations Over pandas Dataframes","loc":"http://chrisalbon.com/python/pandas_apply_operations_to_dataframes.html","text":"Import Modules import pandas as pd import numpy as np Create a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 5 rows × 4 columns Create a capitalization lambda function capitalizer = lambda x : x . upper () Apply the capitalizer function over the column 'name' apply() can apply a function along any axis of the dataframe df [ 'name' ] . apply ( capitalizer ) Cochice JASON Pima MOLLY Santa Cruz TINA Maricopa JAKE Yuma AMY Name: name, dtype: object Map the capitalizer lambda function over each element in the series 'name' map() applies an operation over each element of a series df [ 'name' ] . map ( capitalizer ) Cochice JASON Pima MOLLY Santa Cruz TINA Maricopa JAKE Yuma AMY Name: name, dtype: object Apply a square root function to every single cell in the whole data frame applymap() applies a function to every single element in the entire dataframe. # Drop the string variable so that applymap() can run df = df . drop ( 'name' , axis = 1 ) # Return the square root of every cell in the dataframe df . applymap ( np . sqrt ) coverage reports year Cochice 5.000000 2.000000 44.855323 Pima 9.695360 4.898979 44.855323 Santa Cruz 7.549834 5.567764 44.866469 Maricopa 7.874008 1.414214 44.877611 Yuma 8.366600 1.732051 44.877611 5 rows × 3 columns Applying A Function Over A Dataframe Create a function that multiplies all non-strings by 100 # create a function called times100 def times100 ( x ): # that, if x is a string, if type ( x ) is str : # just returns it untouched return x # but, if not, return it multiplied by 100 elif x : return 100 * x # and leave everything else else : return Apply the times100 over every cell in the dataframe df . applymap ( times100 ) coverage reports year Cochice 2500 400 201200 Pima 9400 2400 201200 Santa Cruz 5700 3100 201300 Maricopa 6200 200 201400 Yuma 7000 300 201400 5 rows × 3 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_apply_operations_to_dataframes.html"},{"title":"Apply Operations To Groups In Pandas","loc":"http://chrisalbon.com/python/pandas_apply_operations_to_groups.html","text":"Preliminaries # import modules import pandas as pd # Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 12 rows × 5 columns # Create a groupby variable that groups preTestScores by regiment groupby_regiment = df [ 'preTestScore' ] . groupby ( df [ 'company' ]) groupby_regiment <pandas.core.groupby.SeriesGroupBy object at 0x1075fb610> \"This grouped variable is now a GroupBy object. It has not actually computed anything yet except for some intermediate data about the group key df['key1']. The idea is that this object has all of the information needed to then apply some operation to each of the groups.\" - Python for Data Analysis View a grouping Use list() to show what a grouping looks like list ( df [ 'preTestScore' ] . groupby ( df [ 'company' ])) [('1st', 0 4 1 24 4 3 5 4 8 2 9 3 Name: preTestScore, dtype: int64), ('2nd', 2 31 3 2 6 24 7 31 10 2 11 3 Name: preTestScore, dtype: int64)] Descriptive statistics by group df [ 'preTestScore' ] . groupby ( df [ 'company' ]) . describe () company 1st count 6.000000 mean 6.666667 std 8.524475 min 2.000000 25% 3.000000 50% 3.500000 75% 4.000000 max 24.000000 2nd count 6.000000 mean 15.500000 std 14.652645 min 2.000000 25% 2.250000 50% 13.500000 75% 29.250000 max 31.000000 dtype: float64 Mean of each regiment's preTestScore groupby_regiment . mean () company 1st 6.666667 2nd 15.500000 dtype: float64 Mean preTestScores grouped by regiment and company df [ 'preTestScore' ] . groupby ([ df [ 'regiment' ], df [ 'company' ]]) . mean () regiment company Dragoons 1st 3.5 2nd 27.5 Nighthawks 1st 14.0 2nd 16.5 Scouts 1st 2.5 2nd 2.5 dtype: float64 Mean preTestScores grouped by regiment and company without heirarchical indexing df [ 'preTestScore' ] . groupby ([ df [ 'regiment' ], df [ 'company' ]]) . mean () . unstack () company 1st 2nd regiment Dragoons 3.5 27.5 Nighthawks 14.0 16.5 Scouts 2.5 2.5 3 rows × 2 columns Group the entire dataframe by regiment and company df . groupby ([ 'regiment' , 'company' ]) . mean () preTestScore postTestScore regiment company Dragoons 1st 3.5 47.5 2nd 27.5 75.5 Nighthawks 1st 14.0 59.5 2nd 16.5 59.5 Scouts 1st 2.5 66.0 2nd 2.5 66.0 6 rows × 2 columns Number of observations in each regiment and company df . groupby ([ 'regiment' , 'company' ]) . size () regiment company Dragoons 1st 2 2nd 2 Nighthawks 1st 2 2nd 2 Scouts 1st 2 2nd 2 dtype: int64 Iterate an operations over groups # Group the dataframe by regiment, and for each regiment, for name , group in df . groupby ( 'regiment' ): # print the name of the regiment print ( name ) # print the data of that regiment print ( group ) Dragoons regiment company name preTestScore postTestScore 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 [4 rows x 5 columns] Nighthawks regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 [4 rows x 5 columns] Scouts regiment company name preTestScore postTestScore 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 [4 rows x 5 columns] Group by columns Specifically in this case: group by the data types of the columns (i.e. axis=1) and then use list() to view what that grouping looks like list ( df . groupby ( df . dtypes , axis = 1 )) [(dtype('int64'), preTestScore postTestScore 0 4 25 1 24 94 2 31 57 3 2 62 4 3 70 5 4 25 6 24 94 7 31 57 8 2 62 9 3 70 10 2 62 11 3 70 [12 rows x 2 columns]), (dtype('O'), regiment company name 0 Nighthawks 1st Miller 1 Nighthawks 1st Jacobson 2 Nighthawks 2nd Ali 3 Nighthawks 2nd Milner 4 Dragoons 1st Cooze 5 Dragoons 1st Jacon 6 Dragoons 2nd Ryaner 7 Dragoons 2nd Sone 8 Scouts 1st Sloan 9 Scouts 1st Piger 10 Scouts 2nd Riani 11 Scouts 2nd Ali [12 rows x 3 columns])] In the dataframe \"df\", group by \"regiments, take the mean values of the other variables for those groups, then display them with the prefix_mean df . groupby ( 'regiment' ) . mean () . add_prefix ( 'mean_' ) mean_preTestScore mean_postTestScore regiment Dragoons 15.50 61.5 Nighthawks 15.25 59.5 Scouts 2.50 66.0 3 rows × 2 columns Create a function to get the stats of a group def get_stats ( group ): return { 'min' : group . min (), 'max' : group . max (), 'count' : group . count (), 'mean' : group . mean ()} Create bins and bin up postTestScore by those pins bins = [ 0 , 25 , 50 , 75 , 100 ] group_names = [ 'Low' , 'Okay' , 'Good' , 'Great' ] df [ 'categories' ] = pd . cut ( df [ 'postTestScore' ], bins , labels = group_names ) Apply the get_stats() function to each postTestScore bin df [ 'postTestScore' ] . groupby ( df [ 'categories' ]) . apply ( get_stats ) . unstack () count max mean min categories Good 8 70 63.75 57 Great 2 94 94.00 94 Low 2 25 25.00 25 3 rows × 4 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_apply_operations_to_groups.html"},{"title":"Binning Data In Pandas","loc":"http://chrisalbon.com/python/pandas_binning_data.html","text":"import modules import pandas as pd Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 12 rows × 5 columns Define bins as 0 to 25, 25 to 50, 60 to 75, 75 to 100 bins = [ 0 , 25 , 50 , 75 , 100 ] Create names for the four groups group_names = [ 'Low' , 'Okay' , 'Good' , 'Great' ] Cut postTestScore categories = pd . cut ( df [ 'postTestScore' ], bins , labels = group_names ) df [ 'categories' ] = pd . cut ( df [ 'postTestScore' ], bins , labels = group_names ) categories Low Great Good Good Good Low Great Good Good Good Good Good Name: postTestScore, Levels (4): Index(['Low', 'Okay', 'Good', 'Great'], dtype=object) Count the number of observations which each value pd . value_counts ( df [ 'categories' ]) View the dataframe df regiment company name preTestScore postTestScore scoresBinned categories 0 Nighthawks 1st Miller 4 25 (0, 25] Low 1 Nighthawks 1st Jacobson 24 94 (75, 100] Great 2 Nighthawks 2nd Ali 31 57 (50, 75] Good 3 Nighthawks 2nd Milner 2 62 (50, 75] Good 4 Dragoons 1st Cooze 3 70 (50, 75] Good 5 Dragoons 1st Jacon 4 25 (0, 25] Low 6 Dragoons 2nd Ryaner 24 94 (75, 100] Great 7 Dragoons 2nd Sone 31 57 (50, 75] Good 8 Scouts 1st Sloan 2 62 (50, 75] Good 9 Scouts 1st Piger 3 70 (50, 75] Good 10 Scouts 2nd Riani 2 62 (50, 75] Good 11 Scouts 2nd Ali 3 70 (50, 75] Good 12 rows × 7 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_binning_data.html"},{"title":"Convert A Categorical Variable Into Dummy Variables","loc":"http://chrisalbon.com/python/pandas_convert_categorical_to_dummies.html","text":"# import modules import pandas as pd # Create a dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'sex' : [ 'male' , 'female' , 'male' , 'female' , 'female' ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'sex' ]) df first_name last_name sex 0 Jason Miller male 1 Molly Jacobson female 2 Tina Ali male 3 Jake Milner female 4 Amy Cooze female 5 rows × 3 columns # Create a set of dummy variables from the sex variable df_sex = pd . get_dummies ( df [ 'sex' ]) # Join the dummy variables to the main dataframe df_new = pd . concat ([ df , df_sex ], axis = 1 ) df_new first_name last_name sex female male 0 Jason Miller male 0 1 1 Molly Jacobson female 1 0 2 Tina Ali male 0 1 3 Jake Milner female 1 0 4 Amy Cooze female 1 0 5 rows × 5 columns # Alterative for joining the new columns df_new = df . join ( df_sex ) df_new first_name last_name sex female male 0 Jason Miller male 0 1 1 Molly Jacobson female 1 0 2 Tina Ali male 0 1 3 Jake Milner female 1 0 4 Amy Cooze female 1 0 5 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_categorical_to_dummies.html"},{"title":"Convert A Numeric Categorical Variable With Patsy","loc":"http://chrisalbon.com/python/pandas_convert_numeric_categorical_to_numeric_with_patsy.html","text":"Note: Originally from: Data Origami. # import modules import pandas as pd import patsy # Create dataframe raw_data = { 'countrycode' : [ 1 , 2 , 3 , 2 , 1 ]} df = pd . DataFrame ( raw_data , columns = [ 'countrycode' ]) df countrycode 0 1 1 2 2 3 3 2 4 1 # Convert the countrycode variable into three binary variables patsy . dmatrix ( 'C(countrycode)-1' , df , return_type = 'dataframe' ) C(countrycode)[1] C(countrycode)[2] C(countrycode)[3] 0 1 0 0 1 0 1 0 2 0 0 1 3 0 1 0 4 1 0 0","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_numeric_categorical_to_numeric_with_patsy.html"},{"title":"Convert A String Categorical Variable With Patsy","loc":"http://chrisalbon.com/python/pandas_convert_string_categorical_to_numeric_with_patsy.html","text":"Originally from: Data Origami. import modules import pandas as pd import patsy Create dataframe raw_data = { 'patient' : [ 1 , 1 , 1 , 0 , 0 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 'strong' , 'weak' , 'normal' , 'weak' , 'strong' ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) df patient obs treatment score 0 1 1 0 strong 1 1 2 1 weak 2 1 3 0 normal 3 0 1 1 weak 4 0 2 0 strong Convert df['score'] into a categorical variable ready for regression (i.e. set one category as the baseline) # On the 'score' variable in the df dataframe, convert to a categorical variable, and spit out a dataframe patsy . dmatrix ( 'score' , df , return_type = 'dataframe' ) Intercept score[T.strong] score[T.weak] 0 1 1 0 1 1 0 1 2 1 0 0 3 1 0 1 4 1 1 0 Convert df['score'] into a categorical variable without setting one category as baseline This is likely what you will want to do # On the 'score' variable in the df dataframe, convert to a categorical variable, and spit out a dataframe patsy . dmatrix ( 'score - 1' , df , return_type = 'dataframe' ) score[normal] score[strong] score[weak] 0 0 1 0 1 0 0 1 2 1 0 0 3 0 0 1 4 0 1 0 Create a variable that is \"1\" if the variables of patient and treatment are both 1 patsy . dmatrix ( 'patient + treatment + patient:treatment-1' , df , return_type = 'dataframe' ) patient treatment patient:treatment 0 1 0 0 1 1 1 1 2 1 0 0 3 0 1 0 4 0 0 0","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_string_categorical_to_numeric_with_patsy.html"},{"title":"Convert A Variable To A Time Variable In Pandas","loc":"http://chrisalbon.com/python/pandas_convert_to_datetime.html","text":"# Import Preliminaries import pandas as pd # Create a dataset with the index being a set of names raw_data = { 'date' : [ '2014-06-01T01:21:38.004053' , '2014-06-02T01:21:38.004053' , '2014-06-03T01:21:38.004053' ], 'score' : [ 25 , 94 , 57 ]} df = pd . DataFrame ( raw_data , columns = [ 'date' , 'score' ]) df date score 0 2014-06-01T01:21:38.004053 25 1 2014-06-02T01:21:38.004053 94 2 2014-06-03T01:21:38.004053 57 # Transpose the dataset, so that the index (in this case the names) are columns df [ \"date\" ] = pd . to_datetime ( df [ \"date\" ]) df = df . set_index ( df [ \"date\" ]) df date score date 2014-06-01 01:21:38.004053 2014-06-01 01:21:38.004053 25 2014-06-02 01:21:38.004053 2014-06-02 01:21:38.004053 94 2014-06-03 01:21:38.004053 2014-06-03 01:21:38.004053 57","tags":"Python","url":"http://chrisalbon.com/python/pandas_convert_to_datetime.html"},{"title":"Create a Column Based on a Conditional in pandas","loc":"http://chrisalbon.com/python/pandas_create_column_using_conditional.html","text":"Preliminaries # Import required modules import pandas as pd import numpy as np Make a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , columns = [ 'name' , 'age' , 'preTestScore' , 'postTestScore' ]) df name age preTestScore postTestScore 0 Jason 42 4 25 1 Molly 52 24 94 2 Tina 36 31 57 3 Jake 24 2 62 4 Amy 73 3 70 5 rows × 4 columns Add a new column for elderly # Create a new column called df.elderly where the value is yes # if df.age is greater than 50 and no if not df [ 'elderly' ] = np . where ( df [ 'age' ] >= 50 , 'yes' , 'no' ) # View the dataframe df name age preTestScore postTestScore elderly 0 Jason 42 4 25 no 1 Molly 52 24 94 yes 2 Tina 36 31 57 no 3 Jake 24 2 62 no 4 Amy 73 3 70 yes 5 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_create_column_using_conditional.html"},{"title":"Create A Pandas Column With A For Loop","loc":"http://chrisalbon.com/python/pandas_create_column_with_loop.html","text":"Preliminaries import pandas as pd import numpy as np Create an example dataframe raw_data = { 'student_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'test_score' : [ 76 , 88 , 84 , 67 , 53 , 96 , 64 , 91 , 77 , 73 , 52 , np . NaN ]} df = pd . DataFrame ( raw_data , columns = [ 'student_name' , 'test_score' ]) Create a function to assign letter grades # Create a list to store the data grades = [] # For each row in the column, for row in df [ 'test_score' ]: # if more than a value, if row > 95 : # Append a letter grade grades . append ( 'A' ) # else, if more than a value, elif row > 90 : # Append a letter grade grades . append ( 'A-' ) # else, if more than a value, elif row > 85 : # Append a letter grade grades . append ( 'B' ) # else, if more than a value, elif row > 80 : # Append a letter grade grades . append ( 'B-' ) # else, if more than a value, elif row > 75 : # Append a letter grade grades . append ( 'C' ) # else, if more than a value, elif row > 70 : # Append a letter grade grades . append ( 'C-' ) # else, if more than a value, elif row > 65 : # Append a letter grade grades . append ( 'D' ) # else, if more than a value, elif row > 60 : # Append a letter grade grades . append ( 'D-' ) # otherwise, else : # Append a failing grade grades . append ( 'Failed' ) # Create a column from the list df [ 'grades' ] = grades # View the new dataframe df student_name test_score grades 0 Miller 76 C 1 Jacobson 88 B 2 Ali 84 B- 3 Milner 67 D 4 Cooze 53 Failed 5 Jacon 96 A 6 Ryaner 64 D- 7 Sone 91 A- 8 Sloan 77 C 9 Piger 73 C- 10 Riani 52 Failed 11 Ali NaN Failed","tags":"Python","url":"http://chrisalbon.com/python/pandas_create_column_with_loop.html"},{"title":"Crosstabs In Pandas","loc":"http://chrisalbon.com/python/pandas_crosstabs.html","text":"Import pandas import pandas as pd raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ 'infantry' , 'infantry' , 'cavalry' , 'cavalry' , 'infantry' , 'infantry' , 'cavalry' , 'cavalry' , 'infantry' , 'infantry' , 'cavalry' , 'cavalry' ], 'experience' : [ 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' , 'veteran' , 'rookie' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'experience' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company experience name preTestScore postTestScore 0 Nighthawks infantry veteran Miller 4 25 1 Nighthawks infantry rookie Jacobson 24 94 2 Nighthawks cavalry veteran Ali 31 57 3 Nighthawks cavalry rookie Milner 2 62 4 Dragoons infantry veteran Cooze 3 70 5 Dragoons infantry rookie Jacon 4 25 6 Dragoons cavalry veteran Ryaner 24 94 7 Dragoons cavalry rookie Sone 31 57 8 Scouts infantry veteran Sloan 2 62 9 Scouts infantry rookie Piger 3 70 10 Scouts cavalry veteran Riani 2 62 11 Scouts cavalry rookie Ali 3 70 12 rows × 6 columns Create a crosstab table by company and regiment Counting the number of observations by regiment and category pd . crosstab ( df . regiment , df . company , margins = True ) company cavalry infantry All regiment Dragoons 2 2 4 Nighthawks 2 2 4 Scouts 2 2 4 All 6 6 12 4 rows × 3 columns Create a crosstab of the number of rookie and veteran cavalry and infantry soldiers per regiment pd . crosstab ([ df . company , df . experience ], df . regiment , margins = True ) regiment Dragoons Nighthawks Scouts All company experience cavalry rookie 1 1 1 3 veteran 1 1 1 3 infantry rookie 1 1 1 3 veteran 1 1 1 3 All 4 4 4 12 5 rows × 4 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_crosstabs.html"},{"title":"pandas Data Structures","loc":"http://chrisalbon.com/python/pandas_data_structures.html","text":"Import modules import pandas as pd Series 101 Series are one-dimensional arrays (like R's vectors) Create a series of the number of floodingReports floodingReports = pd . Series ([ 5 , 6 , 2 , 9 , 12 ]) floodingReports 0 5 1 6 2 2 3 9 4 12 dtype: int64 Note that the first column of numbers (0 to 4) are the index. Set county names to be the index of the floodingReports series floodingReports = pd . Series ([ 5 , 6 , 2 , 9 , 12 ], index = [ 'Cochise County' , 'Pima County' , 'Santa Cruz County' , 'Maricopa County' , 'Yuma County' ]) floodingReports Cochise County 5 Pima County 6 Santa Cruz County 2 Maricopa County 9 Yuma County 12 dtype: int64 View the number of floodingReports in Cochise County floodingReports [ 'Cochise County' ] 5 View the counties with more than 6 flooding reports floodingReports [ floodingReports > 6 ] Maricopa County 9 Yuma County 12 dtype: int64 Create a pandas series from a dictionary Note: when you do this, the dict's key's will become the series's index # Create a dictionary fireReports_dict = { 'Cochise County' : 12 , 'Pima County' : 342 , 'Santa Cruz County' : 13 , 'Maricopa County' : 42 , 'Yuma County' : 52 } # Convert the dictionary into a pd.Series, and view it fireReports = pd . Series ( fireReports_dict ); fireReports Cochise County 12 Maricopa County 42 Pima County 342 Santa Cruz County 13 Yuma County 52 dtype: int64 Change the index of a series to shorter names fireReports . index = [ \"Cochice\" , \"Pima\" , \"Santa Cruz\" , \"Maricopa\" , \"Yuma\" ] fireReports Cochice 12 Pima 42 Santa Cruz 342 Maricopa 13 Yuma 52 dtype: int64 DataFrame 101 DataFrames are like R's Dataframes Create a dataframe from a dict of equal length lists or numpy arrays data = { 'county' : [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data ) df county reports year 0 Cochice 4 2012 1 Pima 24 2012 2 Santa Cruz 31 2013 3 Maricopa 2 2014 4 Yuma 3 2014 5 rows × 3 columns Set the order of the columns using the columns attribute dfColumnOrdered = pd . DataFrame ( data , columns = [ 'county' , 'year' , 'reports' ]) dfColumnOrdered county year reports 0 Cochice 2012 4 1 Pima 2012 24 2 Santa Cruz 2013 31 3 Maricopa 2014 2 4 Yuma 2014 3 5 rows × 3 columns Add a column dfColumnOrdered [ 'newsCoverage' ] = pd . Series ([ 42.3 , 92.1 , 12.2 , 39.3 , 30.2 ]) dfColumnOrdered county year reports newsCoverage 0 Cochice 2012 4 42.3 1 Pima 2012 24 92.1 2 Santa Cruz 2013 31 12.2 3 Maricopa 2014 2 39.3 4 Yuma 2014 3 30.2 5 rows × 4 columns Delete a column del dfColumnOrdered [ 'newsCoverage' ] dfColumnOrdered county year reports 0 Cochice 2012 4 1 Pima 2012 24 2 Santa Cruz 2013 31 3 Maricopa 2014 2 4 Yuma 2014 3 5 rows × 3 columns Transpose the dataframe dfColumnOrdered . T 0 1 2 3 4 county Cochice Pima Santa Cruz Maricopa Yuma year 2012 2012 2013 2014 2014 reports 4 24 31 2 3 3 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_data_structures.html"},{"title":"Count Values In Pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_dataframe_count_values.html","text":"Import the pandas module import pandas as pd Create all the columns of the dataframe as series year = pd . Series ([ 1875 , 1876 , 1877 , 1878 , 1879 , 1880 , 1881 , 1882 , 1883 , 1884 , 1885 , 1886 , 1887 , 1888 , 1889 , 1890 , 1891 , 1892 , 1893 , 1894 ]) guardCorps = pd . Series ([ 0 , 2 , 2 , 1 , 0 , 0 , 1 , 1 , 0 , 3 , 0 , 2 , 1 , 0 , 0 , 1 , 0 , 1 , 0 , 1 ]) corps1 = pd . Series ([ 0 , 0 , 0 , 2 , 0 , 3 , 0 , 2 , 0 , 0 , 0 , 1 , 1 , 1 , 0 , 2 , 0 , 3 , 1 , 0 ]) corps2 = pd . Series ([ 0 , 0 , 0 , 2 , 0 , 2 , 0 , 0 , 1 , 1 , 0 , 0 , 2 , 1 , 1 , 0 , 0 , 2 , 0 , 0 ]) corps3 = pd . Series ([ 0 , 0 , 0 , 1 , 1 , 1 , 2 , 0 , 2 , 0 , 0 , 0 , 1 , 0 , 1 , 2 , 1 , 0 , 0 , 0 ]) corps4 = pd . Series ([ 0 , 1 , 0 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 ]) corps5 = pd . Series ([ 0 , 0 , 0 , 0 , 2 , 1 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 1 , 1 , 1 , 1 , 1 , 1 , 0 ]) corps6 = pd . Series ([ 0 , 0 , 1 , 0 , 2 , 0 , 0 , 1 , 2 , 0 , 1 , 1 , 3 , 1 , 1 , 1 , 0 , 3 , 0 , 0 ]) corps7 = pd . Series ([ 1 , 0 , 1 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 2 , 0 , 0 , 2 , 1 , 0 , 2 , 0 ]) corps8 = pd . Series ([ 1 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ]) corps9 = pd . Series ([ 0 , 0 , 0 , 0 , 0 , 2 , 1 , 1 , 1 , 0 , 2 , 1 , 1 , 0 , 1 , 2 , 0 , 1 , 0 , 0 ]) corps10 = pd . Series ([ 0 , 0 , 1 , 1 , 0 , 1 , 0 , 2 , 0 , 2 , 0 , 0 , 0 , 0 , 2 , 1 , 3 , 0 , 1 , 1 ]) corps11 = pd . Series ([ 0 , 0 , 0 , 0 , 2 , 4 , 0 , 1 , 3 , 0 , 1 , 1 , 1 , 1 , 2 , 1 , 3 , 1 , 3 , 1 ]) corps14 = pd . Series ([ 1 , 1 , 2 , 1 , 1 , 3 , 0 , 4 , 0 , 1 , 0 , 3 , 2 , 1 , 0 , 2 , 1 , 1 , 0 , 0 ]) corps15 = pd . Series ([ 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 1 , 1 , 0 , 0 , 0 , 2 , 2 , 0 , 0 , 0 , 0 ]) Create a dictionary variable that assigns variable names variables = dict ( guardCorps = guardCorps , corps1 = corps1 , corps2 = corps2 , corps3 = corps3 , corps4 = corps4 , corps5 = corps5 , corps6 = corps6 , corps7 = corps7 , corps8 = corps8 , corps9 = corps9 , corps10 = corps10 , corps11 = corps11 , corps14 = corps14 , corps15 = corps15 ) Create a dataframe and set the order of the columns using the columns attribute horsekick = pd . DataFrame ( variables , columns = [ 'guardCorps' , 'corps1' , 'corps2' , 'corps3' , 'corps4' , 'corps5' , 'corps6' , 'corps7' , 'corps8' , 'corps9' , 'corps10' , 'corps11' , 'corps14' , 'corps15' ]) Set the dataframe's index to be year horsekick . index = [ 1875 , 1876 , 1877 , 1878 , 1879 , 1880 , 1881 , 1882 , 1883 , 1884 , 1885 , 1886 , 1887 , 1888 , 1889 , 1890 , 1891 , 1892 , 1893 , 1894 ] View the horsekick dataframe horsekick guardCorps corps1 corps2 corps3 corps4 corps5 corps6 corps7 corps8 corps9 corps10 corps11 corps14 corps15 1875 0 0 0 0 0 0 0 1 1 0 0 0 1 0 1876 2 0 0 0 1 0 0 0 0 0 0 0 1 1 1877 2 0 0 0 0 0 1 1 0 0 1 0 2 0 1878 1 2 2 1 1 0 0 0 0 0 1 0 1 0 1879 0 0 0 1 1 2 2 0 1 0 0 2 1 0 1880 0 3 2 1 1 1 0 0 0 2 1 4 3 0 1881 1 0 0 2 1 0 0 1 0 1 0 0 0 0 1882 1 2 0 0 0 0 1 0 1 1 2 1 4 1 1883 0 0 1 2 0 1 2 1 0 1 0 3 0 0 1884 3 0 1 0 0 0 0 1 0 0 2 0 1 1 1885 0 0 0 0 0 0 1 0 0 2 0 1 0 1 1886 2 1 0 0 1 1 1 0 0 1 0 1 3 0 1887 1 1 2 1 0 0 3 2 1 1 0 1 2 0 1888 0 1 1 0 0 1 1 0 0 0 0 1 1 0 1889 0 0 1 1 0 1 1 0 0 1 2 2 0 2 1890 1 2 0 2 0 1 1 2 0 2 1 1 2 2 1891 0 0 0 1 1 1 0 1 1 0 3 3 1 0 1892 1 3 2 0 1 1 3 0 1 1 0 1 1 0 1893 0 1 0 0 0 1 0 2 0 0 1 3 0 0 1894 1 0 0 0 0 0 0 0 1 0 1 1 0 0 Count the number of times each number of deaths occurs in each regiment result = horsekick . apply ( pd . value_counts ) . fillna ( 0 ); result guardCorps corps1 corps2 corps3 corps4 corps5 corps6 corps7 corps8 corps9 corps10 corps11 corps14 corps15 0 9.0 11.0 12.0 11.0 12.0 10.0 9.0 11.0 13.0 10.0 10.0 6 6 14.0 1 7.0 4.0 4.0 6.0 8.0 9.0 7.0 6.0 7.0 7.0 6.0 8 8 4.0 2 3.0 3.0 4.0 3.0 0.0 1.0 2.0 3.0 0.0 3.0 3.0 2 3 2.0 3 1.0 2.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 0.0 1.0 3 2 0.0 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1 1 0.0 Count the number of times each monthly death total appears in guardCorps pd . value_counts ( horsekick [ 'guardCorps' ] . values , sort = False ) 0 9 1 7 2 3 3 1 dtype: int64 List all the unique values in guardCorps horsekick [ 'guardCorps' ] . unique () array([0, 2, 1, 3])","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_count_values.html"},{"title":"Descriptive Statistics For pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_dataframe_descriptive_stats.html","text":"Import modules import pandas as pd Create dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , columns = [ 'name' , 'age' , 'preTestScore' , 'postTestScore' ]) df name age preTestScore postTestScore 0 Jason 42 4 25 1 Molly 52 24 94 2 Tina 36 31 57 3 Jake 24 2 62 4 Amy 73 3 70 5 rows × 4 columns The sum of all the ages df [ 'age' ] . sum () 227 Mean preTestScore df [ 'preTestScore' ] . mean () 12.800000000000001 Cumulative sum of preTestScores, moving from the rows from the top df [ 'preTestScore' ] . cumsum () 0 4 1 28 2 59 3 61 4 64 Name: preTestScore, dtype: int64 Summary statistics on preTestScore df [ 'preTestScore' ] . describe () count 5.000000 mean 12.800000 std 13.663821 min 2.000000 25% 3.000000 50% 4.000000 75% 24.000000 max 31.000000 Name: preTestScore, dtype: float64 Count the number of non-NA values df [ 'preTestScore' ] . count () 5 Minimum value of preTestScore df [ 'preTestScore' ] . min () 2 Maximum value of preTestScore df [ 'preTestScore' ] . max () 31 Median value of preTestScore df [ 'preTestScore' ] . median () 4.0 Sample variance of preTestScore values df [ 'preTestScore' ] . var () 186.69999999999999 Sample standard deviation of preTestScore values df [ 'preTestScore' ] . std () 13.663820841916802 Skewness of preTestScore values df [ 'preTestScore' ] . skew () 0.74334524573267591 Kurtosis of preTestScore values df [ 'preTestScore' ] . kurt () -2.4673543738411525 Correlation Matrix Of Values df . corr () age preTestScore postTestScore age 1.000000 -0.105651 0.328852 preTestScore -0.105651 1.000000 0.378039 postTestScore 0.328852 0.378039 1.000000 3 rows × 3 columns Covariance Matrix Of Values df . cov () age preTestScore postTestScore age 340.80 -26.65 151.20 preTestScore -26.65 186.70 128.65 postTestScore 151.20 128.65 620.30 3 rows × 3 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_descriptive_stats.html"},{"title":"Simple Example Dataframes In Pandas","loc":"http://chrisalbon.com/python/pandas_dataframe_examples.html","text":"import modules import pandas as pd Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 5 rows × 5 columns Create 2nd dataframe raw_data_2 = { 'first_name' : [ 'Sarah' , 'Gueniva' , 'Know' , 'Sara' , 'Cat' ], 'last_name' : [ 'Mornig' , 'Jaker' , 'Alom' , 'Ormon' , 'Koozer' ], 'age' : [ 53 , 26 , 72 , 73 , 24 ], 'preTestScore' : [ 13 , 52 , 72 , 26 , 26 ], 'postTestScore' : [ 82 , 52 , 56 , 234 , 254 ]} df_2 = pd . DataFrame ( raw_data_2 , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df_2 first_name last_name age preTestScore postTestScore 0 Sarah Mornig 53 13 82 1 Gueniva Jaker 26 52 52 2 Know Alom 72 72 56 3 Sara Ormon 73 26 234 4 Cat Koozer 24 26 254 5 rows × 5 columns Create 3rd dataframe raw_data_3 = { 'first_name' : [ 'Sarah' , 'Gueniva' , 'Know' , 'Sara' , 'Cat' ], 'last_name' : [ 'Mornig' , 'Jaker' , 'Alom' , 'Ormon' , 'Koozer' ], 'postTestScore_2' : [ 82 , 52 , 56 , 234 , 254 ]} df_3 = pd . DataFrame ( raw_data_3 , columns = [ 'first_name' , 'last_name' , 'postTestScore_2' ]) df_3 first_name last_name postTestScore_2 0 Sarah Mornig 82 1 Gueniva Jaker 52 2 Know Alom 56 3 Sara Ormon 234 4 Cat Koozer 254 5 rows × 3 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_examples.html"},{"title":"Loading A CSV Into Pandas","loc":"http://chrisalbon.com/python/pandas_dataframe_importing_csv.html","text":"import modules import pandas as pd import numpy as np Create dataframe (that we will be importing) raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , \".\" , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , \".\" , \".\" ], 'postTestScore' : [ \"25,000\" , \"94,000\" , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25,000 1 Molly Jacobson 52 24 94,000 2 Tina . 36 31 57 3 Jake Milner 24 . 62 4 Amy Cooze 73 . 70 Save dataframe as csv in the working director df . to_csv ( '../data/example.csv' ) Load a csv df = pd . read_csv ( '../data/example.csv' ) df Unnamed: 0 first_name last_name age preTestScore postTestScore 0 0 Jason Miller 42 4 25,000 1 1 Molly Jacobson 52 24 94,000 2 2 Tina . 36 31 57 3 3 Jake Milner 24 . 62 4 4 Amy Cooze 73 . 70 Load a csv with no headers df = pd . read_csv ( '../data/example.csv' , header = None ) df 0 1 2 3 4 5 0 NaN first_name last_name age preTestScore postTestScore 1 0.0 Jason Miller 42 4 25,000 2 1.0 Molly Jacobson 52 24 94,000 3 2.0 Tina . 36 31 57 4 3.0 Jake Milner 24 . 62 5 4.0 Amy Cooze 73 . 70 Load a csv while specifying column names df = pd . read_csv ( '../data/example.csv' , names = [ 'UID' , 'First Name' , 'Last Name' , 'Age' , 'Pre-Test Score' , 'Post-Test Score' ]) df UID First Name Last Name Age Pre-Test Score Post-Test Score 0 NaN first_name last_name age preTestScore postTestScore 1 0.0 Jason Miller 42 4 25,000 2 1.0 Molly Jacobson 52 24 94,000 3 2.0 Tina . 36 31 57 4 3.0 Jake Milner 24 . 62 5 4.0 Amy Cooze 73 . 70 Load a csv with setting the index column to UID df = pd . read_csv ( '../data/example.csv' , index_col = 'UID' , names = [ 'UID' , 'First Name' , 'Last Name' , 'Age' , 'Pre-Test Score' , 'Post-Test Score' ]) df First Name Last Name Age Pre-Test Score Post-Test Score UID NaN first_name last_name age preTestScore postTestScore 0.0 Jason Miller 42 4 25,000 1.0 Molly Jacobson 52 24 94,000 2.0 Tina . 36 31 57 3.0 Jake Milner 24 . 62 4.0 Amy Cooze 73 . 70 Load a csv while setting the index columns to First Name and Last Name df = pd . read_csv ( '../data/example.csv' , index_col = [ 'First Name' , 'Last Name' ], names = [ 'UID' , 'First Name' , 'Last Name' , 'Age' , 'Pre-Test Score' , 'Post-Test Score' ]) df UID Age Pre-Test Score Post-Test Score First Name Last Name first_name last_name NaN age preTestScore postTestScore Jason Miller 0.0 42 4 25,000 Molly Jacobson 1.0 52 24 94,000 Tina . 2.0 36 31 57 Jake Milner 3.0 24 . 62 Amy Cooze 4.0 73 . 70 Load a csv while specifying \".\" as missing values df = pd . read_csv ( '../data/example.csv' , na_values = [ '.' ]) pd . isnull ( df ) Unnamed: 0 first_name last_name age preTestScore postTestScore 0 False False False False False False 1 False False False False False False 2 False False True False False False 3 False False False False True False 4 False False False False True False Load a csv while specifying \".\" and \"NA\" as missing values in the Last Name column and \".\" as missing values in Pre-Test Score column sentinels = { 'Last Name' : [ '.' , 'NA' ], 'Pre-Test Score' : [ '.' ]} df = pd . read_csv ( '../data/example.csv' , na_values = sentinels ) df Unnamed: 0 first_name last_name age preTestScore postTestScore 0 0 Jason Miller 42 4 25,000 1 1 Molly Jacobson 52 24 94,000 2 2 Tina . 36 31 57 3 3 Jake Milner 24 . 62 4 4 Amy Cooze 73 . 70 Load a csv while skipping the top 3 rows df = pd . read_csv ( '../data/example.csv' , na_values = sentinels , skiprows = 3 ) df 2 Tina . 36 31 57 0 3 Jake Milner 24 . 62 1 4 Amy Cooze 73 . 70 Load a csv while interpreting \",\" in strings around numbers as thousands seperators df = pd . read_csv ( '../data/example.csv' , thousands = ',' ) df Unnamed: 0 first_name last_name age preTestScore postTestScore 0 0 Jason Miller 42 4 25000 1 1 Molly Jacobson 52 24 94000 2 2 Tina . 36 31 57 3 3 Jake Milner 24 . 62 4 4 Amy Cooze 73 . 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_importing_csv.html"},{"title":"Load Excel Spreadsheet As Pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_dataframe_load_xls.html","text":"# import modules import pandas as pd # Import the excel file and call it xls_file xls_file = pd . ExcelFile ( '../data/example.xls' ) xls_file <pandas.io.excel.ExcelFile at 0x111912be0> # View the excel file's sheet names xls_file . sheet_names ['Sheet1'] # Load the xls file's Sheet1 as a dataframe df = xls_file . parse ( 'Sheet1' ) df year deaths_attacker deaths_defender soldiers_attacker soldiers_defender wounded_attacker wounded_defender 0 1945 425 423 2532 37235 41 14 1 1956 242 264 6346 2523 214 1424 2 1964 323 1231 3341 2133 131 131 3 1969 223 23 6732 1245 12 12 4 1971 783 23 12563 2671 123 34 5 1981 436 42 2356 7832 124 124 6 1982 324 124 253 2622 264 1124 7 1992 3321 631 5277 3331 311 1431 8 1999 262 232 2732 2522 132 122 9 2004 843 213 6278 26773 623 2563","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_load_xls.html"},{"title":"Ranking Rows Of pandas Dataframes","loc":"http://chrisalbon.com/python/pandas_dataframe_ranking_rows.html","text":"# import modules import pandas as pd # Create dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 5 rows × 4 columns # Create a new column that is the rank of the value of coverage in ascending order df [ 'coverageRanked' ] = df [ 'coverage' ] . rank ( ascending = 1 ) df coverage name reports year coverageRanked Cochice 25 Jason 4 2012 1 Pima 94 Molly 24 2012 5 Santa Cruz 57 Tina 31 2013 2 Maricopa 62 Jake 2 2014 3 Yuma 70 Amy 3 2014 4 5 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_ranking_rows.html"},{"title":"Reindexing pandas Series And Dataframes","loc":"http://chrisalbon.com/python/pandas_dataframe_reindexing.html","text":"Series # Import Modules import pandas as pd import numpy as np # Create a pandas series of the risk of fire in Southern Arizona brushFireRisk = pd . Series ([ 34 , 23 , 12 , 23 ], index = [ 'Bisbee' , 'Douglas' , 'Sierra Vista' , 'Tombstone' ]) brushFireRisk Bisbee 34 Douglas 23 Sierra Vista 12 Tombstone 23 dtype: int64 # Reindex the series and create a new series variable brushFireRiskReindexed = brushFireRisk . reindex ([ 'Tombstone' , 'Douglas' , 'Bisbee' , 'Sierra Vista' , 'Barley' , 'Tucson' ]) brushFireRiskReindexed Tombstone 23 Douglas 23 Bisbee 34 Sierra Vista 12 Barley NaN Tucson NaN dtype: float64 # Reindex the series and fill in any missing indexes as 0 brushFireRiskReindexed = brushFireRisk . reindex ([ 'Tombstone' , 'Douglas' , 'Bisbee' , 'Sierra Vista' , 'Barley' , 'Tucson' ], fill_value = 0 ) brushFireRiskReindexed Tombstone 23 Douglas 23 Bisbee 34 Sierra Vista 12 Barley 0 Tucson 0 dtype: int64 DataFrames # Create a dataframe data = { 'county' : [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data ) df county reports year 0 Cochice 4 2012 1 Pima 24 2012 2 Santa Cruz 31 2013 3 Maricopa 2 2014 4 Yuma 3 2014 5 rows × 3 columns # Change the order (the index) of the rows df . reindex ([ 4 , 3 , 2 , 1 , 0 ]) county reports year 4 Yuma 3 2014 3 Maricopa 2 2014 2 Santa Cruz 31 2013 1 Pima 24 2012 0 Cochice 4 2012 5 rows × 3 columns # Change the order (the index) of the columns columnsTitles = [ 'year' , 'reports' , 'county' ] df . reindex ( columns = columnsTitles ) year reports county 0 2012 4 Cochice 1 2012 24 Pima 2 2013 31 Santa Cruz 3 2014 2 Maricopa 4 2014 3 Yuma 5 rows × 3 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dataframe_reindexing.html"},{"title":"Delete Duplicates In Pandas","loc":"http://chrisalbon.com/python/pandas_delete_duplicates.html","text":"import modules import pandas as pd Create dataframe with duplicates raw_data = { 'first_name' : [ 'Jason' , 'Jason' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Miller' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 42 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 4 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 25 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Jason Miller 42 4 25 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Identify which observations are duplicates df . duplicated () 0 False 1 True 2 False 3 False 4 False dtype: bool Drop duplicates df . drop_duplicates () first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Drop duplicates in the first name column, but take the last obs in the duplicated set df . drop_duplicates ([ 'first_name' ], keep = 'last' ) first_name last_name age preTestScore postTestScore 1 Jason Miller 42 4 25 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_delete_duplicates.html"},{"title":"Drop A Column That Contains A Certain String In Pandas","loc":"http://chrisalbon.com/python/pandas_drop_column_containing_certain_string.html","text":"import pandas as pd raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Create a variable that drop columns with column names where the first three letters of the column names was 'pre' cols = [ c for c in df . columns if c . lower ()[: 3 ] != 'pre' ] # Create a df of the columns in the variable cols df = df [ cols ] df regiment company name postTestScore 0 Nighthawks 1st Miller 25 1 Nighthawks 1st Jacobson 94 2 Nighthawks 2nd Ali 57 3 Nighthawks 2nd Milner 62 4 Dragoons 1st Cooze 70 5 Dragoons 1st Jacon 25 6 Dragoons 2nd Ryaner 94 7 Dragoons 2nd Sone 57 8 Scouts 1st Sloan 62 9 Scouts 1st Piger 70 10 Scouts 2nd Riani 62 11 Scouts 2nd Ali 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_drop_column_containing_certain_string.html"},{"title":"Dropping Rows And Columns In pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_dropping_column_and_rows.html","text":"Import modules import pandas as pd Create a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 5 rows × 3 columns Drop an observation (row) df . drop ([ 'Cochice' , 'Pima' ]) name reports year Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 3 rows × 3 columns Drop a variable (column) Note: axis=1 denotes that we are referring to a column, not a row df . drop ( 'reports' , axis = 1 ) name year Cochice Jason 2012 Pima Molly 2012 Santa Cruz Tina 2013 Maricopa Jake 2014 Yuma Amy 2014 5 rows × 2 columns Drop a row if it contains a certain value (in this case, \"Tina\") Specifically: Create a new dataframe called df that includes all rows where the value of a cell in the name column does not equal \"Tina\" df = df [ df . name != 'Tina' ] df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Maricopa Jake 2 2014 Yuma Amy 3 2014 4 rows × 3 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_dropping_column_and_rows.html"},{"title":"Expand Cells Containing Lists Into Their Own Variables In Pandas","loc":"http://chrisalbon.com/python/pandas_expand_cells_containing_lists.html","text":"# import pandas import pandas as pd # create a dataset raw_data = { 'score' : [ 1 , 2 , 3 ], 'tags' : [[ 'apple' , 'pear' , 'guava' ],[ 'truck' , 'car' , 'plane' ],[ 'cat' , 'dog' , 'mouse' ]]} df = pd . DataFrame ( raw_data , columns = [ 'score' , 'tags' ]) # view the dataset df score tags 0 1 [apple, pear, guava] 1 2 [truck, car, plane] 2 3 [cat, dog, mouse] # expand df.tags into its own dataframe tags = df [ 'tags' ] . apply ( pd . Series ) # rename each variable is tags tags = tags . rename ( columns = lambda x : 'tag_' + str ( x )) # view the tags dataframe tags tag_0 tag_1 tag_2 0 apple pear guava 1 truck car plane 2 cat dog mouse # join the tags dataframe back to the original dataframe pd . concat ([ df [:], tags [:]], axis = 1 ) score tags tag_0 tag_1 tag_2 0 1 [apple, pear, guava] apple pear guava 1 2 [truck, car, plane] truck car plane 2 3 [cat, dog, mouse] cat dog mouse","tags":"Python","url":"http://chrisalbon.com/python/pandas_expand_cells_containing_lists.html"},{"title":"Find Largest Value In A Dataframe Column","loc":"http://chrisalbon.com/python/pandas_find_largest_value_in_column.html","text":"# import modules % matplotlib inline import pandas as pd import matplotlib.pyplot as plt import numpy as np # Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 5 rows × 5 columns # Index of the row with the highest value in the preTestScore column df [ 'preTestScore' ] . idxmax () 2","tags":"Python","url":"http://chrisalbon.com/python/pandas_find_largest_value_in_column.html"},{"title":"Find Unique Values In Pandas Dataframes","loc":"http://chrisalbon.com/python/pandas_find_unique_values.html","text":"import pandas as pd import numpy as np raw_data = { 'regiment' : [ '51st' , '29th' , '2nd' , '19th' , '12th' , '101st' , '90th' , '30th' , '193th' , '1st' , '94th' , '91th' ], 'trucks' : [ 'MAZ-7310' , np . nan , 'MAZ-7310' , 'MAZ-7310' , 'Tatra 810' , 'Tatra 810' , 'Tatra 810' , 'Tatra 810' , 'ZIS-150' , 'Tatra 810' , 'ZIS-150' , 'ZIS-150' ], 'tanks' : [ 'Merkava Mark 4' , 'Merkava Mark 4' , 'Merkava Mark 4' , 'Leopard 2A6M' , 'Leopard 2A6M' , 'Leopard 2A6M' , 'Arjun MBT' , 'Leopard 2A6M' , 'Arjun MBT' , 'Arjun MBT' , 'Arjun MBT' , 'Arjun MBT' ], 'aircraft' : [ 'none' , 'none' , 'none' , 'Harbin Z-9' , 'Harbin Z-9' , 'none' , 'Harbin Z-9' , 'SH-60B Seahawk' , 'SH-60B Seahawk' , 'SH-60B Seahawk' , 'SH-60B Seahawk' , 'SH-60B Seahawk' ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'trucks' , 'tanks' , 'aircraft' ]) # View the top few rows df . head () regiment trucks tanks aircraft 0 51st MAZ-7310 Merkava Mark 4 none 1 29th NaN Merkava Mark 4 none 2 2nd MAZ-7310 Merkava Mark 4 none 3 19th MAZ-7310 Leopard 2A6M Harbin Z-9 4 12th Tatra 810 Leopard 2A6M Harbin Z-9 # Create a list of unique values by turning the # pandas column into a set list ( set ( df . trucks )) [nan, 'MAZ-7310', 'ZIS-150', 'Tatra 810'] # Create a list of unique values in df.trucks list ( df [ 'trucks' ] . unique ()) ['MAZ-7310', nan, 'Tatra 810', 'ZIS-150']","tags":"Python","url":"http://chrisalbon.com/python/pandas_find_unique_values.html"},{"title":"Group A Time Series With pandas","loc":"http://chrisalbon.com/python/pandas_group_by_time.html","text":"Import required modules import pandas as pd import numpy as np Create a dataframe df = pd . DataFrame () df [ 'german_army' ] = np . random . randint ( low = 20000 , high = 30000 , size = 100 ) df [ 'allied_army' ] = np . random . randint ( low = 20000 , high = 40000 , size = 100 ) df . index = pd . date_range ( '1/1/2014' , periods = 100 , freq = 'H' ) df . head () german_army allied_army 2014-01-01 00:00:00 28755 33938 2014-01-01 01:00:00 25176 28631 2014-01-01 02:00:00 23261 39685 2014-01-01 03:00:00 28686 27756 2014-01-01 04:00:00 24588 25681 Truncate the dataframe df . truncate ( before = '1/2/2014' , after = '1/3/2014' ) german_army allied_army 2014-01-02 00:00:00 26401 20189 2014-01-02 01:00:00 29958 23934 2014-01-02 02:00:00 24492 39075 2014-01-02 03:00:00 25707 39262 2014-01-02 04:00:00 27129 35961 2014-01-02 05:00:00 27903 25418 2014-01-02 06:00:00 20409 25163 2014-01-02 07:00:00 25736 34794 2014-01-02 08:00:00 24057 27209 2014-01-02 09:00:00 26875 33402 2014-01-02 10:00:00 23963 38575 2014-01-02 11:00:00 27506 31859 2014-01-02 12:00:00 23564 25750 2014-01-02 13:00:00 27958 24365 2014-01-02 14:00:00 24915 38866 2014-01-02 15:00:00 23538 33820 2014-01-02 16:00:00 23361 30080 2014-01-02 17:00:00 27284 22922 2014-01-02 18:00:00 24176 32155 2014-01-02 19:00:00 23924 27763 2014-01-02 20:00:00 23111 32343 2014-01-02 21:00:00 20348 28907 2014-01-02 22:00:00 27136 38634 2014-01-02 23:00:00 28649 29950 2014-01-03 00:00:00 21292 26395 Set the dataframe's index df . index = df . index + pd . DateOffset ( months = 4 , days = 5 ) View the dataframe df . head () german_army allied_army 2014-05-06 00:00:00 28755 33938 2014-05-06 01:00:00 25176 28631 2014-05-06 02:00:00 23261 39685 2014-05-06 03:00:00 28686 27756 2014-05-06 04:00:00 24588 25681 Lead a variable 1 hour df . shift ( 1 ) . head () german_army allied_army 2014-05-06 00:00:00 NaN NaN 2014-05-06 01:00:00 28755.0 33938.0 2014-05-06 02:00:00 25176.0 28631.0 2014-05-06 03:00:00 23261.0 39685.0 2014-05-06 04:00:00 28686.0 27756.0 Lag a variable 1 hour df . shift ( - 1 ) . tail () german_army allied_army 2014-05-09 23:00:00 26903.0 39144.0 2014-05-10 00:00:00 27576.0 39759.0 2014-05-10 01:00:00 25232.0 35246.0 2014-05-10 02:00:00 23391.0 21044.0 2014-05-10 03:00:00 NaN NaN Aggregate into days by summing up the value of each hourly observation df . resample ( 'D' ) . sum () german_army allied_army 2014-05-06 605161 755962 2014-05-07 608100 740396 2014-05-08 589744 700297 2014-05-09 607092 719283 2014-05-10 103102 135193 Aggregate into days by averaging up the value of each hourly observation df . resample ( 'D' ) . mean () german_army allied_army 2014-05-06 25215.041667 31498.416667 2014-05-07 25337.500000 30849.833333 2014-05-08 24572.666667 29179.041667 2014-05-09 25295.500000 29970.125000 2014-05-10 25775.500000 33798.250000 Aggregate into days by taking the min value up the value of each hourly observation df . resample ( 'D' ) . median () german_army allied_army 2014-05-06 24882.0 31310.0 2014-05-07 25311.0 30969.5 2014-05-08 24422.5 28318.0 2014-05-09 24941.5 32082.5 2014-05-10 26067.5 37195.0 Aggregate into days by taking the median value of each day's worth of hourly observation df . resample ( 'D' ) . median () german_army allied_army 2014-05-06 24882.0 31310.0 2014-05-07 25311.0 30969.5 2014-05-08 24422.5 28318.0 2014-05-09 24941.5 32082.5 2014-05-10 26067.5 37195.0 Aggregate into days by taking the first value of each day's worth of hourly observation df . resample ( 'D' ) . first () german_army allied_army 2014-05-06 28755 33938 2014-05-07 26401 20189 2014-05-08 21292 26395 2014-05-09 25764 22613 2014-05-10 26903 39144 Aggregate into days by taking the last value of each day's worth of hourly observation df . resample ( 'D' ) . last () german_army allied_army 2014-05-06 28214 32110 2014-05-07 28649 29950 2014-05-08 28379 32600 2014-05-09 26752 22379 2014-05-10 23391 21044 Aggregate into days by taking the first, last, highest, and lowest value of each day's worth of hourly observation df . resample ( 'D' ) . ohlc () german_army allied_army open high low close open high low close 2014-05-06 28755 29206 20037 28214 33938 39955 23417 32110 2014-05-07 26401 29958 20348 28649 20189 39262 20189 29950 2014-05-08 21292 29786 20296 28379 26395 38197 20404 32600 2014-05-09 25764 29952 20738 26752 22613 39695 20189 22379 2014-05-10 26903 27576 23391 23391 39144 39759 21044 21044","tags":"Python","url":"http://chrisalbon.com/python/pandas_group_by_time.html"},{"title":"Grouping Rows In Pandas","loc":"http://chrisalbon.com/python/pandas_group_rows_by.html","text":"# Import modules import pandas as pd # Example dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 # Create a grouping object. In other words, create an object that # represents that particular grouping. In this case we group # pre-test scores by the regiment. regiment_preScore = df [ 'preTestScore' ] . groupby ( df [ 'regiment' ]) # Display the mean value of the each regiment's pre-test score regiment_preScore . mean () regiment Dragoons 15.50 Nighthawks 15.25 Scouts 2.50 dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/pandas_group_rows_by.html"},{"title":"Hierarchical Data In Pandas","loc":"http://chrisalbon.com/python/pandas_hierarchical_data.html","text":"# import modules import pandas as pd # Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 12 rows × 5 columns # Set the hierarchical index but leave the columns inplace df . set_index ([ 'regiment' , 'company' ], drop = False ) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 12 rows × 5 columns # Set the hierarchical index to be by regiment, and then by company df = df . set_index ([ 'regiment' , 'company' ]) df name preTestScore postTestScore regiment company Nighthawks 1st Miller 4 25 1st Jacobson 24 94 2nd Ali 31 57 2nd Milner 2 62 Dragoons 1st Cooze 3 70 1st Jacon 4 25 2nd Ryaner 24 94 2nd Sone 31 57 Scouts 1st Sloan 2 62 1st Piger 3 70 2nd Riani 2 62 2nd Ali 3 70 12 rows × 3 columns # View the index df . index MultiIndex(levels=[['Dragoons', 'Nighthawks', 'Scouts'], ['1st', '2nd']], labels=[[1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2], [0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]], names=['regiment', 'company']) # Swap the levels in the index df . swaplevel ( 'regiment' , 'company' ) name preTestScore postTestScore company regiment 1st Nighthawks Miller 4 25 Nighthawks Jacobson 24 94 2nd Nighthawks Ali 31 57 Nighthawks Milner 2 62 1st Dragoons Cooze 3 70 Dragoons Jacon 4 25 2nd Dragoons Ryaner 24 94 Dragoons Sone 31 57 1st Scouts Sloan 2 62 Scouts Piger 3 70 2nd Scouts Riani 2 62 Scouts Ali 3 70 12 rows × 3 columns # Summarize the results by regiment df . sum ( level = 'regiment' ) preTestScore postTestScore regiment Dragoons 62 246 Nighthawks 61 238 Scouts 10 264 3 rows × 2 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_hierarchical_data.html"},{"title":"Index, Select, And Filter pandas Dataframes","loc":"http://chrisalbon.com/python/pandas_index_select_and_filter.html","text":"Import modules import pandas as pd Create a dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ], 'coverage' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 5 rows × 4 columns View a column of the dataframe df [ 'name' ] Cochice Jason Pima Molly Santa Cruz Tina Maricopa Jake Yuma Amy Name: name, dtype: object View two columns of the dataframe df [[ 'name' , 'reports' ]] name reports Cochice Jason 4 Pima Molly 24 Santa Cruz Tina 31 Maricopa Jake 2 Yuma Amy 3 5 rows × 2 columns View the first two rows of the dataframe df [: 2 ] coverage name reports year Cochice 25 Jason 4 2012 Pima 94 Molly 24 2012 2 rows × 4 columns View all rows where coverage is more than 50 df [ df [ 'coverage' ] > 50 ] coverage name reports year Pima 94 Molly 24 2012 Santa Cruz 57 Tina 31 2013 Maricopa 62 Jake 2 2014 Yuma 70 Amy 3 2014 4 rows × 4 columns View a row df . ix [ 'Maricopa' ] coverage 62 name Jake reports 2 year 2014 Name: Maricopa, dtype: object View a column df . ix [:, 'coverage' ] Cochice 25 Pima 94 Santa Cruz 57 Maricopa 62 Yuma 70 Name: coverage, dtype: int64 View the value based on a row and column df . ix [ 'Yuma' , 'coverage' ] 70","tags":"Python","url":"http://chrisalbon.com/python/pandas_index_select_and_filter.html"},{"title":"Indexing and Selecting Data With Pandas","loc":"http://chrisalbon.com/python/pandas_indexing_selecting.html","text":"Short verison: .iloc[ row , column ] # import the pandas module import pandas as pd # Create an example dataframe about a fictional army raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'deaths' : [ 523 , 52 , 25 , 616 , 43 , 234 , 523 , 62 , 62 , 73 , 37 , 35 ], 'battles' : [ 5 , 42 , 2 , 2 , 4 , 7 , 8 , 3 , 4 , 7 , 8 , 9 ], 'size' : [ 1045 , 957 , 1099 , 1400 , 1592 , 1006 , 987 , 849 , 973 , 1005 , 1099 , 1523 ], 'veterans' : [ 1 , 5 , 62 , 26 , 73 , 37 , 949 , 48 , 48 , 435 , 63 , 345 ], 'readiness' : [ 1 , 2 , 3 , 3 , 2 , 1 , 2 , 3 , 2 , 1 , 2 , 3 ], 'armored' : [ 1 , 0 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0 , 1 , 1 ], 'deserters' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'origin' : [ 'Arizona' , 'California' , 'Texas' , 'Florida' , 'Maine' , 'Iowa' , 'Alaska' , 'Washington' , 'Oregon' , 'Wyoming' , 'Louisana' , 'Georgia' ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'deaths' , 'battles' , 'size' , 'veterans' , 'readiness' , 'armored' , 'deserters' , 'origin' ]) df = df . set_index ( 'origin' ) df . head () regiment company deaths battles size veterans readiness armored deserters origin Arizona Nighthawks 1st 523 5 1045 1 1 1 4 California Nighthawks 1st 52 42 957 5 2 0 24 Texas Nighthawks 2nd 25 2 1099 62 3 1 31 Florida Nighthawks 2nd 616 2 1400 26 3 1 2 Maine Dragoons 1st 43 4 1592 73 2 0 3 Select a column df [ 'size' ] origin Arizona 1045 California 957 Texas 1099 Florida 1400 Maine 1592 Iowa 1006 Alaska 987 Washington 849 Oregon 973 Wyoming 1005 Louisana 1099 Georgia 1523 Name: size, dtype: int64 Select multiple columns df [[ 'size' , 'veterans' ]] size veterans origin Arizona 1045 1 California 957 5 Texas 1099 62 Florida 1400 26 Maine 1592 73 Iowa 1006 37 Alaska 987 949 Washington 849 48 Oregon 973 48 Wyoming 1005 435 Louisana 1099 63 Georgia 1523 345 Select all rows by index label # Select all rows with the index label \"Arizona\" df . loc [: 'Arizona' ] regiment company deaths battles size veterans readiness armored deserters origin Arizona Nighthawks 1st 523 5 1045 1 1 1 4 Select rows by row number # Select every row up to 3 df . iloc [: 2 ] regiment company deaths battles size veterans readiness armored deserters origin Arizona Nighthawks 1st 523 5 1045 1 1 1 4 California Nighthawks 1st 52 42 957 5 2 0 24 # Select the second and third row df . iloc [ 1 : 2 ] regiment company deaths battles size veterans readiness armored deserters origin California Nighthawks 1st 52 42 957 5 2 0 24 # Select every row after the third row df . iloc [ 2 :] regiment company deaths battles size veterans readiness armored deserters origin Texas Nighthawks 2nd 25 2 1099 62 3 1 31 Florida Nighthawks 2nd 616 2 1400 26 3 1 2 Maine Dragoons 1st 43 4 1592 73 2 0 3 Iowa Dragoons 1st 234 7 1006 37 1 1 4 Alaska Dragoons 2nd 523 8 987 949 2 0 24 Washington Dragoons 2nd 62 3 849 48 3 1 31 Oregon Scouts 1st 62 4 973 48 2 0 2 Wyoming Scouts 1st 73 7 1005 435 1 0 3 Louisana Scouts 2nd 37 8 1099 63 2 1 2 Georgia Scouts 2nd 35 9 1523 345 3 1 3 Select columns by column position # Select the first 2 columns df . iloc [:,: 2 ] regiment company origin Arizona Nighthawks 1st California Nighthawks 1st Texas Nighthawks 2nd Florida Nighthawks 2nd Maine Dragoons 1st Iowa Dragoons 1st Alaska Dragoons 2nd Washington Dragoons 2nd Oregon Scouts 1st Wyoming Scouts 1st Louisana Scouts 2nd Georgia Scouts 2nd Select by conditionals (boolean) # Select rows where df.deaths is greater than 50 df [ df [ 'deaths' ] > 50 ] regiment company deaths battles size veterans readiness armored deserters origin Arizona Nighthawks 1st 523 5 1045 1 1 1 4 California Nighthawks 1st 52 42 957 5 2 0 24 Florida Nighthawks 2nd 616 2 1400 26 3 1 2 Iowa Dragoons 1st 234 7 1006 37 1 1 4 Alaska Dragoons 2nd 523 8 987 949 2 0 24 Washington Dragoons 2nd 62 3 849 48 3 1 31 Oregon Scouts 1st 62 4 973 48 2 0 2 Wyoming Scouts 1st 73 7 1005 435 1 0 3 # Select rows where df.deaths is greater than 500 or less than 50 df [( df [ 'deaths' ] > 500 ) | ( df [ 'deaths' ] < 50 )] regiment company deaths battles size veterans readiness armored deserters origin Arizona Nighthawks 1st 523 5 1045 1 1 1 4 Texas Nighthawks 2nd 25 2 1099 62 3 1 31 Florida Nighthawks 2nd 616 2 1400 26 3 1 2 Maine Dragoons 1st 43 4 1592 73 2 0 3 Alaska Dragoons 2nd 523 8 987 949 2 0 24 Louisana Scouts 2nd 37 8 1099 63 2 1 2 Georgia Scouts 2nd 35 9 1523 345 3 1 3 # Select all the regiments not named \"Dragoons\" df [ ~ ( df [ 'regiment' ] == 'Dragoons' )] regiment company deaths battles size veterans readiness armored deserters origin Arizona Nighthawks 1st 523 5 1045 1 1 1 4 California Nighthawks 1st 52 42 957 5 2 0 24 Texas Nighthawks 2nd 25 2 1099 62 3 1 31 Florida Nighthawks 2nd 616 2 1400 26 3 1 2 Oregon Scouts 1st 62 4 973 48 2 0 2 Wyoming Scouts 1st 73 7 1005 435 1 0 3 Louisana Scouts 2nd 37 8 1099 63 2 1 2 Georgia Scouts 2nd 35 9 1523 345 3 1 3 .ix .ix is the combination of both .loc and .iloc. Integers are first considered labels, but if not found, falls back on positional indexing # Select the rows called Texas and Arizona df . ix [[ 'Arizona' , 'Texas' ]] regiment company deaths battles size veterans readiness armored deserters Arizona Nighthawks 1st 523 5 1045 1 1 1 4 Texas Nighthawks 2nd 25 2 1099 62 3 1 31 # Select the third cell in the row named Arizona df . ix [ 'Arizona' , 'deaths' ] 523 # Select the third cell in the row named Arizona df . ix [ 'Arizona' , 2 ] 523 # Select the third cell down in the column named deaths df . ix [ 2 , 'deaths' ] 25","tags":"Python","url":"http://chrisalbon.com/python/pandas_indexing_selecting.html"},{"title":"Join And Merge Pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_join_merge_dataframe.html","text":"import modules import pandas as pd from IPython.display import display from IPython.display import Image Create a dataframe raw_data = { 'subject_id' : [ '1' , '2' , '3' , '4' , '5' ], 'first_name' : [ 'Alex' , 'Amy' , 'Allen' , 'Alice' , 'Ayoung' ], 'last_name' : [ 'Anderson' , 'Ackerman' , 'Ali' , 'Aoni' , 'Atiches' ]} df_a = pd . DataFrame ( raw_data , columns = [ 'subject_id' , 'first_name' , 'last_name' ]) df_a subject_id first_name last_name 0 1 Alex Anderson 1 2 Amy Ackerman 2 3 Allen Ali 3 4 Alice Aoni 4 5 Ayoung Atiches Create a second dataframe raw_data = { 'subject_id' : [ '4' , '5' , '6' , '7' , '8' ], 'first_name' : [ 'Billy' , 'Brian' , 'Bran' , 'Bryce' , 'Betty' ], 'last_name' : [ 'Bonder' , 'Black' , 'Balwner' , 'Brice' , 'Btisan' ]} df_b = pd . DataFrame ( raw_data , columns = [ 'subject_id' , 'first_name' , 'last_name' ]) df_b subject_id first_name last_name 0 4 Billy Bonder 1 5 Brian Black 2 6 Bran Balwner 3 7 Bryce Brice 4 8 Betty Btisan Create a third dataframe raw_data = { 'subject_id' : [ '1' , '2' , '3' , '4' , '5' , '7' , '8' , '9' , '10' , '11' ], 'test_id' : [ 51 , 15 , 15 , 61 , 16 , 14 , 15 , 1 , 61 , 16 ]} df_n = pd . DataFrame ( raw_data , columns = [ 'subject_id' , 'test_id' ]) df_n subject_id test_id 0 1 51 1 2 15 2 3 15 3 4 61 4 5 16 5 7 14 6 8 15 7 9 1 8 10 61 9 11 16 Join the two dataframes along rows df_new = pd . concat ([ df_a , df_b ]) df_new subject_id first_name last_name 0 1 Alex Anderson 1 2 Amy Ackerman 2 3 Allen Ali 3 4 Alice Aoni 4 5 Ayoung Atiches 0 4 Billy Bonder 1 5 Brian Black 2 6 Bran Balwner 3 7 Bryce Brice 4 8 Betty Btisan Join the two dataframes along columns pd . concat ([ df_a , df_b ], axis = 1 ) subject_id first_name last_name subject_id first_name last_name 0 1 Alex Anderson 4 Billy Bonder 1 2 Amy Ackerman 5 Brian Black 2 3 Allen Ali 6 Bran Balwner 3 4 Alice Aoni 7 Bryce Brice 4 5 Ayoung Atiches 8 Betty Btisan Merge two dataframes along the subject_id value pd . merge ( df_new , df_n , on = 'subject_id' ) subject_id first_name last_name test_id 0 1 Alex Anderson 51 1 2 Amy Ackerman 15 2 3 Allen Ali 15 3 4 Alice Aoni 61 4 4 Billy Bonder 61 5 5 Ayoung Atiches 16 6 5 Brian Black 16 7 7 Bryce Brice 14 8 8 Betty Btisan 15 Merge two dataframes with both the left and right dataframes using the subject_id key pd . merge ( df_new , df_n , left_on = 'subject_id' , right_on = 'subject_id' ) subject_id first_name last_name test_id 0 1 Alex Anderson 51 1 2 Amy Ackerman 15 2 3 Allen Ali 15 3 4 Alice Aoni 61 4 4 Billy Bonder 61 5 5 Ayoung Atiches 16 6 5 Brian Black 16 7 7 Bryce Brice 14 8 8 Betty Btisan 15 Merge with outer join \"Full outer join produces the set of all records in Table A and Table B, with matching records from both sides where available. If there is no match, the missing side will contain null.\" - source pd . merge ( df_a , df_b , on = 'subject_id' , how = 'outer' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 1 Alex Anderson NaN NaN 1 2 Amy Ackerman NaN NaN 2 3 Allen Ali NaN NaN 3 4 Alice Aoni Billy Bonder 4 5 Ayoung Atiches Brian Black 5 6 NaN NaN Bran Balwner 6 7 NaN NaN Bryce Brice 7 8 NaN NaN Betty Btisan Merge with inner join \"Inner join produces only the set of records that match in both Table A and Table B.\" - source pd . merge ( df_a , df_b , on = 'subject_id' , how = 'inner' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 4 Alice Aoni Billy Bonder 1 5 Ayoung Atiches Brian Black Merge with right join pd . merge ( df_a , df_b , on = 'subject_id' , how = 'right' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 4 Alice Aoni Billy Bonder 1 5 Ayoung Atiches Brian Black 2 6 NaN NaN Bran Balwner 3 7 NaN NaN Bryce Brice 4 8 NaN NaN Betty Btisan Merge with left join \"Left outer join produces a complete set of records from Table A, with the matching records (where available) in Table B. If there is no match, the right side will contain null.\" - source pd . merge ( df_a , df_b , on = 'subject_id' , how = 'left' ) subject_id first_name_x last_name_x first_name_y last_name_y 0 1 Alex Anderson NaN NaN 1 2 Amy Ackerman NaN NaN 2 3 Allen Ali NaN NaN 3 4 Alice Aoni Billy Bonder 4 5 Ayoung Atiches Brian Black Merge while adding a suffix to duplicate column names pd . merge ( df_a , df_b , on = 'subject_id' , how = 'left' , suffixes = ( '_left' , '_right' )) subject_id first_name_left last_name_left first_name_right last_name_right 0 1 Alex Anderson NaN NaN 1 2 Amy Ackerman NaN NaN 2 3 Allen Ali NaN NaN 3 4 Alice Aoni Billy Bonder 4 5 Ayoung Atiches Brian Black Merge based on indexes pd . merge ( df_a , df_b , right_index = True , left_index = True ) subject_id_x first_name_x last_name_x subject_id_y first_name_y last_name_y 0 1 Alex Anderson 4 Billy Bonder 1 2 Amy Ackerman 5 Brian Black 2 3 Allen Ali 6 Bran Balwner 3 4 Alice Aoni 7 Bryce Brice 4 5 Ayoung Atiches 8 Betty Btisan","tags":"Python","url":"http://chrisalbon.com/python/pandas_join_merge_dataframe.html"},{"title":"Using List Comprehensions With Pandas","loc":"http://chrisalbon.com/python/pandas_list_comprehension.html","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 List Comprehensions As a loop # Create a variable next_year = [] # For each row in df.years, for row in df [ 'year' ]: # Add 1 to the row and append it to next_year next_year . append ( row + 1 ) # Create df.next_year df [ 'next_year' ] = next_year # View the dataframe df name reports year next_year previous_year Cochice Jason 4 2012 2013 2011 Pima Molly 24 2012 2013 2011 Santa Cruz Tina 31 2013 2014 2012 Maricopa Jake 2 2014 2015 2013 Yuma Amy 3 2014 2015 2013 As list comprehension # Subtract 1 from row, for each row in df.year df [ 'previous_year' ] = [ row - 1 for row in df [ 'year' ]] df name reports year next_year previous_year Cochice Jason 4 2012 2013 2011 Pima Molly 24 2012 2013 2011 Santa Cruz Tina 31 2013 2014 2012 Maricopa Jake 2 2014 2015 2013 Yuma Amy 3 2014 2015 2013","tags":"Python","url":"http://chrisalbon.com/python/pandas_list_comprehension.html"},{"title":"List Unique Values In A Pandas Column","loc":"http://chrisalbon.com/python/pandas_list_unique_values_in_column.html","text":"Special thanks to Bob Haffner for pointing out a better way of doing it. Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 List unique values #List unique values in the df['name'] column df . name . unique () array(['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], dtype=object)","tags":"Python","url":"http://chrisalbon.com/python/pandas_list_unique_values_in_column.html"},{"title":"Pandas: Long To Wide Format","loc":"http://chrisalbon.com/python/pandas_long_to_wide.html","text":"import modules import pandas as pd Create \"long\" dataframe raw_data = { 'patient' : [ 1 , 1 , 1 , 2 , 2 ], 'obs' : [ 1 , 2 , 3 , 1 , 2 ], 'treatment' : [ 0 , 1 , 0 , 1 , 0 ], 'score' : [ 6252 , 24243 , 2345 , 2342 , 23525 ]} df = pd . DataFrame ( raw_data , columns = [ 'patient' , 'obs' , 'treatment' , 'score' ]) df patient obs treatment score 0 1 1 0 6252 1 1 2 1 24243 2 1 3 0 2345 3 2 1 1 2342 4 2 2 0 23525 Make a \"wide\" data Now we will create a \"wide\" dataframe with the rows by patient number, the columns being by observation number, and the cell values being the score values. df . pivot ( index = 'patient' , columns = 'obs' , values = 'score' ) obs 1 2 3 patient 1 6252.0 24243.0 2345.0 2 2342.0 23525.0 NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_long_to_wide.html"},{"title":"Lower Case Column Names In Pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_lowercase_column_names.html","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'NAME' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'YEAR' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'REPORTS' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df NAME REPORTS YEAR Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 Lowercase column values # Map the lowering function to all column names df . columns = map ( str . lower , df . columns ) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/pandas_lowercase_column_names.html"},{"title":"Make New Columns Using Functions","loc":"http://chrisalbon.com/python/pandas_make_new_columns_using_functions.html","text":"# Import modules import pandas as pd # Example dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' , 'Jacon' , 'Ryaner' , 'Sone' , 'Sloan' , 'Piger' , 'Riani' , 'Ali' ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 , 25 , 94 , 57 , 62 , 70 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'name' , 'preTestScore' , 'postTestScore' ]) df regiment company name preTestScore postTestScore 0 Nighthawks 1st Miller 4 25 1 Nighthawks 1st Jacobson 24 94 2 Nighthawks 2nd Ali 31 57 3 Nighthawks 2nd Milner 2 62 4 Dragoons 1st Cooze 3 70 5 Dragoons 1st Jacon 4 25 6 Dragoons 2nd Ryaner 24 94 7 Dragoons 2nd Sone 31 57 8 Scouts 1st Sloan 2 62 9 Scouts 1st Piger 3 70 10 Scouts 2nd Riani 2 62 11 Scouts 2nd Ali 3 70 Create one column as a function of two columns # Create a function that takes two inputs, pre and post def pre_post_difference ( pre , post ): # returns the difference between post and pre return post - pre # Create a variable that is the output of the function df [ 'score_change' ] = pre_post_difference ( df [ 'preTestScore' ], df [ 'postTestScore' ]) # View the dataframe df regiment company name preTestScore postTestScore score_change 0 Nighthawks 1st Miller 4 25 21 1 Nighthawks 1st Jacobson 24 94 70 2 Nighthawks 2nd Ali 31 57 26 3 Nighthawks 2nd Milner 2 62 60 4 Dragoons 1st Cooze 3 70 67 5 Dragoons 1st Jacon 4 25 21 6 Dragoons 2nd Ryaner 24 94 70 7 Dragoons 2nd Sone 31 57 26 8 Scouts 1st Sloan 2 62 60 9 Scouts 1st Piger 3 70 67 10 Scouts 2nd Riani 2 62 60 11 Scouts 2nd Ali 3 70 67 Create two columns as a function of one column # Create a function that takes one input, x def score_multipler_2x_and_3x ( x ): # returns two things, x multiplied by 2 and x multiplied by 3 return x * 2 , x * 3 # Create two new variables that take the two outputs of the function df [ 'post_score_x2' ], df [ 'post_score_x3' ] = zip ( * df [ 'postTestScore' ] . map ( score_multipler_2x_and_3x )) df regiment company name preTestScore postTestScore score_change post_score_x2 post_score_x3 0 Nighthawks 1st Miller 4 25 21 50 75 1 Nighthawks 1st Jacobson 24 94 70 188 282 2 Nighthawks 2nd Ali 31 57 26 114 171 3 Nighthawks 2nd Milner 2 62 60 124 186 4 Dragoons 1st Cooze 3 70 67 140 210 5 Dragoons 1st Jacon 4 25 21 50 75 6 Dragoons 2nd Ryaner 24 94 70 188 282 7 Dragoons 2nd Sone 31 57 26 114 171 8 Scouts 1st Sloan 2 62 60 124 186 9 Scouts 1st Piger 3 70 67 140 210 10 Scouts 2nd Riani 2 62 60 124 186 11 Scouts 2nd Ali 3 70 67 140 210","tags":"Python","url":"http://chrisalbon.com/python/pandas_make_new_columns_using_functions.html"},{"title":"Map External Values To Dataframe Values in Pandas","loc":"http://chrisalbon.com/python/pandas_map_values_to_values.html","text":"import modules import pandas as pd Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'city' : [ 'San Francisco' , 'Baltimore' , 'Miami' , 'Douglas' , 'Boston' ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'city' ]) df first_name last_name age city 0 Jason Miller 42 San Francisco 1 Molly Jacobson 52 Baltimore 2 Tina Ali 36 Miami 3 Jake Milner 24 Douglas 4 Amy Cooze 73 Boston 5 rows × 4 columns Create a dictionary of values city_to_state = { 'San Francisco' : 'California' , 'Baltimore' : 'Maryland' , 'Miami' : 'Florida' , 'Douglas' : 'Arizona' , 'Boston' : 'Massachusetts' } Map the values of the city_to_state dictionary to the values in the city variable df [ 'state' ] = df [ 'city' ] . map ( city_to_state ) df first_name last_name age city state 0 Jason Miller 42 San Francisco California 1 Molly Jacobson 52 Baltimore Maryland 2 Tina Ali 36 Miami Florida 3 Jake Milner 24 Douglas Arizona 4 Amy Cooze 73 Boston Massachusetts 5 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_map_values_to_values.html"},{"title":"Missing Data In Pandas Dataframes","loc":"http://chrisalbon.com/python/pandas_missing_data.html","text":"import modules import pandas as pd import numpy as np Create dataframe with missing values raw_data = { 'first_name' : [ 'Jason' , np . nan , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , np . nan , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , np . nan , 36 , 24 , 73 ], 'sex' : [ 'm' , np . nan , 'f' , 'm' , 'f' ], 'preTestScore' : [ 4 , np . nan , np . nan , 2 , 3 ], 'postTestScore' : [ 25 , np . nan , np . nan , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'sex' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 1 NaN NaN NaN NaN NaN NaN 2 Tina Ali 36.0 f NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Drop missing observations df_no_missing = df . dropna () df_no_missing first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Drop rows where all cells in that row is NA df_cleaned = df . dropna ( how = 'all' ) df_cleaned first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 2 Tina Ali 36.0 f NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Create a new column full of missing values df [ 'location' ] = np . nan df first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 1 NaN NaN NaN NaN NaN NaN NaN 2 Tina Ali 36.0 f NaN NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Drop column if they only contain missing values df . dropna ( axis = 1 , how = 'all' ) first_name last_name age sex preTestScore postTestScore 0 Jason Miller 42.0 m 4.0 25.0 1 NaN NaN NaN NaN NaN NaN 2 Tina Ali 36.0 f NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 4 Amy Cooze 73.0 f 3.0 70.0 Drop rows that contain less than five observations This is really mostly useful for time series df . dropna ( thresh = 5 ) first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Fill in missing data with zeros df . fillna ( 0 ) first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 0.0 1 0 0 0.0 0 0.0 0.0 0.0 2 Tina Ali 36.0 f 0.0 0.0 0.0 3 Jake Milner 24.0 m 2.0 62.0 0.0 4 Amy Cooze 73.0 f 3.0 70.0 0.0 Fill in missing in preTestScore with the mean value of preTestScore inplace=True means that the changes are saved to the df right away df [ \"preTestScore\" ] . fillna ( df [ \"preTestScore\" ] . mean (), inplace = True ) df first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 1 NaN NaN NaN NaN 3.0 NaN NaN 2 Tina Ali 36.0 f 3.0 NaN NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Fill in missing in postTestScore with each sex's mean value of postTestScore df [ \"postTestScore\" ] . fillna ( df . groupby ( \"sex\" )[ \"postTestScore\" ] . transform ( \"mean\" ), inplace = True ) df first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 1 NaN NaN NaN NaN 3.0 NaN NaN 2 Tina Ali 36.0 f 3.0 70.0 NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN Select some raws but ignore the missing data points # Select the rows of df where age is not NaN and sex is not NaN df [ df [ 'age' ] . notnull () & df [ 'sex' ] . notnull ()] first_name last_name age sex preTestScore postTestScore location 0 Jason Miller 42.0 m 4.0 25.0 NaN 2 Tina Ali 36.0 f 3.0 70.0 NaN 3 Jake Milner 24.0 m 2.0 62.0 NaN 4 Amy Cooze 73.0 f 3.0 70.0 NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_missing_data.html"},{"title":"Moving Averages In Pandas","loc":"http://chrisalbon.com/python/pandas_moving_average.html","text":"Import modules import pandas as pd Create a dataframe data = { 'score' : [ 1 , 1 , 1 , 2 , 2 , 2 , 3 , 3 , 3 ]} df = pd . DataFrame ( data ) df score 0 1 1 1 2 1 3 2 4 2 5 2 6 3 7 3 8 3 # Calculate the moving average. That is, take # the first two values, average them, # then drop the first and add the third, etc. pd . rolling_mean ( df , 2 ) score 0 NaN 1 1.0 2 1.0 3 1.5 4 2.0 5 2.0 6 2.5 7 3.0 8 3.0","tags":"Python","url":"http://chrisalbon.com/python/pandas_moving_average.html"},{"title":"Pivot Tables In Pandas","loc":"http://chrisalbon.com/python/pandas_pivot_tables.html","text":"import modules import pandas as pd Create dataframe raw_data = { 'regiment' : [ 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Nighthawks' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Dragoons' , 'Scouts' , 'Scouts' , 'Scouts' , 'Scouts' ], 'company' : [ '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' , '1st' , '1st' , '2nd' , '2nd' ], 'TestScore' : [ 4 , 24 , 31 , 2 , 3 , 4 , 24 , 31 , 2 , 3 , 2 , 3 ]} df = pd . DataFrame ( raw_data , columns = [ 'regiment' , 'company' , 'TestScore' ]) df regiment company TestScore 0 Nighthawks 1st 4 1 Nighthawks 1st 24 2 Nighthawks 2nd 31 3 Nighthawks 2nd 2 4 Dragoons 1st 3 5 Dragoons 1st 4 6 Dragoons 2nd 24 7 Dragoons 2nd 31 8 Scouts 1st 2 9 Scouts 1st 3 10 Scouts 2nd 2 11 Scouts 2nd 3 Create a pivot table of group means, by company and regiment pd . pivot_table ( df , index = [ 'regiment' , 'company' ], aggfunc = 'mean' ) TestScore regiment company Dragoons 1st 3.5 2nd 27.5 Nighthawks 1st 14.0 2nd 16.5 Scouts 1st 2.5 2nd 2.5 Create a pivot table of group score counts, by company and regimensts df . pivot_table ( index = [ 'regiment' , 'company' ], aggfunc = 'count' ) TestScore regiment company Dragoons 1st 2 2nd 2 Nighthawks 1st 2 2nd 2 Scouts 1st 2 2nd 2","tags":"Python","url":"http://chrisalbon.com/python/pandas_pivot_tables.html"},{"title":"Breaking Up A String Into Columns Using Regex In Pandas","loc":"http://chrisalbon.com/python/pandas_regex_to_create_columns.html","text":"Based on this tutorial in nbviewer . Import modules import re import pandas as pd Create a dataframe of raw strings # Create a dataframe with a single column of strings data = { 'raw' : [ 'Arizona 1 2014-12-23 3242.0' , 'Iowa 1 2010-02-23 3453.7' , 'Oregon 0 2014-06-20 2123.0' , 'Maryland 0 2014-03-14 1123.6' , 'Florida 1 2013-01-15 2134.0' , 'Georgia 0 2012-07-14 2345.6' ]} df = pd . DataFrame ( data , columns = [ 'raw' ]) df raw 0 Arizona 1 2014-12-23 3242.0 1 Iowa 1 2010-02-23 3453.7 2 Oregon 0 2014-06-20 2123.0 3 Maryland 0 2014-03-14 1123.6 4 Florida 1 2013-01-15 2134.0 5 Georgia 0 2012-07-14 2345.6 Search a column of strings for a pattern # Which rows of df['raw'] contain 'xxxx-xx-xx'? df [ 'raw' ] . str . contains ( '....-..-..' , regex = True ) 0 True 1 True 2 True 3 True 4 True 5 True Name: raw, dtype: bool Extract the column of single digits # In the column 'raw', extract single digit in the strings df [ 'female' ] = df [ 'raw' ] . str . extract ( '(\\d)' , expand = True ) df [ 'female' ] 0 1 1 1 2 0 3 0 4 1 5 0 Name: female, dtype: object Extract the column of dates # In the column 'raw', extract xxxx-xx-xx in the strings df [ 'date' ] = df [ 'raw' ] . str . extract ( '(....-..-..)' , expand = True ) df [ 'date' ] 0 2014-12-23 1 2010-02-23 2 2014-06-20 3 2014-03-14 4 2013-01-15 5 2012-07-14 Name: date, dtype: object Extract the column of thousands # In the column 'raw', extract ####.## in the strings df [ 'score' ] = df [ 'raw' ] . str . extract ( '(\\d\\d\\d\\d\\.\\d)' , expand = True ) df [ 'score' ] 0 3242.0 1 3453.7 2 2123.0 3 1123.6 4 2134.0 5 2345.6 Name: score, dtype: object Extract the column of words # In the column 'raw', extract the word in the strings df [ 'state' ] = df [ 'raw' ] . str . extract ( '([A-Z]\\w{0,})' , expand = True ) df [ 'state' ] 0 Arizona 1 Iowa 2 Oregon 3 Maryland 4 Florida 5 Georgia Name: state, dtype: object View the final dataframe df raw female date score state 0 Arizona 1 2014-12-23 3242.0 1 2014-12-23 3242.0 Arizona 1 Iowa 1 2010-02-23 3453.7 1 2010-02-23 3453.7 Iowa 2 Oregon 0 2014-06-20 2123.0 0 2014-06-20 2123.0 Oregon 3 Maryland 0 2014-03-14 1123.6 0 2014-03-14 1123.6 Maryland 4 Florida 1 2013-01-15 2134.0 1 2013-01-15 2134.0 Florida 5 Georgia 0 2012-07-14 2345.6 0 2012-07-14 2345.6 Georgia","tags":"Python","url":"http://chrisalbon.com/python/pandas_regex_to_create_columns.html"},{"title":"Rename Multiple Pandas Dataframe Column Names At Once","loc":"http://chrisalbon.com/python/pandas_rename_multiple_columns.html","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'Commander' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'Date' : [ '2012, 02, 08' , '2012, 02, 08' , '2012, 02, 08' , '2012, 02, 08' , '2012, 02, 08' ], 'Score' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df Commander Date Score Cochice Jason 2012, 02, 08 4 Pima Molly 2012, 02, 08 24 Santa Cruz Tina 2012, 02, 08 31 Maricopa Jake 2012, 02, 08 2 Yuma Amy 2012, 02, 08 3 Rename Column Names df . columns = [ 'Leader' , 'Time' , 'Score' ] df Leader Time Score Cochice Jason 2012, 02, 08 4 Pima Molly 2012, 02, 08 24 Santa Cruz Tina 2012, 02, 08 31 Maricopa Jake 2012, 02, 08 2 Yuma Amy 2012, 02, 08 3 df . rename ( columns = { 'Leader' : 'Commander' }, inplace = True ) df Commander Time Score Cochice Jason 2012, 02, 08 4 Pima Molly 2012, 02, 08 24 Santa Cruz Tina 2012, 02, 08 31 Maricopa Jake 2012, 02, 08 2 Yuma Amy 2012, 02, 08 3","tags":"Python","url":"http://chrisalbon.com/python/pandas_rename_multiple_columns.html"},{"title":"Replacing Values In Pandas","loc":"http://chrisalbon.com/python/pandas_replace_values.html","text":"import modules import pandas as pd import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ - 999 , - 999 , - 999 , 2 , 1 ], 'postTestScore' : [ 2 , 2 , - 999 , 2 , - 999 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 -999 2 1 Molly Jacobson 52 -999 2 2 Tina Ali 36 -999 -999 3 Jake Milner 24 2 2 4 Amy Cooze 73 1 -999 Replace all values of -999 with NAN df . replace ( - 999 , np . nan ) first_name last_name age preTestScore postTestScore 0 Jason Miller 42 NaN 2.0 1 Molly Jacobson 52 NaN 2.0 2 Tina Ali 36 NaN NaN 3 Jake Milner 24 2.0 2.0 4 Amy Cooze 73 1.0 NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_replace_values.html"},{"title":"Random Sampling Dataframe","loc":"http://chrisalbon.com/python/pandas_sampling_dataframe.html","text":"import modules import pandas as pd import numpy as np Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 5 rows × 5 columns Select a random subset of 2 without replacement df . take ( np . random . permutation ( len ( df ))[: 2 ]) first_name last_name age preTestScore postTestScore 3 Jake Milner 24 2 62 0 Jason Miller 42 4 25 2 rows × 5 columns","tags":"Python","url":"http://chrisalbon.com/python/pandas_sampling_dataframe.html"},{"title":"Saving A Pandas Dataframe As A CSV","loc":"http://chrisalbon.com/python/pandas_saving_dataframe_as_csv.html","text":"import modules import pandas as pd Create dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Jacobson' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 52 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 24 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 94 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Molly Jacobson 52 24 94 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Safe the dataframe called \"df\" as csv Note: I've commented out this line of code so it does not run. Just remove the # to run. # df.to_csv('example.csv')","tags":"Python","url":"http://chrisalbon.com/python/pandas_saving_dataframe_as_csv.html"},{"title":"Search A Pandas Column For A Value","loc":"http://chrisalbon.com/python/pandas_search_column_for_value.html","text":"# Import modules import pandas as pd raw_data = { 'first_name' : [ 'Jason' , 'Jason' , 'Tina' , 'Jake' , 'Amy' ], 'last_name' : [ 'Miller' , 'Miller' , 'Ali' , 'Milner' , 'Cooze' ], 'age' : [ 42 , 42 , 36 , 24 , 73 ], 'preTestScore' : [ 4 , 4 , 31 , 2 , 3 ], 'postTestScore' : [ 25 , 25 , 57 , 62 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'last_name' , 'age' , 'preTestScore' , 'postTestScore' ]) df first_name last_name age preTestScore postTestScore 0 Jason Miller 42 4 25 1 Jason Miller 42 4 25 2 Tina Ali 36 31 57 3 Jake Milner 24 2 62 4 Amy Cooze 73 3 70 Find where a value exists in a column # View preTestscore where postTestscore is greater than 50 df [ 'preTestScore' ] . where ( df [ 'postTestScore' ] > 50 ) 0 NaN 1 NaN 2 31 3 2 4 3 Name: preTestScore, dtype: float64","tags":"Python","url":"http://chrisalbon.com/python/pandas_search_column_for_value.html"},{"title":"Select Rows That Have A Certain Value","loc":"http://chrisalbon.com/python/pandas_select_rows_containing_values.html","text":"import pandas as pd # Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' ], 'country' : [[ 'Syria' , 'Lebanon' ],[ 'Spain' , 'Morocco' ]]} df = pd . DataFrame ( data ) df country name 0 [Syria, Lebanon] Jason 1 [Spain, Morocco] Molly df [ df [ 'country' ] . map ( lambda country : 'Syria' in country )] country name 0 [Syria, Lebanon] Jason","tags":"Python","url":"http://chrisalbon.com/python/pandas_select_rows_containing_values.html"},{"title":"Select Rows With Multiple Filters","loc":"http://chrisalbon.com/python/pandas_select_rows_multiple_filters.html","text":"# import pandas as pd import pandas as pd # Create an example dataframe data = { 'name' : [ 'A' , 'B' , 'C' , 'D' , 'E' ], 'score' : [ 1 , 2 , 3 , 4 , 5 ]} df = pd . DataFrame ( data ) df name score 0 A 1 1 B 2 2 C 3 3 D 4 4 E 5 # Select rows of the dataframe where df.score is greater than 1 and less and 5 df [( df [ 'score' ] > 1 ) & ( df [ 'score' ] < 5 )] name score 1 B 2 2 C 3 3 D 4","tags":"Python","url":"http://chrisalbon.com/python/pandas_select_rows_multiple_filters.html"},{"title":"Select Rows When Columns Contain Certain Values","loc":"http://chrisalbon.com/python/pandas_select_rows_when_column_has_certain_values.html","text":"Preliminaries # Import modules import pandas as pd # Set ipython's max row display pd . set_option ( 'display.max_row' , 1000 ) # Set iPython's max column width to 50 pd . set_option ( 'display.max_columns' , 50 ) Create an example dataframe # Create an example dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 4 , 24 , 31 , 2 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 Maricopa Jake 2 2014 Yuma Amy 3 2014 Grab rows based on column values value_list = [ 'Tina' , 'Molly' , 'Jason' ] #Grab DataFrame rows where column has certain values df [ df . name . isin ( value_list )] name reports year Cochice Jason 4 2012 Pima Molly 24 2012 Santa Cruz Tina 31 2013 #Grab DataFrame rows where column doesn't have certain values df [ ~ df . name . isin ( value_list )] name reports year Maricopa Jake 2 2014 Yuma Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/pandas_select_rows_when_column_has_certain_values.html"},{"title":"Selecting Pandas DataFrame Rows Based On Conditions","loc":"http://chrisalbon.com/python/pandas_selecting_rows_on_conditions.html","text":"Preliminaries # Import modules import pandas as pd import numpy as np # Create a dataframe raw_data = { 'first_name' : [ 'Jason' , 'Molly' , np . nan , np . nan , np . nan ], 'nationality' : [ 'USA' , 'USA' , 'France' , 'UK' , 'UK' ], 'age' : [ 42 , 52 , 36 , 24 , 70 ]} df = pd . DataFrame ( raw_data , columns = [ 'first_name' , 'nationality' , 'age' ]) df first_name nationality age 0 Jason USA 42 1 Molly USA 52 2 NaN France 36 3 NaN UK 24 4 NaN UK 70 Method 1: Using Boolean Variables # Create variable with TRUE if nationality is USA american = df [ 'nationality' ] == \"USA\" # Create variable with TRUE if age is greater than 50 elderly = df [ 'age' ] > 50 # Select all casess where nationality is USA and age is greater than 50 df [ american & elderly ] first_name nationality age 1 Molly USA 52 Method 2: Using variable attributes # Select all cases where the first name is not missing and nationality is USA df [ df [ 'first_name' ] . notnull () & ( df [ 'nationality' ] == \"USA\" )] first_name nationality age 0 Jason USA 42 1 Molly USA 52","tags":"Python","url":"http://chrisalbon.com/python/pandas_selecting_rows_on_conditions.html"},{"title":"Sorting Rows In pandas Dataframes","loc":"http://chrisalbon.com/python/pandas_sorting_rows_dataframe.html","text":"import modules import pandas as pd Create dataframe data = { 'name' : [ 'Jason' , 'Molly' , 'Tina' , 'Jake' , 'Amy' ], 'year' : [ 2012 , 2012 , 2013 , 2014 , 2014 ], 'reports' : [ 1 , 2 , 1 , 2 , 3 ], 'coverage' : [ 2 , 2 , 3 , 3 , 3 ]} df = pd . DataFrame ( data , index = [ 'Cochice' , 'Pima' , 'Santa Cruz' , 'Maricopa' , 'Yuma' ]) df coverage name reports year Cochice 2 Jason 1 2012 Pima 2 Molly 2 2012 Santa Cruz 3 Tina 1 2013 Maricopa 3 Jake 2 2014 Yuma 3 Amy 3 2014 Sort the dataframe's rows by reports, in descending order df . sort_values ( by = 'reports' , ascending = 0 ) coverage name reports year Yuma 3 Amy 3 2014 Pima 2 Molly 2 2012 Maricopa 3 Jake 2 2014 Cochice 2 Jason 1 2012 Santa Cruz 3 Tina 1 2013 Sort the dataframe's rows by coverage and then by reports, in ascending order df . sort_values ( by = [ 'coverage' , 'reports' ]) coverage name reports year Cochice 2 Jason 1 2012 Pima 2 Molly 2 2012 Santa Cruz 3 Tina 1 2013 Maricopa 3 Jake 2 2014 Yuma 3 Amy 3 2014","tags":"Python","url":"http://chrisalbon.com/python/pandas_sorting_rows_dataframe.html"},{"title":"Split Combined Lat/Long Coordinate Variables Into Seperate Variables In Pandas","loc":"http://chrisalbon.com/python/pandas_split_lat_and_long_into_variables.html","text":"Preliminaries import pandas as pd import numpy as np Create an example dataframe raw_data = { 'geo' : [ '40.0024, -105.4102' , '40.0068, -105.266' , '39.9318, -105.2813' , np . nan ]} df = pd . DataFrame ( raw_data , columns = [ 'geo' ]) df geo 0 40.0024, -105.4102 1 40.0068, -105.266 2 39.9318, -105.2813 3 NaN Split the geo variable into seperate lat and lon variables # Create two lists for the loop results to be placed lat = [] lon = [] # For each row in a varible, for row in df [ 'geo' ]: # Try to, try : # Split the row by comma and append # everything before the comma to lat lat . append ( row . split ( ',' )[ 0 ]) # Split the row by comma and append # everything after the comma to lon lon . append ( row . split ( ',' )[ 1 ]) # But if you get an error except : # append a missing value to lat lat . append ( np . NaN ) # append a missing value to lon lon . append ( np . NaN ) # Create two new columns from lat and lon df [ 'latitude' ] = lat df [ 'longitude' ] = lon View the dataframe df geo latitude longitude 0 40.0024, -105.4102 40.0024 -105.4102 1 40.0068, -105.266 40.0068 -105.266 2 39.9318, -105.2813 39.9318 -105.2813 3 NaN NaN NaN","tags":"Python","url":"http://chrisalbon.com/python/pandas_split_lat_and_long_into_variables.html"},{"title":"Pandas Time Series Basics","loc":"http://chrisalbon.com/python/pandas_time_series_basics.html","text":"Import modules from datetime import datetime import pandas as pd % matplotlib inline import matplotlib.pyplot as pyplot Create a dataframe data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'battle_deaths' : [ 34 , 25 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 41 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'battle_deaths' ]) print ( df ) date battle_deaths 0 2014-05-01 18:47:05.069722 34 1 2014-05-01 18:47:05.119994 25 2 2014-05-02 18:47:05.178768 26 3 2014-05-02 18:47:05.230071 15 4 2014-05-02 18:47:05.230071 15 5 2014-05-02 18:47:05.280592 14 6 2014-05-03 18:47:05.332662 26 7 2014-05-03 18:47:05.385109 25 8 2014-05-04 18:47:05.436523 62 9 2014-05-04 18:47:05.486877 41 [10 rows x 2 columns] Convert df['date'] from string to datetime df [ 'date' ] = pd . to_datetime ( df [ 'date' ]) Set df['date'] as the index and delete the column df . index = df [ 'date' ] del df [ 'date' ] df battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 10 rows × 1 columns View all observations that occured in 2014 df [ '2014' ] battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 10 rows × 1 columns View all observations that occured in May 2014 df [ '2014-05' ] battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 10 rows × 1 columns Observations after May 3rd, 2014 df [ datetime ( 2014 , 5 , 3 ):] battle_deaths date 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 4 rows × 1 columns Observations between May 3rd and May 4th df [ '5/3/2014' : '5/4/2014' ] battle_deaths date 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 4 rows × 1 columns Truncation observations after May 2nd 2014 df . truncate ( after = '5/3/2014' ) battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 6 rows × 1 columns Observations of May 2014 df . ix [ '5-2014' ] battle_deaths date 2014-05-01 18:47:05.069722 34 2014-05-01 18:47:05.119994 25 2014-05-02 18:47:05.178768 26 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.230071 15 2014-05-02 18:47:05.280592 14 2014-05-03 18:47:05.332662 26 2014-05-03 18:47:05.385109 25 2014-05-04 18:47:05.436523 62 2014-05-04 18:47:05.486877 41 10 rows × 1 columns Count the number of observations per timestamp df . groupby ( level = 0 ) . count () battle_deaths date 2014-05-01 18:47:05.069722 1 2014-05-01 18:47:05.119994 1 2014-05-02 18:47:05.178768 1 2014-05-02 18:47:05.230071 2 2014-05-02 18:47:05.280592 1 2014-05-03 18:47:05.332662 1 2014-05-03 18:47:05.385109 1 2014-05-04 18:47:05.436523 1 2014-05-04 18:47:05.486877 1 9 rows × 1 columns Mean value of battle_deaths per day df . resample ( 'D' , how = 'mean' ) battle_deaths date 2014-05-01 29.5 2014-05-02 17.5 2014-05-03 25.5 2014-05-04 51.5 4 rows × 1 columns Total value of battle_deaths per day df . resample ( 'D' , how = 'sum' ) battle_deaths date 2014-05-01 59 2014-05-02 70 2014-05-03 51 2014-05-04 103 4 rows × 1 columns Plot of the total battle deaths per day df . resample ( 'D' , how = 'sum' ) . plot () <matplotlib.axes.AxesSubplot at 0x10809de50>","tags":"Python","url":"http://chrisalbon.com/python/pandas_time_series_basics.html"},{"title":"Using Seaborn To Visualize A Pandas Dataframe","loc":"http://chrisalbon.com/python/pandas_with_seaborn.html","text":"Preliminaries import pandas as pd % matplotlib inline import random import matplotlib.pyplot as plt import seaborn as sns df = pd . DataFrame () df [ 'x' ] = random . sample ( range ( 1 , 100 ), 25 ) df [ 'y' ] = random . sample ( range ( 1 , 100 ), 25 ) df . head () x y 0 14 52 1 88 92 2 39 69 3 19 98 4 60 76 Scatterplot sns . lmplot ( 'x' , 'y' , data = df , fit_reg = False ) <seaborn.axisgrid.FacetGrid at 0x10dc2b1d0> Density Plot sns . kdeplot ( df . y ) <matplotlib.axes._subplots.AxesSubplot at 0x10c30e050> sns . kdeplot ( df . y , df . x ) <matplotlib.axes._subplots.AxesSubplot at 0x10c5536d0> sns . distplot ( df . x ) <matplotlib.axes._subplots.AxesSubplot at 0x10b669550> Histogram plt . hist ( df . x , alpha =. 3 ) sns . rugplot ( df . x ); Boxplot sns . boxplot ([ df . y , df . x ]) <matplotlib.axes._subplots.AxesSubplot at 0x10a5c9b50> Violin Plot sns . violinplot ([ df . y , df . x ]) <matplotlib.axes._subplots.AxesSubplot at 0x10dca4b50> Heatmap sns . heatmap ([ df . y , df . x ], annot = True , fmt = \"d\" ) <matplotlib.axes._subplots.AxesSubplot at 0x10dab5110> Clustermap sns . clustermap ( df ) <seaborn.matrix.ClusterGrid at 0x10de304d0>","tags":"Python","url":"http://chrisalbon.com/python/pandas_with_seaborn.html"},{"title":"Parsing Dates","loc":"http://chrisalbon.com/r-stats/parsing-dates.html","text":"Often imported data will need to be parsed. If the dates are character strings, you can use strptime to parse the date-time. # Import some moon landing day/times moon_landings_str <- c ( \"20:17:40 20/07/1969\" , \"06:54:35 19/11/1969\" , \"09:18:11 05/02/1971\" , \"22:16:29 30/07/1971\" , \"02:23:35 21/04/1972\" , \"19:54:57 11/12/1972\" ) moon_landings_str [1] \"20:17:40 20/07/1969\" \"06:54:35 19/11/1969\" \"09:18:11 05/02/1971\" [4] \"22:16:29 30/07/1971\" \"02:23:35 21/04/1972\" \"19:54:57 11/12/1972\" # convert the date-time character strings into dates moon_landings_lt <- strptime ( moon_landings_str , \"%H:%M:%S %d/%m/%Y\" , tz = \"UTC\" ) moon_landings_lt [1] \"1969-07-20 20:17:40 UTC\" \"1969-11-19 06:54:35 UTC\" [3] \"1971-02-05 09:18:11 UTC\" \"1971-07-30 22:16:29 UTC\" [5] \"1972-04-21 02:23:35 UTC\" \"1972-12-11 19:54:57 UTC\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/parsing-dates.html"},{"title":"Password Generator","loc":"http://chrisalbon.com/r-stats/password-generator.html","text":"Credit to: http://ryouready.wordpress.com/2008/12/18/generate-random-string-name/ Credit to: http://stats.stackexchange.com/questions/7900/generate-random-strings-based-on-regular-expressions-in-r # enter the url here: url <- \"http://chrisralbon.com\" # password generating function password <- function ( x ) { length = nchar ( x ) - sample ( 1 : 10 , 5 ) # take length of url minus a random five digit number random.string <- c ( 1 : 1 ) # create a atomic vector of length 1 for ( i in 1 : 1 ) { random.string [ i ] <- paste ( sample ( c ( 0 : 9 , letters , LETTERS ), length , replace = TRUE ), collapse = \"\" ) # for that vector add random upper and lower case numbers, with replacement of length \"length\" } return ( random.string ) # display the created string } # generate the password password ( url ) [1] \"0bd4Kaqy3CPjCV\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/password-generator.html"},{"title":"Pie Chart","loc":"http://chrisalbon.com/r-stats/pie-chart.html","text":"Original source: http://www.statmethods.net/graphs/pie.html # Create some casualty numbers slices <- c ( 4 , 6 , 1 , 0 , 0 , 0 , 5 ) # Create labels that correspond to the casualty numbers lbls <- c ( \"CrisisNET\" , \"V3\" , \"V2\" , \"Crowdmap\" , \"Ping\" , \"SMSSync\" , \"Other\" ) # Create Percents For Each Slice pct <- round ( slices / sum ( slices ) * 100 ) # Add Percents To Labels lbls <- paste ( lbls , pct ) # Add The Percent Symbol To Labels lbls <- paste ( lbls , \"%\" , sep = \"\" ) # Create a pie chart with labels, with each slice colored by the terrain color pallete pie ( slices , labels = lbls , col = terrain.colors ( length ( lbls )), main = \"Main Ushahidi Blog Posts\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/pie-chart.html"},{"title":"Plot Points On A Map","loc":"http://chrisalbon.com/r-stats/plots-on-map.html","text":"Original source: http://stackoverflow.com/questions/11056738/plotting-points-from-a-data-frame-using-openstreetmap # load the ggmap package library ( ggmap ) Warning message: : package ‘ggmap' was built under R version 3.1.3Loading required package: ggplot2 # make a data frame with the long/lat of three stores and their names stores <- data.frame ( name = c ( \"Commercial\" , \"Union\" , \"Bedford\" ), longitude = c ( -70.25042295455933 , -70.26050806045532 , -70.27726650238037 ), latitude = c ( 43.657471302616806 , 43.65663299041943 , 43.66091757424481 ) ) # create vector with the long/lat with two opposite corner of the map to mark the zoom level. Specifically, the first value is the bottom left of the map and the second value is the top right downtown.pdx <- c ( -70.2954 , 43.64278 , -70.2350 , 43.68093 ) # Fetch map from stamen (use source = \"osm\" for open street maps) portland <- get_map ( location = downtown.pdx , source = \"stamen\" ) Map from URL : http://tile.stamen.com/terrain/14/4992/5977.jpg Map from URL : http://tile.stamen.com/terrain/14/4993/5977.jpg Map from URL : http://tile.stamen.com/terrain/14/4994/5977.jpg Map from URL : http://tile.stamen.com/terrain/14/4995/5977.jpg Map from URL : http://tile.stamen.com/terrain/14/4992/5978.jpg Map from URL : http://tile.stamen.com/terrain/14/4993/5978.jpg Map from URL : http://tile.stamen.com/terrain/14/4994/5978.jpg Map from URL : http://tile.stamen.com/terrain/14/4995/5978.jpg Map from URL : http://tile.stamen.com/terrain/14/4992/5979.jpg Map from URL : http://tile.stamen.com/terrain/14/4993/5979.jpg Map from URL : http://tile.stamen.com/terrain/14/4994/5979.jpg Map from URL : http://tile.stamen.com/terrain/14/4995/5979.jpg Map from URL : http://tile.stamen.com/terrain/14/4992/5980.jpg Map from URL : http://tile.stamen.com/terrain/14/4993/5980.jpg Map from URL : http://tile.stamen.com/terrain/14/4994/5980.jpg Map from URL : http://tile.stamen.com/terrain/14/4995/5980.jpg # view the map portlandMap <- ggmap ( portland ) # add the points to the map portlandMap <- portlandMap + geom_point ( data = stores , aes ( x = longitude , y = latitude ), size = 5 ) # Add lavels for the points portlandMap + geom_text ( data = stores , aes ( label = name , x = longitude +.001 , y = latitude ), hjust = 0 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/plots-on-map.html"},{"title":"Making Pretty Numbers with prettyNum","loc":"http://chrisalbon.com/r-stats/pretty-numbers.html","text":"We are taking two input numbers 1e10 and 1e-14, and formatting them with commas after zero and peroids before. prettyNum ( c ( 1e10 , 1e-14 ), big.mark = \",\" , small.mark = \".\" , preserve.width = \"individual\" , scientific = FALSE ) [1] \"10,000,000,000\" \"0.00000.00000.0001\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/pretty-numbers.html"},{"title":"Proporton Table","loc":"http://chrisalbon.com/r-stats/proportion-table.html","text":"Original source: The R Book # create a matrix of simulated count data counts <- matrix ( sample ( 1 : 100 , 20 , replace = T ), nrow = 4 ); counts [,1] [,2] [,3] [,4] [,5] [1,] 77 79 14 94 7 [2,] 100 54 86 28 60 [3,] 38 99 77 84 81 [4,] 75 93 19 21 39 # calculate each cell's proportion of the entire row's total counts prop.table ( counts , 1 ) [,1] [,2] [,3] [,4] [,5] [1,] 0.2841328 0.2915129 0.05166052 0.34686347 0.02583026 [2,] 0.3048780 0.1646341 0.26219512 0.08536585 0.18292683 [3,] 0.1002639 0.2612137 0.20316623 0.22163588 0.21372032 [4,] 0.3036437 0.3765182 0.07692308 0.08502024 0.15789474 # calculate each cell's proportion of the entire columns's total counts prop.table ( counts , 2 ) [,1] [,2] [,3] [,4] [,5] [1,] 0.2655172 0.2430769 0.07142857 0.41409692 0.03743316 [2,] 0.3448276 0.1661538 0.43877551 0.12334802 0.32085561 [3,] 0.1310345 0.3046154 0.39285714 0.37004405 0.43315508 [4,] 0.2586207 0.2861538 0.09693878 0.09251101 0.20855615","tags":"R Stats","url":"http://chrisalbon.com/r-stats/proportion-table.html"},{"title":"Proportional Stacked Bar Graph","loc":"http://chrisalbon.com/r-stats/proportional-stacked-bar-graph.html","text":"Original source: r graphics cookbook # load the ggplot2 package library ( ggplot2 ) # load the gcookbook package library ( gcookbook ) # load the plyr package library ( plyr ) # do dataframe to dataframe apply. split up cabbage_exp by \"Date\" (there are three dates), and create a new variable which is percent_weight ce <- ddply ( cabbage_exp , \"Date\" , transform , percent_weight = Weight / sum ( Weight ) * 100 ) # create a ggplot data object of date, percent_weight, and filled by cultivar ggplot ( ce , aes ( x = Date , y = percent_weight , fill = Cultivar )) + # plot the bars that are black geom_bar ( stat = \"identity\" , colour = \"black\" ) + # reserve the legend so it matches the colors guides ( fill = guide_legend ( reverse = TRUE )) + # fill with the pastel1 color palette scale_fill_brewer ( palette = \"Pastel1\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/proportional-stacked-bar-graph.html"},{"title":"Getting Familiar With R","loc":"http://chrisalbon.com/r-stats/r-basics.html","text":"# Arithmetic operators 1 + 1 # 1 plus 1 4 - 3 # 4 minus 3 14 / 10 # 14 divided by 10 10 * 5 # 10 multiplied by 5 3 &#94; 2 # 3 squared 5 %% 2 # 5 mod 2 4 %/% 2 # 4 divided by 2 (integer division) [1] 2 [1] 1 [1] 1.4 [1] 50 [1] 9 [1] 1 [1] 2 # Logical operators 2 < 4 # 2 is less than 4 2 <= 4 # 2 is less than or equal to 4 2 > 4 # 2 is greater than 4 3 >= 5 # 3 is greater than or equal to 5 5 == 5 # 5 is equal to 5 5 != 4 # 5 is not equal to 4 ! 2 # Not 2 2 | 4 # 2 or 4 4 & 5 # 3 and 5 isTRUE ( 4 + 4 == 8 ) # is \"4 plus 4 equals 5\" true? [1] TRUE [1] TRUE [1] FALSE [1] FALSE [1] TRUE [1] TRUE [1] FALSE [1] TRUE [1] TRUE [1] TRUE How R Handles Data In R, a collection of one or more values is called a vector. Think of vectors as a collection of numbers. # We can create a vector by creating an object my.age <- 29 # create an vector object called my.age that contains the value \"29\" my.age # view the contents of our.ages our.ages <- c ( 29 , 29 , 43 , 4 ) # create a vector object called our.ages containing the values 29, 29, 43, and 4 our.ages # view the contents of our.ages [1] 29 [1] 29 29 43 4 Notice two things about these objects. First I am giving them very descriptive names. Second, I am seperating words with periods since spaces are not allowed in object names. # We can even create objects from objects number.of.us <- length ( our.ages ) # create a vector object called number.of.us whose value is the length of the our.ages vector number.of.us # view the contents of number.of.us [1] 4 # We can even tell R to create values for us one.to.ten <- 1 : 10 # create an object called one.to.ten that contains the all integiers between one and ten # We can also do a lot of different functions with vector objects max ( our.ages ) # find the maximum value in our.ages min ( our.ages ) # find the minimum value in our.ages sum ( our.ages ) # find the sum of all values in our.ages mean ( our.ages ) # find the mean of all values in our.ages median ( our.ages ) # find the median of all values in our.ages range ( our.ages ) # find the range of all values in our.ages sort ( our.ages ) # sort values in our.ages in ascending order rank ( our.ages ) # rank the values in our.ages order ( our.ages ) # display the ascending order of the values in our.ages quantile ( our.ages ) # display the minimum, lower quartile, median, upper quartile, and maxmimum of our.ages [1] 43 [1] 4 [1] 105 [1] 26.25 [1] 29 [1] 4 43 [1] 4 29 29 43 [1] 2.5 2.5 4.0 1.0 [1] 4 1 2 3 0% 25% 50% 75% 100% 4.00 22.75 29.00 32.50 43.00","tags":"R Stats","url":"http://chrisalbon.com/r-stats/r-basics.html"},{"title":"Racetrack Plot","loc":"http://chrisalbon.com/r-stats/racetrack-plot.html","text":"Original source: http://stackoverflow.com/questions/15751442/making-a-circular-barplot-with-a-hollow-center-aka-race-track-plot # load the ggplot2 library library ( ggplot2 ) # create some simulated data Category <- c ( \"Electronics\" , \"Appliances\" , \"Books\" , \"Music\" , \"Clothing\" , \"Cars\" , \"Food/Beverages\" , \"Personal Hygiene\" , \"Personal Health/OTC\" , \"Hair Care\" ) Percent <- c ( 81 , 77 , 70 , 69 , 69 , 68 , 62 , 62 , 61 , 60 ) internetImportance <- data.frame ( Category , Percent ) Create a dataframe that contains the legend information # create a variable called len that equals 4 len <- 4 # create a data frame with three elements, Category, Percent, and Category2. this is all blank data so that the middle of the racetrack is blank df2 <- data.frame ( Category = letters [ 1 : len ], Percent = rep ( 0 , len ), Category2 = rep ( \"\" , len )) # create a new Category2 variable in the internetImportance df that contains the category name, a dash, the importance percent, and a percent sign internetImportance $ Category2 <- paste0 ( internetImportance $ Category , \" - \" , internetImportance $ Percent , \"%\" ) # append number to category name internetImportance <- rbind ( internetImportance , df2 ) # set factor so it will plot in descending order internetImportance $ Category <- factor ( internetImportance $ Category , levels = rev ( internetImportance $ Category )) Plot # great the ggplot data ggplot ( internetImportance , aes ( x = Category , y = Percent , fill = Category2 )) + # create a bar plot geom_bar ( width = 0.9 , stat = \"identity\" ) + scale_fill_brewer ( palette = \"Set3\" ) + # curve the bar chart coord_polar ( theta = \"y\" ) + # remove the x and y axis labels xlab ( \"\" ) + ylab ( \"\" ) + # let the \"circumference\" of the racetrack to 100 ylim ( c ( 0 , 100 )) + # add a title at the top of the chart ggtitle ( \"Top Product Categories Influenced by Internet\" ) + # add the legend text geom_text ( data = internetImportance , hjust = 1 , size = 3 , aes ( x = Category , y = 0 , label = Category2 )) + # add the text in the middle of the racetrack geom_text ( label = \"GLOBAL\" , x = .5 , y = .5 , size = 4 ) + # use the minimal theme to make is pretty theme_minimal () + # remove the external legend, and remove all the extra elements theme ( legend.position = \"none\" , panel.grid.major = element_blank (), panel.grid.minor = element_blank (), axis.line = element_blank (), axis.text.y = element_blank (), axis.text.x = element_blank (), axis.ticks = element_blank ())","tags":"R Stats","url":"http://chrisalbon.com/r-stats/racetrack-plot.html"},{"title":"Radial Plot","loc":"http://chrisalbon.com/r-stats/radial-plot.html","text":"Inspiration: http://onertipaday.blogspot.com/2009/01/radar-chart.html # Load the plotrix library library ( plotrix ) # Create a vector with the number of objects to be plotted country <- c ( 1 : 8 ) # Create a vector of country names for the labels of the vector above country.names <- names ( country ) <- c ( \"Yemen\" , \"Spain\" , \"Russia\" , \"Portugal\" , \"Italy\" , \"Kenya\" , \"USA\" , \"Iceland\" ) # Create a vector of the data to be plotted country <- c ( 324 , 234 , 123 , 63 , 234 , 423 , 324 , 452 ) # Set the font size on the radial plot par ( ps = 12 ) # Create a radial plot radial.plot ( country , labels = country.names , rp.type = \"p\" , main = \"Radar Chart\" , radial.lim = c ( 0 , 500 ), line.col = \"blue\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/radial-plot.html"},{"title":"Random Numbers","loc":"http://chrisalbon.com/r-stats/random-numbers.html","text":"# Generate 10 random numbers between 0 and 1 runif ( 10 ) [1] 0.13315436 0.14357165 0.52478178 0.21880271 0.18908152 0.02590017 [7] 0.05517990 0.58034118 0.32128920 0.90293812 # Generate 10 random numbers between 5 and 1- runif ( 10 , 5 , 10 ) [1] 5.179785 5.485701 5.885241 6.159060 5.281826 7.630544 9.717838 9.091188 [9] 5.138212 7.781516 # Generate 10 random integers between 1 and 10, with replacement sample ( 1 : 10 , 10 , replace = T ) [1] 1 6 10 10 1 10 10 2 7 2 # Select 3 random integers from a pool of 100 integers that range between 1 and 100 sample ( 1 : 100 , 6 , replace = F ) [1] 76 18 32 37 94 78 # Select three state names witout replacement sample ( state.name , 3 ) [1] \"Connecticut\" \"Nevada\" \"Wyoming\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/random-numbers.html"},{"title":"Recursive Functions","loc":"http://chrisalbon.com/python/recursive_functions.html","text":"Simple factorial print ( 5 * 4 * 3 * 2 * 1 ) 120 Recursive function The tell-tale sign of a recursive function is a function that calls itself # Create a function inputing n, that, def factorial ( n ): # if n is less than or equal to 1, if n <= 1 : # return n, return n # if not, return n multiplied by the output # of the factorial function of one less than n return n * factorial ( n - 1 ) # run the function factorial ( 5 ) 120","tags":"Python","url":"http://chrisalbon.com/python/recursive_functions.html"},{"title":"Regular Expression Basics","loc":"http://chrisalbon.com/python/regular_expressions_basics.html","text":"Import the regex (re) package import re Import sys import sys Create a simple text string. text = 'The quick brown fox jumped over the lazy black bear.' Create a pattern to match three_letter_word = '...' Convert the string into a regex object pattern_re = re . compile ( three_letter_word ); pattern_re re.compile(r'...', re.UNICODE) Does a three letter word appear in text? re_search = re . search ( '..own' , text ) If the search query is at all true, if re_search : # Print the search results print ( re_search . group ()) brown re.match re.match() is for matching ONLY the beginning of a string or the whole string For anything else, use re.search Match all three letter words in text re_match = re . match ( '..own' , text ) If re_match is true, print the match, else print \"No Matches\" if re_match : # Print all the matches print ( re_match . group ()) else : # Print this print ( 'No matches' ) No matches re.split Split up the string using \"e\" as the seperator. re_split = re . split ( 'e' , text ); re_split ['Th', ' quick brown fox jump', 'd ov', 'r th', ' lazy black b', 'ar.'] re.sub Replaces occurrences of the regex pattern with something else The \"3\" references to the maximum number of substitutions to make. Substitute the first three instances of \"e\" with \"E\", then print it re_sub = re . sub ( 'e' , 'E' , text , 3 ); print ( re_sub ) ThE quick brown fox jumpEd ovEr the lazy black bear.","tags":"Python","url":"http://chrisalbon.com/python/regular_expressions_basics.html"},{"title":"Removing Variables","loc":"http://chrisalbon.com/r-stats/remote-variable.html","text":"# Create two variables of 50 observations percent.sms <- runif ( 50 ) location <- state.name percent.sms location [1] 0.58913023 0.54043206 0.14073087 0.68065186 0.74324175 0.75807962 [7] 0.29425877 0.27024361 0.95795936 0.59034216 0.56754098 0.59455496 [13] 0.21656371 0.60917559 0.73271437 0.22795936 0.45305943 0.42421408 [19] 0.40083340 0.17028350 0.88662551 0.17400449 0.42711263 0.10408158 [25] 0.43336437 0.32618460 0.90744570 0.53893304 0.36471753 0.29214559 [31] 0.91914909 0.83578129 0.10637126 0.15559371 0.35368828 0.19322608 [37] 0.23442976 0.33605854 0.17083567 0.87818844 0.51282318 0.89569921 [43] 0.65206229 0.06575875 0.81599312 0.99551732 0.04941348 0.80749258 [49] 0.68767939 0.21466972 [1] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" [5] \"California\" \"Colorado\" \"Connecticut\" \"Delaware\" [9] \"Florida\" \"Georgia\" \"Hawaii\" \"Idaho\" [13] \"Illinois\" \"Indiana\" \"Iowa\" \"Kansas\" [17] \"Kentucky\" \"Louisiana\" \"Maine\" \"Maryland\" [21] \"Massachusetts\" \"Michigan\" \"Minnesota\" \"Mississippi\" [25] \"Missouri\" \"Montana\" \"Nebraska\" \"Nevada\" [29] \"New Hampshire\" \"New Jersey\" \"New Mexico\" \"New York\" [33] \"North Carolina\" \"North Dakota\" \"Ohio\" \"Oklahoma\" [37] \"Oregon\" \"Pennsylvania\" \"Rhode Island\" \"South Carolina\" [41] \"South Dakota\" \"Tennessee\" \"Texas\" \"Utah\" [45] \"Vermont\" \"Virginia\" \"Washington\" \"West Virginia\" [49] \"Wisconsin\" \"Wyoming\" # Remove the location variable rm ( location ) percent.sms location [1] 0.58913023 0.54043206 0.14073087 0.68065186 0.74324175 0.75807962 [7] 0.29425877 0.27024361 0.95795936 0.59034216 0.56754098 0.59455496 [13] 0.21656371 0.60917559 0.73271437 0.22795936 0.45305943 0.42421408 [19] 0.40083340 0.17028350 0.88662551 0.17400449 0.42711263 0.10408158 [25] 0.43336437 0.32618460 0.90744570 0.53893304 0.36471753 0.29214559 [31] 0.91914909 0.83578129 0.10637126 0.15559371 0.35368828 0.19322608 [37] 0.23442976 0.33605854 0.17083567 0.87818844 0.51282318 0.89569921 [43] 0.65206229 0.06575875 0.81599312 0.99551732 0.04941348 0.80749258 [49] 0.68767939 0.21466972 Error in eval(expr, envir, enclos): object 'location' not found","tags":"R Stats","url":"http://chrisalbon.com/r-stats/remote-variable.html"},{"title":"Remove Duplicate Rows","loc":"http://chrisalbon.com/r-stats/remove-duplicate-rows.html","text":"Original source: the r book # create a dataframe with simulated values x <- c ( 1 , 2 , 3 , 1 , 2 , 2 ) y <- c ( 1 , 6 , 3 , 1 , 2 , 2 ) z <- c ( 1 , 2 , 3 , 1 , 2 , 2 ) a <- c ( 1 , 5 , 6 , 1 , 2 , 2 ) data <- data.frame ( x , y , z , a ) rm ( x , y , z , a ) # find all the rows that are the same duplicates <- data [ duplicated ( data ),]; duplicates x y z a 4 1 1 1 1 6 2 2 2 2 # find all the rows that are unique not.duplicates <- unique ( data ); not.duplicates x y z a 1 1 1 1 1 2 2 6 2 5 3 3 3 3 6 5 2 2 2 2","tags":"R Stats","url":"http://chrisalbon.com/r-stats/remove-duplicate-rows.html"},{"title":"Removing Observations In Data Frames With Missing Data","loc":"http://chrisalbon.com/r-stats/removing-missing-obs.html","text":"# create a data frame of fake data names <- c ( \"Gob\" , \"George Michael\" , \"Michael\" , \"Maebe\" , \"Jake\" , \"NA\" , \"Taylor\" , \"NA\" , \"Jack\" ) score.1 <- c ( 4355 , NA , 435345 , 435435 , 347754 , 5754364 , 34534543 , 43534534 , NA ) score.2 <- c ( 4355 , 324 , 435345 , 435435 , 347754 , NA , NA , NA , 9230 ) df <- data.frame ( names , score.1 , score.2 ) # index only the rows with no missing data df [ complete.cases ( df ),] names score.1 score.2 1 Gob 4355 4355 3 Michael 435345 435345 4 Maebe 435435 435435 5 Jake 347754 347754","tags":"R Stats","url":"http://chrisalbon.com/r-stats/removing-missing-obs.html"},{"title":"Renaming Filenames","loc":"http://chrisalbon.com/r-stats/rename-filesnames.html","text":"# create a variable with the directory of the files we want to rename loc.dir <- paste ( getwd (), \"/test\" , sep = \"\" ); loc.dir [1] \"/Users/chrisralbon/cra/cra_projects/peripheral_brain/notebooks/rstats/test\" # create a list of files in the directory refs <- list.files ( path = loc.dir , all.files = TRUE , recursive = TRUE , full.names = TRUE ); refs character(0) # create a vector with the numbers we will add to the file names numbers <- 1 : length ( refs ); numbers [1] 1 0 # rename the files in \"refs\" to albon-*.txt, wherein * is the digits in the variable \"numbers\" # file.rename(refs, paste0(loc.dir, \"/\", \"albon-\", numbers, \".txt\"))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/rename-filesnames.html"},{"title":"Reorder Columns","loc":"http://chrisalbon.com/r-stats/reorder-columns.html","text":"Original source: http://stackoverflow.com/questions/5620885/how-does-one-reorder-columns-in-r # create some simulated data df <- data.frame ( 1 , 2 , 3 , 4 ) df X1 X2 X3 X4 1 1 2 3 4 # move the third element to the second element's spot df <- df [, c ( 1 , 3 , 2 , 4 )] df X1 X3 X2 X4 1 1 3 2 4","tags":"R Stats","url":"http://chrisalbon.com/r-stats/reorder-columns.html"},{"title":"Replacing Data","loc":"http://chrisalbon.com/r-stats/replacing-data.html","text":"# create simulated crime data of 20 observations number.of.crimes <- runif ( 20 ) # replace the 10th and 11th value in number of crimes with 14 and 1, respectively replace ( number.of.crimes , c ( 10 , 11 ), c ( 14 , 1 )) [1] 0.76663091 0.90760481 0.17940086 0.73056629 0.10152773 0.79474331 [7] 0.85948238 0.75014760 0.08862732 14.00000000 1.00000000 0.77783735 [13] 0.34835337 0.97988156 0.06695829 0.47863876 0.48860696 0.19188663 [19] 0.66221872 0.74794979","tags":"R Stats","url":"http://chrisalbon.com/r-stats/replacing-data.html"},{"title":"Replacing Factor Names Using Match","loc":"http://chrisalbon.com/r-stats/replacing-factor-names.html","text":"# create simulated factor names district <- factor ( c ( \"NORTH\" , \"NORTHWEST\" , \"CENTRAL\" , \"SOUTH\" , \"SOUTHWEST\" , \"WESTERN\" )) # for the levels of the Baltimore crime district variable, find instances where the names of the levels are WESTERN and replace with WEST levels ( district )[ match ( \"WESTERN\" , levels ( district ))] <- \"WEST\" You can also do another way. Since every level in a factor can be identified by both a name (as done in the previous example) and a number. # view the levels of the district factor levels ( district ) [1] \"CENTRAL\" \"NORTH\" \"NORTHWEST\" \"SOUTH\" \"SOUTHWEST\" \"WEST\" # replace the name of the third factor, NORTHEASTERN, with NORTHEAST levels ( district )[ 3 ] <- 'NORTHEAST' district [1] NORTH NORTHEAST CENTRAL SOUTH SOUTHWEST WEST Levels: CENTRAL NORTH NORTHEAST SOUTH SOUTHWEST WEST","tags":"R Stats","url":"http://chrisalbon.com/r-stats/replacing-factor-names.html"},{"title":"Loops With Replicate","loc":"http://chrisalbon.com/r-stats/replication-loops.html","text":"Original source: Learning R Replicate applies an action individually to every element of a vector # Run runif(1) five times replicate ( 5 , runif ( 1 )) [1] 0.2769136 0.8304662 0.5407557 0.8074367 0.2867881 The below more complicated example, we create a function that first chooses a method of transportation and then based on what method of transport was selection, calculates a travel time based on that method of transport (hence the switch function). rnorm and rlnorm are simply what the example uses to create fake travel time data time_for_commute <- function () { #Choose a mode of transport for the day mode_of_transport <- sample ( c ( \"car\" , \"bus\" , \"train\" , \"bike\" ), size = 1 , prob = c ( 0.1 , 0.2 , 0.3 , 0.4 ) ) #Find the time to travel, depending upon mode of transport time <- switch ( mode_of_transport , car = rlnorm ( 1 , log ( 30 ), 0.5 ), bus = rlnorm ( 1 , log ( 40 ), 0.5 ), train = rnorm ( 1 , 30 , 10 ), bike = rnorm ( 1 , 60 , 5 ) ) names ( time ) <- mode_of_transport time } # Run the function time_for_commute () bike 60.55362 # Run the function five times replicate ( 5 , time_for_commute ()) bike bus train train bike 65.26913 71.15877 23.13382 29.23679 58.59026","tags":"R Stats","url":"http://chrisalbon.com/r-stats/replication-loops.html"},{"title":"Rounding Numbers","loc":"http://chrisalbon.com/r-stats/rounding.html","text":"# round up to the nearest integer ceiling ( 3.14159 ) [1] 4 # round down to the nearest integer floor ( 3.14159 ) [1] 3 # cut off everything after the decimal point trunc ( 3.14159 ) [1] 3 # round the number to the third decimal place round ( 3.14159 , digits = 3 ) [1] 3.142 # round to five significant digits signif ( 3.14159 , digits = 5 ) [1] 3.1416","tags":"R Stats","url":"http://chrisalbon.com/r-stats/rounding.html"},{"title":"Rug Plot","loc":"http://chrisalbon.com/r-stats/rugplot.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # create the scatterplot sps <- ggplot ( heightweight , aes ( x = ageYear , y = heightIn , colour = sex )) + geom_point () + scale_colour_brewer ( palette = \"Set1\" ) # add the riug plot sps + geom_rug ( position = \"jitter\" , size = .2 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/rugplot.html"},{"title":"Sampling in R","loc":"http://chrisalbon.com/r-stats/sampling.html","text":"# Create a population of data population <- rnorm ( 1000 ) # Sample 100 observation from the population of data, without replacement sample <- sample ( population , 100 , replace = FALSE ) # View the sample sample [1] -0.129219040 1.518986856 -1.121348640 -1.150077471 -0.489263654 [6] -0.841177103 -0.774176234 0.070402146 -0.109056603 -1.648204067 [11] 0.998132544 -0.249399062 0.792774951 0.497899767 0.779363069 [16] 1.343803830 -0.260186522 1.121706668 -1.458195137 1.603245141 [21] 0.424563292 -2.208635666 1.433034664 -1.087884969 0.505870371 [26] 1.139785998 -0.825174901 0.604944521 0.482636039 0.329966134 [31] -0.356832455 0.202305723 -1.269608482 -1.876274254 -1.568588222 [36] 1.027197383 0.569795152 0.394942749 -0.059843038 1.051632852 [41] -0.761036116 -1.177838442 -0.252533986 0.447810962 -1.377854032 [46] -0.563699314 -0.376471704 0.097157070 0.821499638 -0.274195994 [51] 0.640105046 -1.177857754 1.021021671 0.723894247 -0.557805790 [56] 2.313566185 0.070609946 -0.941583920 -0.851167218 -1.113739717 [61] 0.738163098 -0.274254246 1.052859723 -0.691817242 -0.003154583 [66] 0.039907038 0.301564691 -1.773501450 -0.207059604 0.059896893 [71] -0.478809890 0.305387446 0.154420143 -1.398018312 -0.748575560 [76] 0.772815759 0.398396468 0.478987088 -0.677324774 -0.422754198 [81] 0.064750414 -0.813921740 1.220945535 0.054736122 -1.230677889 [86] -1.105874754 -0.705068905 -0.496520306 1.332298141 1.246263878 [91] -0.020308835 -1.801665946 -0.607985694 -1.138447356 -0.375253558 [96] 0.787363533 -1.925421450 -1.608369372 0.187454701 1.008443172","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sampling.html"},{"title":"Getting A Sample Of Random Rows From A Dataframe","loc":"http://chrisalbon.com/r-stats/sampling-a-dataframe.html","text":"Original source: http://stackoverflow.com/questions/8273313/random-rows-in-dataframe-in-r # create some simulated data ID <- 1 : 10 Age <- c ( 26 , 65 , 15 , 7 , 88 , 43 , 28 , 66 , 45 , 12 ) Sex <- c ( 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ) Weight <- c ( 132 , 122 , 184 , 145 , 118 , NA , 128 , 154 , 166 , 164 ) Height <- c ( 60 , 63 , 57 , 59 , 64 , NA , 67 , 65 , NA , 60 ) Married <- c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ) # create a dataframe of the simulated data df <- data.frame ( ID , Age , Sex , Weight , Height , Married ) df ID Age Sex Weight Height Married 1 1 26 1 132 60 0 2 2 65 0 122 63 0 3 3 15 1 184 57 0 4 4 7 1 145 59 0 5 5 88 0 118 64 0 6 6 43 1 NA NA 0 7 7 28 1 128 67 1 8 8 66 1 154 65 1 9 9 45 0 166 NA 0 10 10 12 1 164 60 1 # create some simulated data ID <- 1 : 10 Age <- c ( 26 , 65 , 15 , 7 , 88 , 43 , 28 , 66 , 45 , 12 ) Sex <- c ( 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ) Weight <- c ( 132 , 122 , 184 , 145 , 118 , NA , 128 , 154 , 166 , 164 ) Height <- c ( 60 , 63 , 57 , 59 , 64 , NA , 67 , 65 , NA , 60 ) Married <- c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ) # create a new object that is comprised of three rows of the dataframe df, taken as random. literally what it is doing is chosing three numbers at random out of the total number of rows in a dataframe, and use that as the row index to definite which rows are included. df.sample <- df [ sample ( nrow ( df ), 3 ), ] df.sample ID Age Sex Weight Height Married 7 7 28 1 128 67 1 2 2 65 0 122 63 0 5 5 88 0 118 64 0","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sampling-a-dataframe.html"},{"title":"Scatter Plot (Basic)","loc":"http://chrisalbon.com/r-stats/scatterplot.html","text":"# load the ggplot2 package library ( ggplot2 ) # plot using base graphics plot ( mtcars $ wt , mtcars $ mpg ) # plot using ggplot2 ggplot ( mtcars , aes ( x = wt , y = mpg )) + geom_point ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplot.html"},{"title":"Smooth A Scatterplot Trend Line","loc":"http://chrisalbon.com/r-stats/scatterplot-trend-line.html","text":"Original source: ggplot2 book # load the ggplot2 library library ( ggplot2 ) # set the seed so we can reproduce the results set.seed ( 1410 ) # create a variable that is the first 100 rows of the diamonds dataset dsmall <- diamonds [ sample ( nrow ( diamonds ), 100 ), ] # create a scatterplot with a smoothing line with a wiggly trend line p1 <- qplot ( carat , price , data = dsmall , geom = c ( \"point\" , \"smooth\" ), span = 0.2 ); p1 geom_smooth: method=\"auto\" and size of largest group is <1000, so using loess. Use 'method = x' to change the smoothing method. # create a scatterplot with a smoothing line with a smooth trend line p2 <- qplot ( carat , price , data = dsmall , geom = c ( \"point\" , \"smooth\" ), span = 1 ); p2 geom_smooth: method=\"auto\" and size of largest group is <1000, so using loess. Use 'method = x' to change the smoothing method.","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplot-trend-line.html"},{"title":"Scatter plot two sets of data with smooth lines","loc":"http://chrisalbon.com/r-stats/scatterplot-two-sets-of-data.html","text":"# Create fake infant mortality data for Spain spain.imr <- rnorm ( 50 ) # Create fake GDP data for Spain spain.gdp <- rnorm ( 50 , 3 , 1 ) # Create fake infant mortality data for Yemen yemen.imr <- rnorm ( 50 ) # Create fake GDP data for Yemen yemen.gdp <- rnorm ( 50 , 1 , 1 ) # Create a scatter spot with both variables from both sets of Spanish and Yemeni data, all data is blue plot ( spain.imr , spain.gdp , xlim = range ( c ( spain.imr , yemen.imr )), ylim = range ( c ( spain.gdp , yemen.gdp )), col = \"blue\" , xlab = \"Infant Mortality Rate\" , ylab = \"Gross Domestic Product\" ) # Change all Yemen data to red points ( yemen.imr , yemen.gdp , col = \"red\" ) # Create a smoothing line for both Yemen's and Spain's data points ( loess.smooth ( spain.imr , spain.gdp ), type = \"l\" , col = \"blue\" ) points ( loess.smooth ( yemen.imr , yemen.gdp ), type = \"l\" , col = \"red\" ) # Create a legend to explain the colors legend ( \"topright\" , c ( \"Spain\" , \"Yemen\" ), col = c ( \"blue\" , \"red\" ), text.col = \"black\" , lty = c ( 0 , 0 ), pch = c ( 1 , 1 ), bg = \"white\" ) # Create a title with black font in bold title ( main = \"GDP and IMR in Spain and Yemen\" , col.main = \"Black\" , font.main = 4 )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplot-two-sets-of-data.html"},{"title":"Plot Ellipses With Scatterplot","loc":"http://chrisalbon.com/r-stats/scatterplot-with-ellipses-by-group.html","text":"Original source: http://stackoverflow.com/questions/2397097/how-can-a-data-ellipse-be-superimposed-on-a-ggplot2-scatterplot # create a dataframe of simulated data x <- c ( 2 , 3 , 4 , 10 , 12 , 10 , 20 , 21 , 23 ) y <- c ( 50 , 54 , 49 , 30 , 25 , 26 , 5 , 6 , 5 ) group <- c ( \"a\" , \"a\" , \"a\" , \"b\" , \"b\" , \"b\" , \"c\" , \"c\" , \"c\" ) df <- data.frame ( x , y , group ) rm ( x , y , group ) # load packages library ( ggplot2 ) library ( ellipse ) # calculating the ellipses by df$group # create an empty dataframe df_ell <- data.frame () # for each level in df$groups for ( g in levels ( df $ group )){ # create 100 points per variable around the mean of each group df_ell <- rbind ( df_ell , cbind ( as.data.frame ( with ( df [ df $ group == g ,], ellipse ( cor ( x , y ), scale = c ( sd ( x ), sd ( y )), centre = c ( mean ( x ), mean ( y )) ) ) ), group = g )) } # create the ggplot with points colored by grouo ggplot ( data = df , aes ( x = x , y = y , colour = group )) + # draw points geom_point ( size = 1.5 , alpha = .6 ) + # draw ellipse lines geom_path ( data = df_ell , aes ( x = x , y = y , colour = group ), size = 1 , linetype = 1 ) + # style as black and white theme theme_bw ()","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplot-with-ellipses-by-group.html"},{"title":"Scatterplot With Point's Labelled","loc":"http://chrisalbon.com/r-stats/scatterplot-with-point-labels.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # reset the graphing device dev.off () null device 1 # create the scatterplot sps <- ggplot ( heightweight , aes ( x = ageYear , y = heightIn , colour = sex )) + geom_point () + scale_colour_brewer ( palette = \"Set1\" ) # add a layer of point labels to the scatterplot, offset of .2 of the x axis sps + geom_text ( aes ( x = ageYear + .2 , label = sex ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplot-with-point-labels.html"},{"title":"Scatterplot With Trend Line","loc":"http://chrisalbon.com/r-stats/scatterplot-with-trendline.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) # load plyr package library ( plyr ) # reset the graphing device dev.off () null device 1 # create the scatterplot sps <- ggplot ( heightweight , aes ( x = ageYear , y = heightIn , colour = sex )) + geom_point () + scale_colour_brewer ( palette = \"Set1\" ) # create a scatterplot with loess smoothing sps + geom_smooth () geom_smooth: method=\"auto\" and size of largest group is <1000, so using loess. Use 'method = x' to change the smoothing method. # create a scatterplot with lm (straight line) trend line sps + geom_smooth ( method = lm , se = FALSE , fullrange = TRUE )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplot-with-trendline.html"},{"title":"Scatterplots using ggplot2","loc":"http://chrisalbon.com/r-stats/scatterplots-ggplot2.html","text":"Original source: http://rforpublichealth.blogspot.com/2013/11/ggplot2-cheatsheet-for-scatterplots.html # load the ggplot2 package library ( ggplot2 ) # load the gridExtra package library ( gridExtra ) # load the mt cars dataset mtc <- mtcars # create a basic scatterplot base layer, p1 p1 <- ggplot ( mtc , aes ( x = hp , y = mpg )) # print the scatter plot with points p1 + geom_point () # print the scatter plot is red points p2 <- p1 + geom_point ( color = \"red\" ) # print the scatter plot with point colors determined by a continuous variable p3 <- p1 + geom_point ( aes ( color = wt )) # print the scatter plot with point colors determined by a factor variable p4 <- p1 + geom_point ( aes ( color = factor ( am ))) # print the scatter plot with point with custom colors for a factor variable p1 + geom_point ( aes ( color = factor ( am ))) + scale_color_manual ( values = c ( \"orange\" , \"purple\" )) # print scatterplots p2, p3, and p4 in a single row grid.arrange ( p2 , p3 , p4 , nrow = 1 ) # create a scatter plot, p2, with large point sizes p2 <- p1 + geom_point ( size = 5 ) # print the scatter plot with large point sizes p2 + geom_point ( size = 5 ) # create a scatter plot, p3, with large point sizes p3 <- p1 + geom_point ( aes ( size = wt )) # print the scatter plot with point sizes determined by a continous variable p3 + geom_point ( aes ( size = wt )) # create a scatter plot, p4, with large point sizes p4 <- p1 + geom_point ( aes ( shape = factor ( am ))) # print the scatter plot with point shape determined by a factor variable p4 + geom_point ( aes ( shape = factor ( am ))) # print scatterplots p2, p3, and p4 in a single row grid.arrange ( p2 , p3 , p4 , nrow = 1 ) # create a scatter plot with blue points, connected by a line p2 <- p1 + geom_point ( color = \"blue\" ) + geom_line () # create a scatter plot with red points, and a regression line p3 <- p1 + geom_point ( color = \"red\" ) + geom_smooth ( method = \"lm\" , se = TRUE ) # create a scatter plot with a red vertical line at x = 100 p4 <- p1 + geom_point () + geom_vline ( xintercept = 100 , color = \"red\" ) # print scatterplots p2, p3, and p4 in a single row grid.arrange ( p2 , p3 , p4 , nrow = 1 ) # print scatterplot with no points, just a line, colored by a factor ggplot ( mtc , aes ( x = wt , y = qsec )) + geom_line ( size = 2 , aes ( color = factor ( vs ))) # create a scatter plot of hp and mpg p2 <- ggplot ( mtc , aes ( x = hp , y = mpg )) + geom_point () # create a scatter plot with x and y labels p2 + labs ( x = \"Horsepower\" , y = \"Miles per Gallon\" ) # create a scatter plot with a bold, large x label p2 + theme ( axis.title.x = element_text ( face = \"bold\" , size = 20 )) + labs ( x = \"Horsepower\" ) # print a scatter plot with a longer x axis (limits at 0 and 400) and more vertical lines (breaks start a 0, end at 400, draw a line every 50) p2 + scale_x_continuous ( \"Horsepower\" , limits = c ( 0 , 400 ), breaks = seq ( 0 , 400 , 50 )) # create a scatter plot of points with the color determined by wt g5 <- ggplot ( mtc , aes ( x = hp , y = mpg )) + geom_point ( size = 2 , aes ( color = wt )) # print a scatterplot with a legend titled \"Weight, with the legend color determined by the min, max, and mean of a continous variable (wt), with the labels on the being Light, Medium, and Heavy, and the gradient goes between red and pink g5 + scale_color_continuous ( name = \"Weight\" , breaks = with ( mtc , c ( min ( wt ), mean ( wt ), max ( wt ))), labels = c ( \"Light\" , \"Medium\" , \"Heavy\" ), low = \"pink\" , high = \"red\" ) # create a scatterplot g2 <- ggplot ( mtc , aes ( x = hp , y = mpg )) + geom_point () # create a theme with no lines, a white background, and only axis lines t1 <- theme ( plot.background = element_blank (), panel.grid.major = element_blank (), panel.grid.minor = element_blank (), panel.border = element_blank (), panel.background = element_blank (), axis.line = element_line ( size = .4 ) ) # create a theme styling the axis labels and plot title t2 <- theme ( axis.title.x = element_text ( face = \"bold\" , color = \"black\" , size = 10 ), axis.title.y = element_text ( face = \"bold\" , color = \"black\" , size = 10 ), plot.title = element_text ( face = \"bold\" , color = \"black\" , size = 12 ) ) # create a scatterplot of g2, styled with theme t1 g3 <- g2 + t1 # create a scatterplot with the built-in black and white theme g4 <- g2 + theme_bw () # create a scatterplot with the built-in black and white theme AND the t2 custom theme g5 <- g2 + theme_bw () + t2 + labs ( x = \"Horsepower\" , y = \"Miles per Gallon\" , title = \"MPG vs Horsepower\" ) # print the scatter plots g2, g3, g4, g5 in two rows grid.arrange ( g2 , g3 , g4 , g5 , nrow = 2 ) # create a scatter plot (notes inline) g2 <- ggplot ( mtc , aes ( x = hp , y = mpg )) + # create points with size two, and the color and scape determined by a factor variable geom_point ( size = 2 , aes ( color = factor ( vs ), shape = factor ( vs ))) + # add a linear regression line that displays standard error shading geom_smooth ( aes ( color = factor ( vs )), method = \"lm\" , se = TRUE ) + # add a legend titled \"Engine\" with two parts: V-engine, colored red, and Straight engine, colored blue scale_color_manual ( name = \"Engine\" , labels = c ( \"V-engine\" , \"Straight engine\" ), values = c ( \"red\" , \"blue\" )) + # add to the legend custom shapes scale_shape_manual ( name = \"Engine\" , labels = c ( \"V-engine\" , \"Straight engine\" ), values = c ( 0 , 2 )) + # style the scatterplot with the built in black and white theme theme_bw () + # stype the scatterplot with the custom theme theme ( axis.title.x = element_text ( face = \"bold\" , color = \"black\" , size = 12 ), axis.title.y = element_text ( face = \"bold\" , color = \"black\" , size = 12 ), plot.title = element_text ( face = \"bold\" , color = \"black\" , size = 12 ), legend.position = c ( 1 , 1 ), legend.justification = c ( 1 , 1 )) + labs ( x = \"Horsepower\" , y = \"Miles per Gallon\" , title = \"Linear Regression (95% CI) of MPG vs Horsepower by Engine type\" ) g2","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scatterplots-ggplot2.html"},{"title":"Scheduling Jobs In The Future","loc":"http://chrisalbon.com/python/schedule_run_in_the_future.html","text":"# Import required modules import sched import time # setup the scheduler with our time settings s = sched . scheduler ( time . time , time . sleep ) # Create a function we want to run in the future. def print_time (): print ( \"Executive Order 66\" ) # Create a function for the delay def print_some_times (): # Create a scheduled job that will run # the function called 'print_time' # after 10 seconds, and with priority 1. s . enter ( 10 , 1 , print_time ) # Run the scheduler s . run () # Run the function for the delay print_some_times () Executive Order 66","tags":"Python","url":"http://chrisalbon.com/python/schedule_run_in_the_future.html"},{"title":"Simple Clustering With SciPy","loc":"http://chrisalbon.com/python/scipy_simple_clustering.html","text":"Import modules import numpy as np % matplotlib inline import matplotlib.pyplot as plt from scipy.cluster import vq Create coordinates for battles for each year of the war # create 100 coordinate pairs (i.e. two values), then add 5 to all of them year_1 = np . random . randn ( 100 , 2 ) + 5 # create 30 coordinatee pairs (i.e. two values), then subtract 5 to all of them year_2 = np . random . randn ( 30 , 2 ) - 5 # create 50 coordinatee pairs (i.e. two values) year_3 = np . random . randn ( 50 , 2 ) View the first 3 entries of each year of battles print ( 'year 1 battles:' , year_1 [ 0 : 3 ]) print ( 'year 2 battles:' , year_2 [ 0 : 3 ]) print ( 'year 3 battles:' , year_3 [ 0 : 3 ]) year 1 battles: [[ 3.87032104 4.93418141] [ 4.47603646 3.23230121] [ 6.15905943 4.55274026]] year 2 battles: [[-3.55642932 -3.13125097] [-5.83295449 -5.75787649] [-5.12144789 -5.00466761]] year 3 battles: [[-0.27557365 -0.65002898] [ 0.94593878 -0.46056352] [ 0.91003511 0.27888337]] Pool all three years of coordinates # vertically stack year_1, year_2, and year_3 elements battles = np . vstack ([ year_1 , year_2 , year_3 ]) Cluster the battle locations into three groups # calculate the centroid coordinates of each cluster # and the variance of all the clusters centroids , variance = vq . kmeans ( battles , 3 ) View the centroid coordinate for each of the three clusters centroids array([[ 4.89478443, 5.00806609], [ 0.16770004, 0.01639683], [-5.06447231, -4.99956259]]) View the variance of the clusters (they all share the same) variance 1.2382236882037887 Seperate the battle data into clusters identified , distance = vq . vq ( battles , centroids ) View the cluster of each battle identified View the distance of each individual battle from their cluster's centroid distance Index the battles data by the cluster to which they belong cluster_1 = battles [ identified == 0 ] cluster_2 = battles [ identified == 1 ] cluster_3 = battles [ identified == 2 ] Print the first three coordinate pairs of each cluster print ( cluster_1 [ 0 : 3 ]) print ( cluster_2 [ 0 : 3 ]) print ( cluster_3 [ 0 : 3 ]) [[ 3.87032104 4.93418141] [ 4.47603646 3.23230121] [ 6.15905943 4.55274026]] [[-0.27557365 -0.65002898] [ 0.94593878 -0.46056352] [ 0.91003511 0.27888337]] [[-3.55642932 -3.13125097] [-5.83295449 -5.75787649] [-5.12144789 -5.00466761]] Plot all the battles, color each battle by cluster # create a scatter plot there the x-axis is the first column of battles # the y-axis is the second column of battles, the size is 100, and # the color of each point is determined by the indentified variable plt . scatter ( battles [:, 0 ], battles [:, 1 ], s = 100 , c = identified ) <matplotlib.collections.PathCollection at 0x10771b890>","tags":"Python","url":"http://chrisalbon.com/python/scipy_simple_clustering.html"},{"title":"Scraping Web Pages","loc":"http://chrisalbon.com/r-stats/scraping-webpages.html","text":"Original source: Learning R # Load RCurl Library library ( RCurl ) # Create a string with the URL to the website time_url <- \"http://tycho.usno.navy.mil/cgi-bin/timer.pl\" # Download the HTML time_page <- getURL ( time_url ) # Use concatenate and view the html in a pretty way cat ( time_page ) <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final\"//EN> <html> <body> <TITLE>What time is it?</TITLE> <H2> US Naval Observatory Master Clock Time</H2> <H3><PRE> <BR>Mar. 14, 18:14:26 UTC Universal Time <BR>Mar. 14, 02:14:26 PM EDT Eastern Time <BR>Mar. 14, 01:14:26 PM CDT Central Time <BR>Mar. 14, 12:14:26 PM MDT Mountain Time <BR>Mar. 14, 11:14:26 AM PDT Pacific Time <BR>Mar. 14, 10:14:26 AM AKDT Alaska Time <BR>Mar. 14, 08:14:26 AM HAST Hawaii-Aleutian Time </PRE></H3><P><A HREF=\"http://www.usno.navy.mil\"> US Naval Observatory</A> </body></html> # load XML library library ( XML ) # parse the HTML time_doc <- htmlParse ( time_page ); time_doc <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final\"> <html><body> <p>/EN&gt; </p> <title>What time is it?</title> <h2> US Naval Observatory Master Clock Time</h2> <h3><pre> <br>Mar. 14, 18:14:26 UTC Universal Time <br>Mar. 14, 02:14:26 PM EDT Eastern Time <br>Mar. 14, 01:14:26 PM CDT Central Time <br>Mar. 14, 12:14:26 PM MDT Mountain Time <br>Mar. 14, 11:14:26 AM PDT Pacific Time <br>Mar. 14, 10:14:26 AM AKDT Alaska Time <br>Mar. 14, 08:14:26 AM HAST Hawaii-Aleutian Time </pre></h3> <p><a href=\"http://www.usno.navy.mil\"> US Naval Observatory</a> </p> </body></html> # extract everything within the \"pre\" tag. The // denotes that we are searching the entire document. The [[1]] refers to the fact we are not moving a list to pre but moving the contents of the list. pre <- xpathSApply ( time_doc , \"//pre\" )[[ 1 ]] # split along newline \\n, divides up each time values <- strsplit ( xmlValue ( pre ), \"\\n\" )[[ 1 ]][ -1 ] # split along the tabs \\t+ divides each time into time the time and timezone times <- strsplit ( values , \"\\t+\" ) times [[1]] [1] \"Mar. 14, 18:14:26 UTC\" \"Universal Time\" [[2]] [1] \"Mar. 14, 02:14:26 PM EDT\" \"Eastern Time\" [[3]] [1] \"Mar. 14, 01:14:26 PM CDT\" \"Central Time\" [[4]] [1] \"Mar. 14, 12:14:26 PM MDT\" \"Mountain Time\" [[5]] [1] \"Mar. 14, 11:14:26 AM PDT\" \"Pacific Time\" [[6]] [1] \"Mar. 14, 10:14:26 AM AKDT\" \"Alaska Time\" [[7]] [1] \"Mar. 14, 08:14:26 AM HAST\" \"Hawaii-Aleutian Time\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/scraping-webpages.html"},{"title":"Color Palettes in Seaborn","loc":"http://chrisalbon.com/python/seaborn_color_palettes.html","text":"Preliminaries import pandas as pd % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'deaths_regiment_1' : [ 34 , 43 , 14 , 15 , 15 , 14 , 31 , 25 , 62 , 41 ], 'deaths_regiment_2' : [ 52 , 66 , 78 , 15 , 15 , 5 , 25 , 25 , 86 , 1 ], 'deaths_regiment_3' : [ 13 , 73 , 82 , 58 , 52 , 87 , 26 , 5 , 56 , 75 ], 'deaths_regiment_4' : [ 44 , 75 , 26 , 15 , 15 , 14 , 54 , 25 , 24 , 72 ], 'deaths_regiment_5' : [ 25 , 24 , 25 , 15 , 57 , 68 , 21 , 27 , 62 , 5 ], 'deaths_regiment_6' : [ 84 , 84 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 24 ], 'deaths_regiment_7' : [ 46 , 57 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 41 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'battle_deaths' , 'deaths_regiment_1' , 'deaths_regiment_2' , 'deaths_regiment_3' , 'deaths_regiment_4' , 'deaths_regiment_5' , 'deaths_regiment_6' , 'deaths_regiment_7' ]) df = df . set_index ( df . date ) View some color palettes sns . palplot ( sns . color_palette ( \"deep\" , 10 )) sns . palplot ( sns . color_palette ( \"muted\" , 10 )) sns . palplot ( sns . color_palette ( \"bright\" , 10 )) sns . palplot ( sns . color_palette ( \"dark\" , 10 )) sns . palplot ( sns . color_palette ( \"colorblind\" , 10 )) sns . palplot ( sns . color_palette ( \"Paired\" , 10 )) sns . palplot ( sns . color_palette ( \"BuGn\" , 10 )) sns . palplot ( sns . color_palette ( \"GnBu\" , 10 )) sns . palplot ( sns . color_palette ( \"OrRd\" , 10 )) sns . palplot ( sns . color_palette ( \"PuBu\" , 10 )) sns . palplot ( sns . color_palette ( \"YlGn\" , 10 )) sns . palplot ( sns . color_palette ( \"YlGnBu\" , 10 )) sns . palplot ( sns . color_palette ( \"YlOrBr\" , 10 )) sns . palplot ( sns . color_palette ( \"YlOrRd\" , 10 )) sns . palplot ( sns . color_palette ( \"BrBG\" , 10 )) sns . palplot ( sns . color_palette ( \"PiYG\" , 10 )) sns . palplot ( sns . color_palette ( \"PRGn\" , 10 )) sns . palplot ( sns . color_palette ( \"PuOr\" , 10 )) sns . palplot ( sns . color_palette ( \"RdBu\" , 10 )) sns . palplot ( sns . color_palette ( \"RdGy\" , 10 )) sns . palplot ( sns . color_palette ( \"RdYlBu\" , 10 )) sns . palplot ( sns . color_palette ( \"RdYlGn\" , 10 )) sns . palplot ( sns . color_palette ( \"Spectral\" , 10 )) Create a color palette and set it as the current color palette flatui = [ \"#9b59b6\" , \"#3498db\" , \"#95a5a6\" , \"#e74c3c\" , \"#34495e\" , \"#2ecc71\" ] sns . set_palette ( flatui ) sns . palplot ( sns . color_palette ()) Set the color of a plot sns . tsplot ([ df . deaths_regiment_1 , df . deaths_regiment_2 , df . deaths_regiment_3 , df . deaths_regiment_4 , df . deaths_regiment_5 , df . deaths_regiment_6 , df . deaths_regiment_7 ], color = \"#34495e\" ) <matplotlib.axes._subplots.AxesSubplot at 0x10aa2c650>","tags":"Python","url":"http://chrisalbon.com/python/seaborn_color_palettes.html"},{"title":"Creating A Time Series Plot With Seaborn And Pandas","loc":"http://chrisalbon.com/python/seaborn_pandas_timeseries_plot.html","text":"Preliminaries import pandas as pd % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'deaths_regiment_1' : [ 34 , 43 , 14 , 15 , 15 , 14 , 31 , 25 , 62 , 41 ], 'deaths_regiment_2' : [ 52 , 66 , 78 , 15 , 15 , 5 , 25 , 25 , 86 , 1 ], 'deaths_regiment_3' : [ 13 , 73 , 82 , 58 , 52 , 87 , 26 , 5 , 56 , 75 ], 'deaths_regiment_4' : [ 44 , 75 , 26 , 15 , 15 , 14 , 54 , 25 , 24 , 72 ], 'deaths_regiment_5' : [ 25 , 24 , 25 , 15 , 57 , 68 , 21 , 27 , 62 , 5 ], 'deaths_regiment_6' : [ 84 , 84 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 24 ], 'deaths_regiment_7' : [ 46 , 57 , 26 , 15 , 15 , 14 , 26 , 25 , 62 , 41 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'battle_deaths' , 'deaths_regiment_1' , 'deaths_regiment_2' , 'deaths_regiment_3' , 'deaths_regiment_4' , 'deaths_regiment_5' , 'deaths_regiment_6' , 'deaths_regiment_7' ]) df = df . set_index ( df . date ) Time Series Plot sns . tsplot ([ df . deaths_regiment_1 , df . deaths_regiment_2 , df . deaths_regiment_3 , df . deaths_regiment_4 , df . deaths_regiment_5 , df . deaths_regiment_6 , df . deaths_regiment_7 ], color = \"indianred\" ) <matplotlib.axes._subplots.AxesSubplot at 0x10bd2c350> Time Series Splot With Confidence Interval Lines But No Lines sns . tsplot ([ df . deaths_regiment_1 , df . deaths_regiment_2 , df . deaths_regiment_3 , df . deaths_regiment_4 , df . deaths_regiment_5 , df . deaths_regiment_6 , df . deaths_regiment_7 ], err_style = \"ci_bars\" , interpolate = False ) <matplotlib.axes._subplots.AxesSubplot at 0x10c9b7b50>","tags":"Python","url":"http://chrisalbon.com/python/seaborn_pandas_timeseries_plot.html"},{"title":"Creating Scatterplots with Seaborn","loc":"http://chrisalbon.com/python/seaborn_scatterplot.html","text":"Preliminaries import pandas as pd % matplotlib inline import random import matplotlib.pyplot as plt import seaborn as sns df = pd . DataFrame () df [ 'x' ] = random . sample ( range ( 1 , 1000 ), 5 ) df [ 'y' ] = random . sample ( range ( 1 , 1000 ), 5 ) df [ 'z' ] = [ 1 , 0 , 0 , 1 , 0 ] df [ 'k' ] = [ 'male' , 'male' , 'male' , 'female' , 'female' ] df . head () x y z k 0 859 714 1 male 1 70 321 0 male 2 378 12 0 male 3 737 93 1 female 4 375 956 0 female Scatterplot sns . set_context ( \"notebook\" , font_scale = 1.1 ) sns . set_style ( \"ticks\" ) sns . lmplot ( 'x' , 'y' , data = df , fit_reg = False , dropna = True , hue = \"z\" , scatter_kws = { \"marker\" : \"D\" , \"s\" : 100 }) plt . title ( 'Histogram of IQ' ) plt . xlabel ( 'Time' ) plt . ylabel ( 'Deaths' ) <matplotlib.text.Text at 0x10b4a0850>","tags":"Python","url":"http://chrisalbon.com/python/seaborn_scatterplot.html"},{"title":"Selecting Data According To Patterns","loc":"http://chrisalbon.com/r-stats/select-data-according-to-patterns.html","text":"# create simulated district names district <- factor ( c ( \"NORTH\" , \"NORTHWEST\" , \"CENTRAL\" , \"SOUTH\" , \"SOUTHWEST\" , \"WESTERN\" )) #Select all the cases where Baltimore's district name is \"North\" district [ grep ( \"NORTH\" , as.character ( district ))] [1] NORTH NORTHWEST Levels: CENTRAL NORTH NORTHWEST SOUTH SOUTHWEST WESTERN","tags":"R Stats","url":"http://chrisalbon.com/r-stats/select-data-according-to-patterns.html"},{"title":"Select Rows By Logical Test","loc":"http://chrisalbon.com/r-stats/select-rows-by-logical-test.html","text":"Original source: the r book # create a dataframe with simulated values x <- runif ( 10 ) y <- runif ( 10 ) z <- runif ( 10 ) a <- runif ( 10 ) data <- data.frame ( x , y , z , a ) rm ( x , y , z , a ) # select all rows where y is greater than x data [ data $ y > data $ x ,] x y z a 2 0.3617916 0.5513793 0.73714905 0.8678027 3 0.4121965 0.8947345 0.00735662 0.3614312 6 0.7360850 0.8569630 0.97564971 0.6338547 8 0.2491201 0.3543410 0.41584055 0.5313946 9 0.1619093 0.6615272 0.51492096 0.9567799 # select all rows where y IS NOT greater than x data [ ! ( data $ y > data $ x ),] x y z a 1 0.8097187 0.4683501 0.9130371 0.18605450 4 0.8210452 0.2808778 0.1992804 0.03930297 5 0.2189346 0.1651140 0.2851813 0.64965427 7 0.6266258 0.2833319 0.7272680 0.92645638 10 0.8173277 0.3407574 0.7902216 0.11055400","tags":"R Stats","url":"http://chrisalbon.com/r-stats/select-rows-by-logical-test.html"},{"title":"Create a sequences of numbers","loc":"http://chrisalbon.com/r-stats/sequences.html","text":"We can use the collem to create a sequence of integers. # Create a sequence from 1 to 5, spread one number apart 1 : 5 [1] 1 2 3 4 5 However, for more control we can use seq.int to specify the spacing the sequence # Create a sequence from 1 to 10, spread 2.5 apart seq.int ( 1 , 10 , 2.5 ) [1] 1.0 3.5 6.0 8.5","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sequences.html"},{"title":"Set The Color Of A Matplotlib Plot","loc":"http://chrisalbon.com/python/set_the_color_of_a_matplotlib.html","text":"Import numpy and matplotlib.pyplot % matplotlib inline import numpy as np import matplotlib.pyplot as plt Create some simulated data. n = 100 r = 2 * np . random . rand ( n ) theta = 2 * np . pi * np . random . rand ( n ) area = 200 * r ** 2 * np . random . rand ( n ) colors = theta Create a scatterplot using the a colormap. Full list of colormaps: http://wiki.scipy.org/Cookbook/Matplotlib/Show_colormaps c = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . RdYlGn ) c1 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . Blues ) c2 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . BrBG ) c3 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . Greens ) c4 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . RdGy ) c5 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . YlOrRd ) c6 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . autumn ) c7 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . binary ) c8 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . gist_earth ) c9 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . gist_heat ) c10 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . hot ) c11 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . spring ) c12 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . summer ) c12 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . winter ) c13 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . bone ) c14 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . cool ) c15 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . YlGn ) c16 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . RdBu ) c17 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . PuOr ) c18 = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . Oranges )","tags":"Python","url":"http://chrisalbon.com/python/set_the_color_of_a_matplotlib.html"},{"title":"Simple Venn Diagram","loc":"http://chrisalbon.com/r-stats/simple-venn-diagram.html","text":"Original source: http://stackoverflow.com/questions/8713994/venn-diagram-in-r-proportional-and-color-shading-possible-semi-transparency-sup # load the venneuler package require ( venneuler ) Loading required package: venneuler Loading required package: rJava # run the package on some simulated data v <- venneuler ( c ( A = 100 , B = 200 , \"A&B\" = 50 )) # plot it plot ( v )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/simple-venn-diagram.html"},{"title":"Simulated Data From Normal Distribution Function","loc":"http://chrisalbon.com/r-stats/simulated-data-from-norm-distributions.html","text":"Original source: http://rforpublichealth.blogspot.com/2013/02/normal-distribution-functions.html # set seed to 3000 for reproducability set.seed ( 3000 ) # create a sequence from 04 to 4 in increments of 0.01 xseq <- seq ( -4 , 4 , .01 ) # create a probability density function of xseq, with mean 0 and an standard deviation of 1 densities <- dnorm ( xseq , 0 , 1 ) # create a cumative distribution function of xseq with mean 0 and a SD of 1 cumulative <- pnorm ( xseq , 0 , 1 ) # create 1000 random numbers from a normal distribution with mean 0 and an sd of 1 randomdeviates <- rnorm ( 1000 , 0 , 1 ) # create a grid to hold all the plots par ( mfrow = c ( 1 , 3 ), mar = c ( 3 , 4 , 4 , 2 )) # make the first plot plot ( xseq , densities , col = \"darkgreen\" , xlab = \"\" , ylab = \"Density\" , type = \"l\" , lwd = 2 , cex = 2 , main = \"PDF of Standard Normal\" , cex.axis = .8 ) # make the second plot plot ( xseq , cumulative , col = \"darkorange\" , xlab = \"\" , ylab = \"Cumulative Probability\" , type = \"l\" , lwd = 2 , cex = 2 , main = \"CDF of Standard Normal\" , cex.axis = .8 ) # make the third plot hist ( randomdeviates , main = \"Random draws from Std Normal\" , cex.axis = .8 , xlim = c ( -4 , 4 ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/simulated-data-from-norm-distributions.html"},{"title":"Size Plot","loc":"http://chrisalbon.com/r-stats/sizeplot.html","text":"Original source: http://www.r-bloggers.com/using-r-coloured-sizeplot-with-ggplot2/ # load the ggplot2 package library ( ggplot2 ) # load the reshape2 library ( reshape2 ) # create some simulated data data <- data.frame ( x = c ( 0 , 0 , 0 , 0 , 1 , 1 , 2 , 2 , 3 , 3 , 4 , 4 ), y = c ( 0 , 0 , 0 , 3 , 1 , 1 , 1 , 2 , 2 , 1 , 4 , 4 ), group = c ( rep ( 1 , 6 ), rep ( 2 , 4 ), rep ( 3 , 2 ))) # create a new object that \"melts\" the data so each row is a unique id-variable combination counts <- melt ( table ( data [ 1 : 2 ])) # add column names colnames ( counts ) <- c ( colnames ( data )[ 1 : 2 ], \"count\" ) # remove zeros counts <- subset ( counts , count != 0 ) # plot with the size of the dot being count sizeplot <- qplot ( x = x , y = y , size = count , data = counts ) + scale_size ( range = c ( 5 , 10 )) # view the size plot sizeplot # create some factors counts.and.groups <- merge ( counts , unique ( data )) # create a sizeplot with color determined by a factor sizeplot.colour <- qplot ( x = x , y = y , size = count , colour = factor ( group ), data = counts.and.groups ) + scale_size ( range = c ( 5 , 10 )) # view the plot sizeplot.colour dev.off () null device 1","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sizeplot.html"},{"title":"Sorting A Data Frame By A Single Column","loc":"http://chrisalbon.com/r-stats/sort-dataframe-by-columns.html","text":"Original source: http://onertipaday.blogspot.com/2007/08/sortingordering-dataframe-according.html # Create three vectors of data x = rnorm ( 20 ) y = sample ( rep ( 1 : 2 , each = 10 )) z = sample ( rep ( 1 : 4 , 5 )) # Create a dataframe containing those vectors data.df <- data.frame ( x , y , z ) # Sort the entire dataframe by column y, then by column z sort.by <- c ( \"y\" , \"z\" ) data.df.sorted = data.df [ do.call ( order , data.df [ sort.by ]), ] # View the sorted data frame print ( data.df.sorted ) x y z 2 0.6573883 1 1 7 0.7038443 1 1 5 -0.8647954 1 2 17 0.6415467 1 2 20 -2.0368409 1 2 16 -0.1966456 1 3 18 -0.1435698 1 3 19 1.7946029 1 3 9 0.4068842 1 4 11 -1.4022449 1 4 3 -0.3688571 2 1 4 -1.6158611 2 1 14 -0.4293812 2 1 1 -0.2070859 2 2 13 -0.8712713 2 2 6 0.4727199 2 3 10 -0.7866490 2 3 8 0.3843080 2 4 12 0.7966665 2 4 15 1.4994587 2 4","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sort-dataframe-by-columns.html"},{"title":"Sorting Columns Of A Data Frame","loc":"http://chrisalbon.com/r-stats/sort-dataframe-columns.html","text":"Original source: http://stackoverflow.com/questions/1296646/how-to-sort-a-dataframe-by-columns-in-r # create some simulated data df <- data.frame ( score = c ( runif ( 5 )), team = c ( letters [ 1 : 5 ]), state = c ( state.name [ 1 : 5 ])) # sort by decending score, THEN sort by ascending team letter df [ with ( df , order ( - score , team )), ] score team state 1 0.99573538 a Alabama 2 0.82771332 b Alaska 5 0.61670246 e California 3 0.32848427 c Arizona 4 0.04267817 d Arkansas","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sort-dataframe-columns.html"},{"title":"Sorting Data Frames","loc":"http://chrisalbon.com/r-stats/sorting-dataframes.html","text":"Original source: the r book # create a dataframe with simulated values x <- runif ( 1000 ) y <- runif ( 1000 ) z <- runif ( 1000 ) a <- runif ( 1000 ) data <- data.frame ( x , y , z , a ) rm ( x , y , z , a ) # decsending order by x sorted.by.x <- data [ order ( data $ x ),] # ascending order by x rev.sorted.by.x <- data [ rev ( order ( data $ x )),] # decsending order by x, then decsending order by y data [ order ( data $ x , data $ y ),] x y z a 303 5.295919e-05 0.1204359422 0.9703838271 0.7231496098 383 1.237667e-03 0.0232438489 0.1465805110 0.0851279420 619 1.673124e-03 0.1365318072 0.9513013442 0.9480992046 734 3.485756e-03 0.3896681438 0.7818035772 0.4832775032 729 4.047136e-03 0.6032040203 0.8416923857 0.7388109227 540 4.450090e-03 0.7839346875 0.8724803079 0.7039877982 526 4.811044e-03 0.1644457914 0.8606297760 0.2149697687 221 5.262941e-03 0.7496829620 0.5915243248 0.5111032834 976 5.915495e-03 0.1467177877 0.5859653547 0.7332504124 655 5.926213e-03 0.9971644639 0.9051821379 0.0360658306 148 6.219728e-03 0.6400448126 0.4648889787 0.8088550009 377 6.332923e-03 0.9780813723 0.9711977362 0.3919512555 408 7.536154e-03 0.9335733175 0.2217237891 0.3612280563 711 1.088239e-02 0.7630118555 0.0229528935 0.3443508230 911 1.095516e-02 0.2363499582 0.8551534784 0.6333606143 65 1.144816e-02 0.7084740028 0.4861036197 0.2580578546 363 1.150916e-02 0.1917956767 0.1217198134 0.9937900794 705 1.153814e-02 0.4445220514 0.9677767872 0.0999215310 46 1.212282e-02 0.1686070703 0.9703171740 0.4584224371 899 1.378918e-02 0.8179709974 0.5233321108 0.4256918016 735 1.403186e-02 0.3021712338 0.0021654337 0.6069038438 483 1.563843e-02 0.3273374741 0.7909335610 0.8833499607 209 1.657456e-02 0.1767632426 0.1946207290 0.8856602288 889 1.669147e-02 0.1116664631 0.2913762052 0.6909863462 290 1.768020e-02 0.7419006687 0.4205079689 0.2014388400 350 1.843499e-02 0.6882277138 0.5400942031 0.2174427991 634 2.099280e-02 0.5042717620 0.4111023666 0.9850776410 608 2.102597e-02 0.9125351012 0.3687413326 0.4323339632 369 2.159906e-02 0.3393238424 0.6898832079 0.8640246578 921 2.161493e-02 0.8665527280 0.4066659261 0.5577461172 517 2.177702e-02 0.2203728077 0.4019209784 0.1270760628 558 2.200838e-02 0.7305671314 0.2817107844 0.2291091264 305 2.347650e-02 0.9955582197 0.8692406646 0.9987308430 87 2.462679e-02 0.1568865965 0.4025761602 0.1186037876 999 2.734852e-02 0.6253932496 0.8977466887 0.2464844906 676 2.810440e-02 0.1526070507 0.6128168963 0.3262831955 934 2.846152e-02 0.4572117925 0.6621152451 0.6681876427 874 2.921400e-02 0.5304798745 0.6218344197 0.1185710018 50 2.956842e-02 0.3610363912 0.3614374173 0.1628155101 681 2.987074e-02 0.5666154118 0.4266912919 0.9947711267 553 3.057003e-02 0.5300741000 0.2638640178 0.5263646862 411 3.073603e-02 0.6707223190 0.7584439979 0.4931763541 599 3.145138e-02 0.7321615338 0.3246885270 0.5940708797 618 3.393911e-02 0.7505416535 0.0093650285 0.5140734110 397 3.520756e-02 0.1078986151 0.4296066554 0.4330902901 904 3.544645e-02 0.2732609522 0.1537657166 0.8951381890 851 3.588146e-02 0.8418821299 0.5244332757 0.4464718336 241 3.820585e-02 0.4993953491 0.8776033325 0.6530134045 224 3.878373e-02 0.3318573360 0.8290444997 0.5772491475 928 3.963831e-02 0.3233868040 0.4938660888 0.4603016216 456 4.126051e-02 0.2253331910 0.9907204665 0.7801127329 401 4.130090e-02 0.3177338517 0.1623618144 0.4811617702 142 4.312398e-02 0.6395165434 0.7354726135 0.4028844202 852 4.394213e-02 0.3481224021 0.4233243514 0.5320787481 21 4.434660e-02 0.2497578112 0.8797780857 0.9574402480 30 4.496744e-02 0.6161348606 0.0847757433 0.9894191732 967 4.504113e-02 0.6513337488 0.4220791035 0.1859614677 937 4.532184e-02 0.8382168321 0.8400939773 0.8855387792 995 4.584503e-02 0.0271474563 0.7668463141 0.2119980438 242 4.603780e-02 0.7619820423 0.2458257792 0.7378844658 821 4.674379e-02 0.0166342987 0.4126277715 0.6160769064 908 4.946176e-02 0.0091021021 0.4468885898 0.0090823087 143 5.100950e-02 0.7026735116 0.7360279341 0.8216344712 236 5.135953e-02 0.2848562498 0.7926417384 0.0981135592 728 5.192096e-02 0.5092771295 0.7860547192 0.7448664235 171 5.326051e-02 0.0437449550 0.7885682331 0.0253061599 785 5.344370e-02 0.5329161407 0.4281311787 0.3757226518 300 5.359152e-02 0.3323590995 0.3461532383 0.9071706980 680 5.437680e-02 0.4370128007 0.8494382517 0.9763537988 423 5.501376e-02 0.4076104611 0.0438082027 0.8262532642 481 6.013254e-02 0.1913458656 0.1564872453 0.4487843381 992 6.130392e-02 0.9707749269 0.7951784439 0.6119592739 987 6.140300e-02 0.8006444923 0.0416771774 0.7460391636 210 6.414659e-02 0.3537784393 0.0846279950 0.7698855968 796 6.420895e-02 0.4469339005 0.8818357901 0.8459305370 570 6.879368e-02 0.4973240555 0.4140109108 0.3700329748 132 7.066501e-02 0.1489249596 0.4417911761 0.8732577751 238 7.090596e-02 0.3736766148 0.6918589759 0.1156264695 325 7.166583e-02 0.8231499449 0.4514926588 0.9410166505 719 7.358591e-02 0.7362889817 0.8642575250 0.1459352195 99 7.501090e-02 0.1468499436 0.3422441678 0.2728359555 503 7.628972e-02 0.3158682638 0.5492015106 0.0130719901 119 7.804197e-02 0.0522002825 0.6306210069 0.2220336939 548 7.855756e-02 0.3452578599 0.2307801633 0.5593708898 248 8.033126e-02 0.1580555479 0.6182884742 0.8759877649 833 8.122244e-02 0.0198877931 0.6419535785 0.2249651374 622 8.396215e-02 0.2677419160 0.1691124088 0.6537323631 449 8.406411e-02 0.2419310336 0.1509686657 0.6919554539 387 8.514177e-02 0.6099002031 0.8857401924 0.6317475943 797 8.536099e-02 0.4467296456 0.4600141398 0.7379282799 818 8.542814e-02 0.1817985533 0.7599228204 0.5263032902 76 8.681552e-02 0.7230798278 0.8983805981 0.4566086177 102 8.696013e-02 0.1175417614 0.6400333359 0.3491354068 900 8.740703e-02 0.3365302091 0.1553983837 0.5604570936 331 8.936316e-02 0.4218797863 0.8597500215 0.4508296601 611 8.983309e-02 0.7597077242 0.9903257932 0.6333668409 617 9.025822e-02 0.2005643472 0.5054868960 0.3714372120 66 9.244001e-02 0.0573582314 0.0759581481 0.7967719724 352 9.449414e-02 0.7987474408 0.7056939036 0.7613940041 664 9.486022e-02 0.0499700524 0.1165187771 0.6856629150 783 9.549639e-02 0.2468090081 0.2759313905 0.0874742933 69 9.572212e-02 0.9290507932 0.9094156583 0.3639779282 777 9.581410e-02 0.2230371083 0.5727534154 0.5023611484 812 9.752244e-02 0.0245434877 0.7326447358 0.7140685990 272 9.819848e-02 0.2854620267 0.1995433618 0.9614824026 888 1.011627e-01 0.6620087090 0.6474910183 0.5868304179 43 1.032426e-01 0.0385671076 0.7062795826 0.2852701705 336 1.043461e-01 0.5878648369 0.2307583739 0.9414495456 306 1.051190e-01 0.1315009643 0.7646281160 0.1740205891 441 1.076016e-01 0.5211635227 0.4803114492 0.0771867586 712 1.078300e-01 0.6954053908 0.9156185745 0.4517467176 475 1.106256e-01 0.0411601234 0.5687046226 0.2774727587 645 1.118481e-01 0.3424647367 0.7254116433 0.1621024685 275 1.120379e-01 0.2945494929 0.5415715270 0.4471226027 843 1.145983e-01 0.9296154366 0.5516482291 0.4360027246 354 1.155032e-01 0.7562749099 0.3403204479 0.7598956733 10 1.166751e-01 0.5011702846 0.8261445113 0.9347630676 414 1.173640e-01 0.9080552256 0.2559056785 0.2705574429 278 1.207690e-01 0.9542516356 0.6175723448 0.9224836840 228 1.210666e-01 0.4951358859 0.9136697706 0.7717993308 68 1.216683e-01 0.8941288562 0.7396404808 0.3906601327 813 1.228572e-01 0.4330684659 0.6852584721 0.6598067970 282 1.234722e-01 0.3098714687 0.7643518972 0.2856825176 867 1.257734e-01 0.2556371638 0.9465384076 0.6422632975 188 1.258499e-01 0.5162700103 0.9820387554 0.5120323771 863 1.265929e-01 0.7324287251 0.4941041304 0.1332823653 288 1.279143e-01 0.8875839782 0.0309752289 0.5915752491 417 1.279827e-01 0.2104529741 0.8151354077 0.4709833325 536 1.281145e-01 0.7859399298 0.4695611615 0.8082950492 659 1.290129e-01 0.8069579978 0.3364724431 0.9712063069 403 1.295632e-01 0.2794160566 0.9860429587 0.3162912230 365 1.300516e-01 0.7150643694 0.4975708255 0.5660184689 506 1.302042e-01 0.6312998738 0.0620556350 0.5548376273 642 1.308067e-01 0.0013431723 0.1973925270 0.9894325910 884 1.320222e-01 0.1778903401 0.8586054551 0.1471254919 444 1.333019e-01 0.1334567671 0.5410031325 0.5287399625 203 1.366516e-01 0.9159404980 0.2461089417 0.6619110606 806 1.388367e-01 0.6205462012 0.0115005071 0.7853795716 531 1.411790e-01 0.9533379236 0.5312284003 0.6812794947 249 1.414287e-01 0.9678450976 0.7943479766 0.2274796267 271 1.438542e-01 0.4313102271 0.6462384327 0.5443700356 746 1.448906e-01 0.0626933277 0.0059710245 0.5867867141 641 1.492535e-01 0.4511115847 0.1407388942 0.9400621753 455 1.518463e-01 0.6019272611 0.4034022538 0.2756703270 963 1.538543e-01 0.2662833256 0.7370990687 0.9348711511 788 1.589920e-01 0.8915024998 0.8962593076 0.4332808317 767 1.595698e-01 0.1239718527 0.4242470439 0.4853637510 114 1.604085e-01 0.4695464338 0.3053941941 0.5406844700 379 1.608284e-01 0.1461109286 0.5038406698 0.9981220178 186 1.634714e-01 0.0005769860 0.4550667666 0.1180435738 591 1.656973e-01 0.5657906232 0.0600195713 0.1548888127 543 1.663672e-01 0.5086076288 0.2579812245 0.3770146084 260 1.672085e-01 0.9116117321 0.3212330469 0.9785724634 800 1.680122e-01 0.5968177791 0.2971039934 0.4990459483 862 1.717816e-01 0.7828774306 0.2580362521 0.0136169791 239 1.722342e-01 0.2828563754 0.8911008784 0.6632314883 982 1.730323e-01 0.6462155005 0.7242968616 0.1386128590 969 1.744939e-01 0.3371558865 0.5422904682 0.7790650672 957 1.748412e-01 0.3792455345 0.9891095238 0.1617391284 694 1.758694e-01 0.4571128420 0.9276080274 0.6196527567 662 1.774941e-01 0.9184210084 0.7908893817 0.3496594164 1000 1.780107e-01 0.2238740043 0.2594315715 0.2798480354 839 1.788648e-01 0.0248888575 0.5598794708 0.6028470178 895 1.789758e-01 0.3789934304 0.5050733918 0.3028235210 291 1.792941e-01 0.1387758548 0.3612693811 0.4474917704 24 1.823168e-01 0.0364284790 0.1686885243 0.4409426432 80 1.827815e-01 0.9236537693 0.3603928899 0.9025421415 358 1.828792e-01 0.9693342664 0.7683095105 0.0968836213 463 1.831089e-01 0.2057016394 0.5988530584 0.3969082676 360 1.837155e-01 0.6175905804 0.9713022753 0.0128712875 939 1.860403e-01 0.0311234654 0.4534639602 0.1092480838 445 1.874115e-01 0.8238960730 0.5237150232 0.5091205132 739 1.881659e-01 0.4525536264 0.5976812707 0.2378866794 717 1.893043e-01 0.0615848196 0.1475877156 0.1603101646 315 1.893885e-01 0.1579472586 0.1210900787 0.1170516189 106 1.908774e-01 0.6208431956 0.8718189425 0.6533969066 994 1.910219e-01 0.6594044792 0.9339261218 0.9604345404 972 1.925350e-01 0.4458122354 0.9810437113 0.6161918917 375 1.946765e-01 0.2886357633 0.3167450821 0.9686302582 274 1.957325e-01 0.4072826724 0.5339882637 0.4637067351 465 1.978833e-01 0.4415738271 0.6350358857 0.2214772967 101 1.979887e-01 0.8717487578 0.6464070878 0.5765860328 643 1.992732e-01 0.9866736601 0.2743548946 0.1406542747 621 1.995099e-01 0.7530929749 0.2137596754 0.8529831967 697 1.998103e-01 0.7199090919 0.5274791338 0.7371366573 985 1.999177e-01 0.5086374085 0.7700634766 0.7519063933 981 2.010562e-01 0.5071215848 0.5625947225 0.7985087971 193 2.013598e-01 0.7314066666 0.5295274598 0.5573889134 165 2.014583e-01 0.6479915930 0.1938577478 0.3527618959 7 2.026606e-01 0.5330454030 0.1366985671 0.4480833313 742 2.027100e-01 0.8902581416 0.0268363901 0.7276656167 15 2.027650e-01 0.8129677738 0.8414302471 0.9436975354 925 2.029543e-01 0.2063854493 0.4169447056 0.6261302039 498 2.029762e-01 0.2926641484 0.1579704657 0.1515534353 657 2.048246e-01 0.0055501577 0.8907399543 0.9881188199 698 2.063688e-01 0.0181274817 0.0203584495 0.1672040038 898 2.088939e-01 0.2599187759 0.0674623761 0.7187134419 701 2.109302e-01 0.5555387314 0.3904901173 0.2177885640 202 2.111497e-01 0.9238840048 0.1216083106 0.3398643585 924 2.131557e-01 0.0610013672 0.3303496081 0.3943242652 744 2.136491e-01 0.0309270986 0.6928482489 0.8113772077 620 2.136692e-01 0.6661918047 0.1486096624 0.9039954825 579 2.141058e-01 0.5372361627 0.8358786628 0.7886014427 233 2.149797e-01 0.4229304951 0.3623190704 0.9161259145 399 2.175812e-01 0.6904438531 0.6187522344 0.8051158963 388 2.177552e-01 0.0361186173 0.2843023047 0.6345029022 638 2.179667e-01 0.2538098600 0.7990020686 0.6231123060 312 2.180464e-01 0.1913908569 0.2779193851 0.4957792428 173 2.184310e-01 0.7937851991 0.7868569088 0.7476029955 26 2.186469e-01 0.5917677486 0.3645884150 0.0947834160 945 2.190688e-01 0.4025944280 0.4600291906 0.6182562776 875 2.193149e-01 0.4962440175 0.2276616327 0.5780980457 607 2.196941e-01 0.2526949986 0.6416545510 0.6057983767 343 2.204080e-01 0.0917197722 0.1279629515 0.5222965346 927 2.214427e-01 0.1400777020 0.9898258562 0.2826892468 310 2.219501e-01 0.7768511022 0.5983939569 0.9063221444 33 2.223597e-01 0.8073081044 0.6215855239 0.4891047792 177 2.228428e-01 0.3091596784 0.0920874318 0.9067007161 896 2.238178e-01 0.9708517455 0.8743774362 0.8223777907 656 2.247572e-01 0.9363373027 0.5742347545 0.9546632071 510 2.263591e-01 0.7349382823 0.0941556166 0.5521948873 501 2.283184e-01 0.3147070403 0.7214696566 0.0803757827 756 2.291862e-01 0.9092682106 0.2259766459 0.1836482836 675 2.293265e-01 0.5384221354 0.3561719162 0.4658662528 557 2.304261e-01 0.2861708128 0.9315623036 0.8454713183 550 2.312550e-01 0.4366058665 0.9041293175 0.8108497376 850 2.316470e-01 0.3041056052 0.5301092600 0.9312702133 584 2.330164e-01 0.9117321803 0.0637836913 0.9651588376 880 2.339240e-01 0.8412088368 0.6275388738 0.6942260445 856 2.381860e-01 0.4413369121 0.0241518363 0.3464159558 255 2.383492e-01 0.3771610740 0.7303420822 0.0591932142 121 2.387627e-01 0.7117998011 0.6902609586 0.4231519022 942 2.410779e-01 0.2666493570 0.4743217954 0.0439299403 633 2.417913e-01 0.6525685345 0.9745749561 0.0067962343 467 2.423623e-01 0.9791743557 0.9381054998 0.6042272812 328 2.456180e-01 0.6576498363 0.2612114684 0.4943328069 988 2.466015e-01 0.9552967723 0.1726024998 0.9876111625 696 2.491056e-01 0.8569217806 0.4782660627 0.4855837401 857 2.491425e-01 0.0840879711 0.0096960112 0.7593174637 512 2.495429e-01 0.3564567892 0.6379948375 0.9765872930 234 2.503620e-01 0.3396797148 0.3195888950 0.7739610290 658 2.512612e-01 0.1251344807 0.5596213653 0.7272206997 500 2.513379e-01 0.0270989009 0.0238732018 0.3856624139 901 2.513504e-01 0.2988421095 0.1741138715 0.7146249756 983 2.528734e-01 0.5861750741 0.1436152798 0.6569436865 446 2.534733e-01 0.7391413350 0.7779662379 0.6905429582 494 2.539427e-01 0.2995252719 0.3484721477 0.1353867266 583 2.583545e-01 0.2207459626 0.9248382833 0.0602291140 164 2.584014e-01 0.0236136573 0.4712793115 0.8847655088 316 2.588176e-01 0.6513790507 0.3226476428 0.8921571865 590 2.597419e-01 0.3658532950 0.7648373847 0.9899566153 915 2.605303e-01 0.3262630333 0.9950398481 0.3413769840 955 2.608551e-01 0.8257905641 0.2189523668 0.6814383599 42 2.613289e-01 0.8023632159 0.6683729908 0.3264660491 968 2.617710e-01 0.2141638633 0.3781841567 0.9485111316 623 2.646702e-01 0.6893601965 0.7923303987 0.6489792366 962 2.650643e-01 0.8319662963 0.8129436856 0.9467826670 273 2.674902e-01 0.7956350858 0.1650277153 0.4141623625 117 2.684855e-01 0.4880060980 0.1938910191 0.3032434415 3 2.689113e-01 0.6817796875 0.2025645131 0.6673764063 554 2.694922e-01 0.5032988894 0.9607492986 0.7041233450 9 2.700182e-01 0.1654136025 0.2592129668 0.1415726235 689 2.702501e-01 0.6951477660 0.1423352370 0.0369965397 280 2.711590e-01 0.7826896780 0.1994322466 0.1207430386 847 2.712344e-01 0.2103831540 0.3629580019 0.8785210687 427 2.723890e-01 0.9496534644 0.1130723094 0.0658651050 524 2.724934e-01 0.1380004643 0.6648305911 0.5742059234 459 2.728992e-01 0.1324613143 0.9238005758 0.8596341466 120 2.733902e-01 0.0350747313 0.8748522894 0.1792458508 136 2.749315e-01 0.3828209499 0.4178526944 0.4109224300 990 2.761953e-01 0.6740565717 0.8572522048 0.1669146356 647 2.778665e-01 0.4660173485 0.1780235462 0.8236296612 259 2.784070e-01 0.5177225142 0.2630384499 0.7234137692 98 2.793191e-01 0.0691160047 0.7084202142 0.4744455572 151 2.794605e-01 0.5627356775 0.4783362777 0.2116347915 268 2.796564e-01 0.7672925233 0.6137025519 0.7428965871 52 2.799644e-01 0.3415094828 0.5512544634 0.0819164398 502 2.800781e-01 0.4522858940 0.9015556676 0.3248593081 431 2.806318e-01 0.5740412320 0.8702439258 0.3799515744 294 2.807754e-01 0.1182135716 0.6964756758 0.2959666257 565 2.814861e-01 0.5812660407 0.5192293059 0.8872892754 111 2.830275e-01 0.2981022836 0.2228988798 0.3683490155 952 2.835676e-01 0.1504781223 0.0582537490 0.2192479763 284 2.837804e-01 0.7524148661 0.2944456129 0.1329575053 466 2.842098e-01 0.1561215129 0.8196740195 0.0544148032 457 2.843400e-01 0.7566384822 0.7862442804 0.0009349058 666 2.844273e-01 0.3573304731 0.7161301884 0.7209587605 751 2.858135e-01 0.6120078964 0.4486187976 0.6144669144 761 2.858994e-01 0.6637463428 0.9543646902 0.3264424368 359 2.859295e-01 0.0342221055 0.2418330978 0.7581248214 782 2.861964e-01 0.0033237042 0.5417127935 0.4286975132 170 2.866850e-01 0.1176809091 0.8013199931 0.1655886418 166 2.868852e-01 0.0632256984 0.8997844588 0.1836313230 319 2.869883e-01 0.1345000318 0.6548616730 0.0227149029 894 2.880165e-01 0.5947766032 0.3396490444 0.6936130512 834 2.882049e-01 0.9148325874 0.0903328496 0.3096023900 256 2.883366e-01 0.4510046795 0.8486938754 0.7759256391 794 2.893607e-01 0.3617334701 0.0563974015 0.3453795526 892 2.897477e-01 0.6351753976 0.7181651546 0.3467758563 394 2.916671e-01 0.5656022674 0.3972646561 0.8179015226 893 2.934437e-01 0.8556779572 0.9830090732 0.5358659069 436 2.942827e-01 0.8068769060 0.2685462448 0.7391511966 764 2.943887e-01 0.0655817401 0.2713962006 0.5815508862 437 2.955894e-01 0.7662792394 0.8518632979 0.6881160925 574 2.971127e-01 0.9356102394 0.4976671345 0.8047614705 684 2.974640e-01 0.6817139194 0.3341088290 0.8769729913 190 2.978522e-01 0.1267642195 0.6979268410 0.9678557776 996 2.980660e-01 0.0539759011 0.9502444109 0.9797827704 541 2.985953e-01 0.9503226259 0.9975623400 0.9327264170 313 2.988111e-01 0.2648781461 0.2402226157 0.7631869055 115 2.988760e-01 0.9847976025 0.7539409697 0.2109662613 448 3.006570e-01 0.0547480287 0.1284553071 0.0879944265 956 3.008615e-01 0.6503962763 0.1355075841 0.8585101864 885 3.010517e-01 0.3590400559 0.6874135509 0.8812075197 716 3.019946e-01 0.0782275973 0.3618925321 0.8645055159 754 3.023762e-01 0.1575430084 0.0882353077 0.6577142917 549 3.043433e-01 0.0875526899 0.0135405655 0.1212508436 986 3.066536e-01 0.5595940007 0.9896102003 0.3185064057 283 3.101489e-01 0.1697668107 0.6956339772 0.9936453230 923 3.101514e-01 0.4417759194 0.5099598754 0.7099299340 685 3.113954e-01 0.4917061003 0.5503319555 0.5804648763 606 3.115789e-01 0.3774928967 0.5472640498 0.0670971449 592 3.127321e-01 0.8891680643 0.5110030330 0.4248592067 935 3.132897e-01 0.5620147043 0.4924487935 0.4175463042 156 3.138050e-01 0.3704298672 0.5820745120 0.6472950992 266 3.139243e-01 0.3414523245 0.3717755564 0.2163331050 693 3.142934e-01 0.9643322856 0.2021481083 0.2693074879 75 3.144305e-01 0.7603062137 0.5526206549 0.2493617386 738 3.155769e-01 0.1578182187 0.0896912965 0.9328418863 490 3.155940e-01 0.4276885930 0.8752154112 0.3337080190 525 3.156666e-01 0.8920901320 0.4985764860 0.7609741420 60 3.159247e-01 0.1828007177 0.3845451586 0.1970368244 86 3.159526e-01 0.2339194675 0.9808777869 0.3075430396 400 3.160690e-01 0.6866364563 0.9450299474 0.2162255510 253 3.165101e-01 0.9750509465 0.3449736682 0.6157163396 4 3.168935e-01 0.1289482813 0.3680540337 0.0069863084 81 3.173228e-01 0.7614697267 0.0967565812 0.6083024545 110 3.177226e-01 0.6685290150 0.1602478381 0.1416200090 189 3.190845e-01 0.0865178760 0.0222487338 0.0243930793 345 3.194095e-01 0.0191048656 0.3938853347 0.3538078603 491 3.196845e-01 0.7594353694 0.8408504722 0.5568419127 721 3.210470e-01 0.0443950274 0.3319904255 0.3756310551 582 3.217285e-01 0.4225569335 0.6092540591 0.7920792059 760 3.228288e-01 0.2499688168 0.9382146292 0.7379004352 366 3.229890e-01 0.7557587998 0.8917617856 0.4897290345 333 3.230616e-01 0.2323329933 0.2151887033 0.9935401687 826 3.242562e-01 0.9397157899 0.9557924832 0.7452610887 671 3.262956e-01 0.3533510461 0.8524183887 0.2902357550 551 3.269440e-01 0.0621213182 0.0061430945 0.2427832116 216 3.281005e-01 0.4032923765 0.5867813202 0.6015733606 700 3.309146e-01 0.2742347750 0.3904041569 0.0969350499 476 3.309495e-01 0.7626585639 0.7651055895 0.7937437217 589 3.312670e-01 0.8665603686 0.7514831200 0.7257742030 787 3.335409e-01 0.9629657844 0.1752820567 0.1361216775 845 3.337531e-01 0.2981623183 0.8475914847 0.5134766432 20 3.350877e-01 0.9836185509 0.2147021878 0.7981811247 334 3.362305e-01 0.4953983803 0.6078860501 0.7429407949 338 3.366431e-01 0.1409076208 0.6463102829 0.1740811800 232 3.400993e-01 0.5360302844 0.3896446326 0.1240013919 588 3.407284e-01 0.8495226952 0.0980924380 0.9474678210 96 3.408660e-01 0.7941593472 0.5458247655 0.5827031103 113 3.411550e-01 0.0670679472 0.3412102235 0.2902924733 944 3.417939e-01 0.1754357824 0.7115404769 0.4750243512 629 3.486617e-01 0.5361834301 0.8267267959 0.4415455770 206 3.492444e-01 0.2692247597 0.3583266325 0.4069417617 44 3.493325e-01 0.5714378268 0.7651691355 0.3831500141 54 3.494585e-01 0.2017341352 0.3432335933 0.0715347170 916 3.528177e-01 0.6748576083 0.9136652532 0.7095495469 747 3.538152e-01 0.1564934745 0.4416535348 0.7692507729 604 3.555535e-01 0.6298408448 0.4087378173 0.9612347146 519 3.560000e-01 0.8579119744 0.2288686573 0.6427409262 649 3.567910e-01 0.8140204358 0.1353636431 0.2171551175 386 3.594522e-01 0.2983218455 0.4547014053 0.7453530210 150 3.602856e-01 0.3224467412 0.6767226227 0.5028195011 762 3.635169e-01 0.9653868624 0.0003763461 0.3888286690 11 3.637046e-01 0.0093511462 0.1945217524 0.5274245292 499 3.643202e-01 0.4791315969 0.6090655215 0.2774705440 443 3.644154e-01 0.4000456897 0.7633731270 0.6321202782 493 3.645367e-01 0.3841888499 0.0110153293 0.2028446491 640 3.652882e-01 0.2541274573 0.5952831144 0.6592480431 340 3.662340e-01 0.3957460236 0.2809392442 0.4369783781 439 3.662500e-01 0.0668757081 0.2550949759 0.3966372083 40 3.662604e-01 0.0780488949 0.1532232889 0.1179569368 971 3.667767e-01 0.2317019384 0.7624066358 0.4427907616 372 3.667913e-01 0.2186665782 0.2600061789 0.5753465905 886 3.682547e-01 0.7661897880 0.5432175023 0.7917252190 849 3.703655e-01 0.8214394832 0.7916978616 0.7280834785 521 3.722723e-01 0.3132955860 0.9091694686 0.2856255437 595 3.730210e-01 0.6310863388 0.7760690895 0.4835735874 230 3.732400e-01 0.0890899904 0.9739366702 0.5115259485 828 3.742592e-01 0.0009077138 0.9477327585 0.3613607199 946 3.748466e-01 0.6614591093 0.7973154869 0.2626820651 277 3.759099e-01 0.7195695755 0.7400143284 0.5216595796 83 3.760220e-01 0.7291084079 0.6013600377 0.9697975989 196 3.764791e-01 0.9196384007 0.0058217049 0.3188493741 39 3.796471e-01 0.4797577551 0.8851604685 0.9668949782 183 3.800887e-01 0.6989876360 0.2619769827 0.1293266530 289 3.886799e-01 0.3412695178 0.0992266219 0.8878536609 779 3.891112e-01 0.5175020259 0.2940462604 0.0190681890 569 3.912822e-01 0.0393550824 0.5136779724 0.8518081412 597 3.926405e-01 0.8101342660 0.4657459247 0.4706842455 690 3.930712e-01 0.0340386257 0.0073758152 0.6553283499 964 3.931794e-01 0.6792508557 0.5673605341 0.6990002212 109 3.933953e-01 0.5259206949 0.1880048099 0.8655526990 648 3.942681e-01 0.6255397480 0.0502692317 0.5051332905 429 3.943008e-01 0.4392142410 0.7090988033 0.5786362235 149 3.956770e-01 0.3869035072 0.1750396304 0.3548116244 250 3.961109e-01 0.7529697805 0.7670140958 0.5959447420 129 3.963484e-01 0.1899410400 0.7006253691 0.4782373917 840 3.979360e-01 0.6545833829 0.6137789898 0.1116776194 873 4.003405e-01 0.1553000868 0.9838622652 0.5681790903 672 4.026469e-01 0.4032308026 0.7239838943 0.0877558994 91 4.028961e-01 0.2969408997 0.0573948107 0.0024146789 285 4.037889e-01 0.2433368107 0.9619317059 0.8023218929 853 4.039784e-01 0.4317585346 0.2517932681 0.1699737783 791 4.044096e-01 0.2825561690 0.7803753542 0.7173345066 753 4.044811e-01 0.9536664686 0.4421028141 0.2394988935 759 4.071290e-01 0.7107417053 0.2296103151 0.8544814989 598 4.090881e-01 0.9301779394 0.1773739404 0.8778969522 161 4.091074e-01 0.6049205984 0.0716251356 0.1584313482 811 4.102487e-01 0.7261732596 0.2430004387 0.1245982163 613 4.105637e-01 0.4134536462 0.4968808827 0.6210870440 917 4.118704e-01 0.2346361170 0.2690016958 0.0873964189 141 4.122347e-01 0.3304774573 0.4096011529 0.2613089518 520 4.148270e-01 0.5580034582 0.6252005496 0.7695945411 949 4.153878e-01 0.9060234518 0.2455864772 0.9038911676 814 4.175446e-01 0.7302911074 0.1826635576 0.0132568697 644 4.178064e-01 0.6322884648 0.1916379323 0.2881191226 951 4.207364e-01 0.6039268502 0.7501884764 0.1827479955 528 4.207812e-01 0.9259474287 0.5362002405 0.9911777065 146 4.211543e-01 0.8482677685 0.2217121776 0.4523197205 771 4.219159e-01 0.7605483592 0.9675000831 0.9505931220 191 4.223846e-01 0.8663291726 0.1065640158 0.0904872217 709 4.233931e-01 0.3954044189 0.4427829327 0.8353298146 367 4.261078e-01 0.9475454211 0.9683079771 0.0723583428 138 4.264735e-01 0.7821625620 0.2959370154 0.4791424072 169 4.271705e-01 0.1686371607 0.7553156693 0.2828554821 546 4.288135e-01 0.7304959968 0.3482856299 0.4420045346 768 4.293443e-01 0.2248999288 0.8462481955 0.3454835617 669 4.296761e-01 0.8052896706 0.4805273113 0.6065118054 262 4.320771e-01 0.8421922845 0.3852153507 0.2024163236 450 4.323232e-01 0.0004161454 0.7555648405 0.3805760259 914 4.342587e-01 0.5060058234 0.8697000230 0.4985534735 185 4.344972e-01 0.2491190191 0.4679122355 0.8442822471 27 4.346728e-01 0.8095986107 0.6811416657 0.9320299763 733 4.369799e-01 0.6063367159 0.4779087156 0.9644276646 529 4.370052e-01 0.7076996695 0.5823667070 0.2019831638 405 4.375060e-01 0.1663671709 0.0087903524 0.3095371919 866 4.400458e-01 0.7540248565 0.3694977064 0.8481951307 691 4.403788e-01 0.1984859353 0.4638277832 0.5650265829 192 4.405165e-01 0.1507633906 0.3053007829 0.7173483265 464 4.415199e-01 0.9363758306 0.3606518186 0.4823854133 442 4.427332e-01 0.7830057349 0.4383156670 0.3221669891 803 4.438660e-01 0.3391791747 0.5233105924 0.0033185233 88 4.459251e-01 0.3835822507 0.5666138688 0.1729355871 530 4.471257e-01 0.0137002638 0.8329177729 0.9212474513 295 4.476236e-01 0.4189234674 0.6971193596 0.4108851014 562 4.491393e-01 0.2819814505 0.6768691775 0.9710939028 745 4.501383e-01 0.6256438470 0.2453030807 0.5365383206 485 4.502767e-01 0.7053245895 0.3941529440 0.8599377130 740 4.504050e-01 0.1145701916 0.5817311739 0.8257142610 868 4.519251e-01 0.3030057407 0.2607793324 0.3063354001 775 4.533857e-01 0.3931569366 0.1097577815 0.6389780079 667 4.535198e-01 0.2469755553 0.0697601824 0.6694170972 317 4.538439e-01 0.1870198494 0.9830209289 0.8755834920 67 4.553079e-01 0.4013069749 0.1939744235 0.1582010586 938 4.572541e-01 0.8984124032 0.5384138096 0.6409242873 508 4.573243e-01 0.1773362493 0.1213013793 0.6291040902 229 4.586074e-01 0.5642764717 0.9871571839 0.1747163085 29 4.601847e-01 0.1819547652 0.6222434612 0.0539266595 63 4.635580e-01 0.0285262880 0.3687569087 0.2202945766 72 4.650251e-01 0.0302960810 0.5288220616 0.3584464351 320 4.650439e-01 0.7202642376 0.5063074527 0.2171144397 74 4.651073e-01 0.1657024221 0.9141052384 0.1791352856 948 4.658221e-01 0.6732426637 0.5395739374 0.0787333404 932 4.661743e-01 0.1916043810 0.5647198176 0.4223980310 396 4.669129e-01 0.7372179236 0.7968291710 0.2855081328 353 4.669982e-01 0.6411473260 0.3903903821 0.9337025192 631 4.688944e-01 0.3208336472 0.2859721058 0.6725344569 918 4.695034e-01 0.6743761648 0.4976717441 0.0012760074 677 4.695781e-01 0.5483194280 0.7974392436 0.9909199590 382 4.696870e-01 0.8889308558 0.9302930927 0.7441132872 489 4.704885e-01 0.6188048956 0.8173991912 0.1838139251 108 4.721108e-01 0.4083683558 0.1175436792 0.2420347549 223 4.730680e-01 0.5138998064 0.8507489427 0.3308724998 977 4.731441e-01 0.5881272459 0.7945554904 0.0604843267 819 4.734250e-01 0.0400259895 0.8967154738 0.0640509934 172 4.739506e-01 0.4766024267 0.4179102115 0.7490206561 434 4.759870e-01 0.0978240753 0.9724293808 0.6608931595 5 4.762391e-01 0.1034168734 0.4192556720 0.1794489126 73 4.780305e-01 0.0465874434 0.2619803816 0.6993461216 301 4.783916e-01 0.5635657604 0.5183868553 0.9996923651 71 4.787995e-01 0.0084515733 0.5069133947 0.8624198041 704 4.790033e-01 0.4607126256 0.3197863360 0.5893032074 225 4.791864e-01 0.7666965022 0.6353233384 0.6769891798 364 4.805777e-01 0.2742147511 0.5059047227 0.1444690358 263 4.809292e-01 0.4772203683 0.2550993715 0.3494646624 824 4.821836e-01 0.2610955699 0.0950506078 0.1755832627 773 4.840489e-01 0.4966092631 0.3199732841 0.9864159699 798 4.843854e-01 0.9331568733 0.6911980431 0.3303487054 133 4.863337e-01 0.7690669273 0.4773845326 0.3841485532 769 4.880144e-01 0.5093817096 0.0634644835 0.9723804675 882 4.884136e-01 0.7248716683 0.7032604073 0.0199562805 947 4.884486e-01 0.4930816812 0.7577282034 0.4475314070 205 4.906736e-01 0.3380518835 0.9811351807 0.4611067567 965 4.909693e-01 0.7390329090 0.8118116148 0.7019733184 523 4.914405e-01 0.6356503167 0.4207023582 0.6377596026 699 4.921422e-01 0.8708772371 0.5397269204 0.5273332743 292 4.935304e-01 0.9691530780 0.4169866901 0.0072207735 140 4.936365e-01 0.2653927018 0.8553324416 0.7955691183 137 4.956760e-01 0.1472927041 0.7754632209 0.1865793609 682 4.977957e-01 0.2496171691 0.3363189118 0.2857060835 198 4.978943e-01 0.4920707236 0.4639232794 0.6112823270 167 4.986655e-01 0.8068375182 0.3088032915 0.8960690068 135 4.989324e-01 0.3708141653 0.3608130836 0.4629232523 997 5.002894e-01 0.1650525543 0.2372761485 0.0018677264 836 5.003753e-01 0.7801269181 0.0415326168 0.4940305427 430 5.006725e-01 0.7106350393 0.0740053330 0.9621260490 646 5.006758e-01 0.2791411271 0.2488279454 0.5446227698 578 5.006870e-01 0.9148136361 0.7931751828 0.8381882564 462 5.024043e-01 0.4307645243 0.5432159514 0.7086414685 533 5.032011e-01 0.0779025201 0.7590592373 0.7034967050 714 5.048437e-01 0.7654472247 0.5667729005 0.9162373685 652 5.084306e-01 0.0153296103 0.8712190611 0.8784312939 130 5.100867e-01 0.0563488628 0.2809457057 0.6229913395 679 5.108027e-01 0.3514100714 0.3634888192 0.3453678091 454 5.133334e-01 0.1151368145 0.9180651016 0.2442261106 432 5.148049e-01 0.3233085270 0.7928966195 0.3309121500 201 5.149678e-01 0.1825054169 0.1720529480 0.4618597026 535 5.150507e-01 0.5775622723 0.6317740486 0.3917948969 45 5.186799e-01 0.0888473983 0.3669696131 0.6495459820 155 5.186879e-01 0.5332671467 0.3154574579 0.8278436232 355 5.194742e-01 0.6076801538 0.3968638892 0.3328662412 538 5.211164e-01 0.5520075462 0.9875942017 0.0938058244 722 5.224377e-01 0.6170583295 0.5088759286 0.8349538334 2 5.226594e-01 0.1826495233 0.8694662431 0.0095718692 55 5.227636e-01 0.5178123338 0.1193500706 0.0622189986 998 5.238490e-01 0.4235605949 0.1992774578 0.5109421576 237 5.239648e-01 0.4641936517 0.3900389166 0.9301422192 293 5.239988e-01 0.7056358743 0.2732711216 0.5102481355 514 5.243487e-01 0.9854825821 0.8097478088 0.7968883319 213 5.257105e-01 0.8593153784 0.3018825171 0.9963832474 665 5.268743e-01 0.0289784560 0.2092607517 0.9642927512 838 5.275107e-01 0.9467506141 0.2047820706 0.6116776632 559 5.281428e-01 0.1072645127 0.1882397931 0.7359275089 226 5.309904e-01 0.1746446481 0.4492739371 0.2466454778 715 5.325052e-01 0.1694740269 0.8210349390 0.3642752392 296 5.332422e-01 0.4466755092 0.3746008908 0.9033433527 820 5.336865e-01 0.3511028290 0.0120223886 0.4918169440 265 5.350807e-01 0.2377800299 0.9368396550 0.0582549954 157 5.355276e-01 0.6073399324 0.6349588104 0.4092756237 630 5.355530e-01 0.8171469301 0.5372231677 0.6438705244 321 5.397518e-01 0.7319559420 0.3068302390 0.8570551157 532 5.399670e-01 0.1673903139 0.4468298147 0.3729196859 77 5.427661e-01 0.7655879820 0.3886864947 0.3612668263 974 5.468890e-01 0.8109393227 0.6629605037 0.7386564531 297 5.469885e-01 0.8115744037 0.6122245551 0.9261345048 743 5.481213e-01 0.8023592462 0.7938953133 0.5200486695 208 5.482485e-01 0.8387997122 0.8911637776 0.6355447355 307 5.489027e-01 0.8993230180 0.8406051113 0.9077746070 204 5.495466e-01 0.8289353470 0.8728481501 0.5948917805 552 5.495468e-01 0.4103866331 0.5365572681 0.2023304729 199 5.496846e-01 0.7685329986 0.7029353448 0.6109855373 23 5.497225e-01 0.5356195453 0.1431192623 0.3628535948 879 5.501700e-01 0.5929844058 0.3300296869 0.5296618214 628 5.505296e-01 0.7763286135 0.3971902232 0.3812324230 361 5.512431e-01 0.1930047141 0.0953012209 0.7941090397 984 5.517030e-01 0.5762303399 0.2190379323 0.0753969501 147 5.525021e-01 0.2966816640 0.2103164103 0.9708051158 219 5.528947e-01 0.6807376139 0.5660170286 0.5194084249 1 5.542866e-01 0.9833090580 0.8589808438 0.0157159127 428 5.543986e-01 0.1659928255 0.2676095066 0.9906809556 217 5.560184e-01 0.3548939449 0.8428651884 0.7841059498 872 5.569067e-01 0.0314917667 0.9564490872 0.2427610965 341 5.596648e-01 0.6030879167 0.9254308732 0.8280399290 175 5.597675e-01 0.7954983420 0.5375595591 0.4342074844 258 5.604668e-01 0.4749796728 0.8185092907 0.6010409456 909 5.610495e-01 0.6445470357 0.5641212391 0.6302746385 484 5.610878e-01 0.9478716364 0.3792359142 0.6983700991 668 5.623139e-01 0.7151841074 0.0737424435 0.4780599240 179 5.644988e-01 0.0109350814 0.0030116627 0.4998437753 447 5.648866e-01 0.5519524594 0.8688292652 0.7715093491 718 5.650405e-01 0.9727777108 0.6347254184 0.5924822493 421 5.654556e-01 0.3517147871 0.1026959904 0.7553463918 95 5.664068e-01 0.9770240395 0.6952666584 0.9034945078 145 5.665427e-01 0.8258572866 0.5055345343 0.1196879742 381 5.667965e-01 0.8992312804 0.5591189840 0.6193258914 929 5.670941e-01 0.8644218594 0.6451284850 0.4235822484 816 5.673945e-01 0.0215597192 0.1532121631 0.5192208635 940 5.689353e-01 0.4438785329 0.5475699864 0.1547614979 267 5.699315e-01 0.7172951994 0.1831561422 0.0951070802 62 5.702588e-01 0.8168040116 0.5645906257 0.1804041085 482 5.705705e-01 0.5905660293 0.5943362180 0.5524430575 269 5.708581e-01 0.1048002720 0.5863820205 0.2226214020 332 5.725086e-01 0.9022526566 0.1975541569 0.6585410880 227 5.741749e-01 0.5783516387 0.6092704381 0.6068926603 560 5.776039e-01 0.4261812209 0.6339307842 0.7583485390 392 5.779540e-01 0.3246255163 0.9579445778 0.4437921850 577 5.790032e-01 0.4601399673 0.2081353611 0.9761888480 18 5.790757e-01 0.7124169879 0.0284312423 0.9253566482 176 5.797191e-01 0.8930173339 0.5119264389 0.9423511117 378 5.801006e-01 0.7596922358 0.3274950781 0.0383493258 808 5.804269e-01 0.3254043635 0.5780410434 0.4205586875 472 5.805449e-01 0.8500073270 0.2968974225 0.9433794934 703 5.821274e-01 0.8242428193 0.2607765545 0.5187251258 804 5.824895e-01 0.9775081601 0.3756203915 0.9448428820 220 5.827965e-01 0.6622160675 0.3409700226 0.5136899052 674 5.844152e-01 0.9012928752 0.4932058989 0.1992344055 802 5.847916e-01 0.4962058694 0.6559390561 0.1971519273 975 5.875734e-01 0.1744224669 0.1446207180 0.3893178073 471 5.882169e-01 0.8407586867 0.4566829388 0.1746196423 626 5.884598e-01 0.2495322810 0.1898027300 0.1437906411 573 5.890615e-01 0.9290959369 0.0932475212 0.0011155535 515 5.917411e-01 0.0896195436 0.7786509120 0.1384014187 555 5.922108e-01 0.5756105019 0.4527943926 0.3215020304 600 5.938688e-01 0.8837461311 0.0997908262 0.3523243468 887 5.944289e-01 0.3385188535 0.8641092435 0.4966968223 663 5.945738e-01 0.0673871159 0.6081863896 0.2451662684 261 5.948316e-01 0.9580841714 0.1551441839 0.7225269713 637 5.955119e-01 0.5112449478 0.0449329976 0.8320567631 827 5.960126e-01 0.2909187234 0.5557609987 0.6351597595 380 5.970380e-01 0.9050194698 0.0380458510 0.9805767045 763 5.979052e-01 0.8776711507 0.5056774986 0.9920564091 906 5.983012e-01 0.3646071889 0.8046670647 0.4465681915 287 5.998383e-01 0.0898992212 0.5624640668 0.1336625160 829 6.001386e-01 0.1994957593 0.3011597476 0.5128888749 991 6.005062e-01 0.3545909601 0.4463499798 0.9508305802 695 6.010344e-01 0.1515621413 0.9137576313 0.9323660510 878 6.015904e-01 0.6397410096 0.6262564780 0.1878085448 323 6.059980e-01 0.7975402572 0.0461637913 0.6941953462 708 6.074231e-01 0.4300395646 0.1013319409 0.2485980364 281 6.074537e-01 0.1384574447 0.4237453099 0.9538388741 941 6.082841e-01 0.5179220503 0.4366972616 0.1073550000 539 6.086619e-01 0.8015716088 0.5545537500 0.8160566613 34 6.102179e-01 0.6922273599 0.7268001828 0.6294319520 318 6.102253e-01 0.0270813990 0.7819606476 0.2634185415 184 6.106203e-01 0.7028834086 0.0949540196 0.1876146460 748 6.110736e-01 0.2925594219 0.8575309059 0.9624532629 683 6.124337e-01 0.1823640370 0.4286859401 0.9675243769 710 6.124987e-01 0.9773474543 0.6809241176 0.9424651945 90 6.145243e-01 0.0454663357 0.6127055106 0.5924441975 776 6.145765e-01 0.9861318956 0.3633094516 0.0776284079 781 6.151358e-01 0.1691890894 0.0384272090 0.8280922652 100 6.158758e-01 0.7934230429 0.4896278144 0.1051221006 605 6.161063e-01 0.0486523500 0.5379591945 0.1235904654 980 6.162017e-01 0.8834291191 0.0125059057 0.3341082947 563 6.177816e-01 0.0727478310 0.9437108212 0.3703289269 480 6.203937e-01 0.6508659315 0.9274903834 0.9826185585 473 6.221428e-01 0.7782653277 0.2810882044 0.5730074237 402 6.224907e-01 0.6708390557 0.3534556485 0.2838573195 107 6.245696e-01 0.4434177419 0.1047650264 0.9712870258 245 6.248213e-01 0.1161872118 0.5404503189 0.7719587157 344 6.258862e-01 0.0063159079 0.9926501208 0.6391590168 913 6.260484e-01 0.2336100459 0.8212828804 0.3136810688 390 6.279206e-01 0.2056652878 0.5800072514 0.5147102110 47 6.302785e-01 0.5742996410 0.9122637485 0.5437478207 53 6.310673e-01 0.7907909718 0.1448096228 0.9332323403 326 6.313157e-01 0.0078068636 0.6825874217 0.8758656923 795 6.320600e-01 0.6154590652 0.4525998365 0.9737815398 688 6.321598e-01 0.1687134092 0.0881426930 0.8106041162 160 6.336830e-01 0.3794772443 0.0107185161 0.8522733778 393 6.339686e-01 0.8391730383 0.6384438023 0.4556299311 322 6.339705e-01 0.6850642005 0.0808434279 0.6037107499 85 6.356154e-01 0.2264867486 0.5228893799 0.7221274942 391 6.359366e-01 0.5966216261 0.4904223161 0.0808905235 314 6.378027e-01 0.3898928552 0.7838839719 0.1829339464 907 6.391823e-01 0.7327565097 0.8388058420 0.3848343294 835 6.419726e-01 0.7800813962 0.9747970519 0.3191948775 692 6.430654e-01 0.8821428614 0.6944420810 0.9729342363 64 6.444099e-01 0.3818384905 0.9151379757 0.3088128457 13 6.445862e-01 0.1154009230 0.2839285629 0.0002715064 181 6.456769e-01 0.5767811334 0.7915839648 0.1608777137 37 6.459876e-01 0.8222660136 0.3156057054 0.9022616434 823 6.464126e-01 0.5134505273 0.8208169939 0.9535903002 374 6.465586e-01 0.1744353804 0.2475209348 0.7527433608 139 6.477567e-01 0.0956963170 0.8580414627 0.6477495497 609 6.494552e-01 0.8862300755 0.8712093509 0.6916173631 723 6.502844e-01 0.4315613196 0.4735236492 0.7862799533 247 6.510836e-01 0.0872595175 0.7645559502 0.8251580466 844 6.519797e-01 0.9021456987 0.2183597228 0.6531453640 571 6.528210e-01 0.9972224352 0.6844558837 0.2140201954 732 6.535183e-01 0.3081739037 0.9127263639 0.2586724053 556 6.538469e-01 0.1630859701 0.9089286162 0.7053650699 765 6.547115e-01 0.0233824153 0.3816484259 0.4316668289 469 6.556245e-01 0.6706818948 0.8966420833 0.7620980176 335 6.557127e-01 0.7927363685 0.3199828686 0.2163206751 822 6.557848e-01 0.9080478670 0.1204762200 0.4729468299 809 6.564585e-01 0.7559439263 0.8147470111 0.1427444958 825 6.567907e-01 0.7116398024 0.5421752834 0.8680316901 194 6.569118e-01 0.7173788552 0.6291707095 0.7881984871 586 6.591504e-01 0.0077304626 0.6770046712 0.2702874134 496 6.609217e-01 0.1974545284 0.7682775576 0.1748170524 458 6.630477e-01 0.7623138290 0.0890833877 0.4926825124 805 6.633159e-01 0.4312630482 0.4758453132 0.4624184526 731 6.643458e-01 0.1773164074 0.5179379119 0.9820576487 860 6.656081e-01 0.4993401438 0.8812128971 0.7276018350 774 6.678106e-01 0.0646402934 0.8558945011 0.2128454850 859 6.714197e-01 0.7141741076 0.1788221581 0.0958971593 409 6.726362e-01 0.5295443756 0.9816420863 0.7146684269 308 6.738851e-01 0.9492473917 0.4146443415 0.6157893066 104 6.747605e-01 0.8126003002 0.2998100929 0.7717536669 207 6.752173e-01 0.5801718119 0.0964090971 0.1979108637 832 6.761062e-01 0.9183911558 0.9810213328 0.8547232042 784 6.769232e-01 0.2607267876 0.9360645076 0.5417719609 772 6.787551e-01 0.2805139625 0.7724622968 0.1576338729 349 6.791238e-01 0.3104267984 0.2233442490 0.7440472860 919 6.804767e-01 0.2408414043 0.6128645355 0.2502806301 706 6.814958e-01 0.4058359400 0.1593671215 0.1861450493 451 6.833079e-01 0.6731802390 0.9112128618 0.9122834024 575 6.833539e-01 0.4539971964 0.2953388444 0.5672606905 603 6.834566e-01 0.6399375782 0.4811843003 0.7722016738 842 6.842934e-01 0.7390040220 0.1635588121 0.6318896175 799 6.843233e-01 0.3660207286 0.0100686369 0.7695688361 518 6.864434e-01 0.4025617607 0.4958153160 0.9100400563 970 6.868917e-01 0.7509573670 0.4734841746 0.6462855572 513 6.901460e-01 0.2605962262 0.3788985009 0.1039564060 357 6.909836e-01 0.0891775682 0.7726180397 0.4490502821 778 6.931612e-01 0.0387141611 0.4236126307 0.7484089786 82 6.935891e-01 0.1667763048 0.4912245253 0.3834771516 758 6.946957e-01 0.8895073575 0.0422565234 0.2190806738 933 6.963413e-01 0.7118422112 0.6081633405 0.6650809911 537 6.973123e-01 0.8621987300 0.7399348009 0.6827726732 749 6.996185e-01 0.5373622235 0.1572593502 0.6782780150 61 7.002564e-01 0.3106824572 0.3575492965 0.4963993572 870 7.003344e-01 0.1261285394 0.6034269524 0.0230121380 755 7.003441e-01 0.8224407574 0.3071978474 0.2799181899 124 7.010736e-01 0.5269747330 0.4280239788 0.7415538060 612 7.024002e-01 0.6842829841 0.9601071202 0.4983925372 298 7.032234e-01 0.1276660901 0.3512601887 0.0340866135 792 7.045130e-01 0.9620934983 0.3627147092 0.3160904276 727 7.071582e-01 0.4966531876 0.9366630393 0.6717651952 547 7.081940e-01 0.0510850132 0.3012347003 0.5276582679 702 7.086468e-01 0.7220031957 0.7641753643 0.7384274995 905 7.087823e-01 0.3400920785 0.2834420544 0.3274093030 474 7.096497e-01 0.4943163039 0.1706792866 0.3797209072 576 7.097229e-01 0.6697358959 0.2472816587 0.6981553293 926 7.112833e-01 0.9070661250 0.5491387870 0.8238516334 973 7.116033e-01 0.8405571890 0.6656020186 0.4780555787 615 7.129391e-01 0.6689185891 0.3939270917 0.4349426499 178 7.160510e-01 0.2616221772 0.4098853387 0.7168865129 581 7.175368e-01 0.4753944150 0.3291584225 0.8620491431 920 7.178089e-01 0.2651346168 0.1737916954 0.1877180710 162 7.219701e-01 0.1288308096 0.6086993811 0.7223363388 903 7.242380e-01 0.4630311569 0.4770370047 0.7296083081 954 7.242465e-01 0.9420146141 0.8184880079 0.2829458143 883 7.244679e-01 0.8044338322 0.7604485743 0.0753104722 419 7.266515e-01 0.9409035060 0.2170105728 0.3179372472 51 7.291256e-01 0.8002877214 0.5221853927 0.8232948999 28 7.296542e-01 0.9085342109 0.0169874108 0.5129944789 724 7.304406e-01 0.4494392695 0.6851461551 0.0859443585 594 7.314602e-01 0.9582618556 0.6439179936 0.9691425657 725 7.324893e-01 0.2906428277 0.2462122766 0.2094656583 837 7.331687e-01 0.1324401530 0.8695874859 0.1682651800 78 7.332325e-01 0.7377662773 0.6570759525 0.6451019046 561 7.333646e-01 0.7181445048 0.1770002791 0.6793971565 134 7.342049e-01 0.3411279677 0.9938213530 0.9083984448 118 7.368277e-01 0.8821639093 0.2311837184 0.2105791783 766 7.388421e-01 0.1265674550 0.0428342707 0.6239855161 660 7.388873e-01 0.6476803708 0.9011265782 0.2309618858 31 7.393332e-01 0.3941297820 0.3040120727 0.8108921472 752 7.411374e-01 0.6492504450 0.5360253584 0.0709396717 407 7.418615e-01 0.8296758418 0.2032252662 0.7851773123 385 7.435141e-01 0.9343839337 0.9612863630 0.7469045895 831 7.445810e-01 0.2951165976 0.2544970426 0.9341622652 989 7.449770e-01 0.3120122191 0.1888474873 0.2196941066 112 7.460919e-01 0.4131432136 0.0488572696 0.2974678637 424 7.465512e-01 0.3439005830 0.5799986548 0.4277474340 461 7.471792e-01 0.4156895136 0.6353187237 0.0445567833 790 7.474378e-01 0.4718927590 0.8287794755 0.1290362533 593 7.485756e-01 0.9330786762 0.0195993925 0.9093055658 368 7.486812e-01 0.0563732551 0.2517378815 0.5637440917 468 7.489578e-01 0.5730690260 0.3280425065 0.1639909553 950 7.493291e-01 0.9782243683 0.5312319873 0.1733841859 197 7.516517e-01 0.7020278042 0.5752629845 0.2831113327 251 7.519108e-01 0.9836574036 0.9666712370 0.2664065447 70 7.524610e-01 0.8378064453 0.2610905915 0.7354227910 910 7.526846e-01 0.7237578677 0.8616248486 0.8663589992 330 7.547180e-01 0.9298742216 0.6937701656 0.8757355644 105 7.559680e-01 0.7755302179 0.5322773447 0.1407593717 243 7.560209e-01 0.7183827434 0.7360222628 0.7267478076 897 7.567138e-01 0.2893224645 0.1563613492 0.2136659799 222 7.571122e-01 0.5814699088 0.2435559430 0.6669155709 846 7.577822e-01 0.9128110702 0.1423033345 0.8426297605 687 7.589842e-01 0.9180029992 0.7355369255 0.0014862404 14 7.590962e-01 0.2049082045 0.0261619014 0.1985577063 41 7.604080e-01 0.2337068685 0.0242175811 0.6538959157 440 7.605166e-01 0.5556561854 0.6919907336 0.8918771115 959 7.615009e-01 0.4284613947 0.0652694525 0.8948239263 25 7.630428e-01 0.2540152548 0.1112241079 0.8569147480 311 7.655057e-01 0.4413962513 0.2651017730 0.7119948044 815 7.656616e-01 0.3726096097 0.5964104799 0.6720197324 435 7.657858e-01 0.8701281233 0.9471498795 0.2934360288 211 7.658911e-01 0.3615081077 0.7200392080 0.4921354908 174 7.672040e-01 0.5039923128 0.7442860540 0.6963942447 257 7.683173e-01 0.3927386599 0.7199692731 0.1406834126 384 7.691808e-01 0.2326473442 0.5500388185 0.5600185785 347 7.699130e-01 0.8895159913 0.5809026656 0.2160334345 507 7.700584e-01 0.1101190043 0.8286216890 0.1625572555 564 7.701469e-01 0.6632656991 0.6394137386 0.2954028551 477 7.711463e-01 0.6787456679 0.2079299872 0.4506866627 212 7.711707e-01 0.5922194030 0.2058695713 0.8755839665 651 7.719844e-01 0.7269901678 0.6065133384 0.9390950403 159 7.724596e-01 0.3009231358 0.2654541722 0.2384871654 153 7.737392e-01 0.5822822566 0.0916815251 0.1662947452 348 7.737587e-01 0.4533255412 0.8876228398 0.5565644915 49 7.760507e-01 0.0711592236 0.9366041552 0.0630050227 286 7.774148e-01 0.0709939937 0.0919741148 0.7579669629 877 7.774868e-01 0.0996623957 0.8454964852 0.4827407731 324 7.799013e-01 0.2265801702 0.2334021130 0.4062399340 406 7.815694e-01 0.1994928129 0.0131786685 0.1347073053 422 7.820526e-01 0.8257539968 0.0654338982 0.6394523354 195 7.829030e-01 0.0367938112 0.1963058307 0.9123884584 376 7.836784e-01 0.9915810802 0.1602107240 0.1569827895 585 7.838838e-01 0.3960373956 0.8142921377 0.4860298010 309 7.844560e-01 0.4935832117 0.9721041515 0.2956447061 244 7.845776e-01 0.6179245175 0.6562649868 0.8313285091 487 7.859899e-01 0.8644494871 0.6431337448 0.8378899603 841 7.884362e-01 0.2693220291 0.8267543942 0.4432307591 780 7.907022e-01 0.0941319310 0.8671918460 0.0164902185 279 7.915233e-01 0.7240768741 0.2201960841 0.6099090038 902 7.919423e-01 0.0210037972 0.0681268934 0.6757276298 32 7.931890e-01 0.4906755164 0.1459774249 0.0573002277 200 7.940600e-01 0.7302230257 0.5891999314 0.8560034230 433 7.973521e-01 0.4766952733 0.7655220605 0.5636546768 425 7.986406e-01 0.5800062795 0.9824033568 0.2933521927 492 8.032926e-01 0.2696289890 0.7234498500 0.4449163976 504 8.057210e-01 0.8116289293 0.8859689974 0.0868719965 930 8.063987e-01 0.4496320696 0.9459621930 0.8475502222 89 8.071271e-01 0.3453023648 0.9808681344 0.9615261129 304 8.073902e-01 0.4916327423 0.5162256425 0.4499665268 337 8.075701e-01 0.0032390470 0.0324333485 0.5020182340 653 8.082186e-01 0.8218527972 0.9888564991 0.1820764721 218 8.094471e-01 0.2288034735 0.4047433098 0.8907583426 810 8.097651e-01 0.9974015751 0.0801545810 0.5794915496 235 8.102196e-01 0.9311630665 0.1092158533 0.3799604913 568 8.105447e-01 0.0591207538 0.5399352699 0.5844651673 122 8.107736e-01 0.9024968164 0.5797378900 0.0226751026 516 8.116259e-01 0.8475515670 0.1807801232 0.8790299611 654 8.120373e-01 0.8115541227 0.0624054326 0.1199278336 479 8.128112e-01 0.9136537146 0.8958826554 0.5166771670 726 8.129765e-01 0.8169613513 0.8134864552 0.2576767718 993 8.133766e-01 0.4866270225 0.7504620280 0.5116696516 17 8.135744e-01 0.0097722874 0.1678766063 0.6016244597 452 8.156984e-01 0.0972256372 0.2611511138 0.9410424922 544 8.177219e-01 0.5364770035 0.2852610673 0.2652223944 398 8.183166e-01 0.7705236273 0.8831598663 0.6738218018 602 8.185494e-01 0.4822108911 0.2506417148 0.3788252161 966 8.247321e-01 0.7534746469 0.6620846100 0.1691668655 246 8.249923e-01 0.6169260067 0.3588936338 0.3896467038 861 8.270498e-01 0.5712378072 0.3421249390 0.1330011967 636 8.285125e-01 0.5253571735 0.2355318307 0.7679767178 678 8.298097e-01 0.2541926729 0.8874790054 0.1876928294 488 8.299994e-01 0.4741573678 0.1832184405 0.8162291159 103 8.351063e-01 0.1366254615 0.0916007031 0.4508070713 770 8.351672e-01 0.2085835058 0.4562757211 0.4928286786 807 8.362216e-01 0.2655481843 0.3892830824 0.6689318160 639 8.379324e-01 0.1658554061 0.1693391551 0.2898453081 497 8.413542e-01 0.1672935609 0.2538938900 0.2144021480 817 8.416803e-01 0.5118132285 0.8940587379 0.1426475565 346 8.425048e-01 0.5102354251 0.5498691662 0.3060663191 509 8.444156e-01 0.8654404006 0.9758608311 0.8447875571 123 8.452073e-01 0.9409901495 0.5009641680 0.1685873135 453 8.473971e-01 0.1879401333 0.0280796126 0.0484661902 505 8.494752e-01 0.3776379165 0.6265328680 0.0341225287 327 8.509638e-01 0.5261285228 0.5340994452 0.6199407806 572 8.521219e-01 0.0180913578 0.3564948512 0.3481953521 48 8.527561e-01 0.6810861547 0.0402703986 0.1751700849 601 8.533542e-01 0.0478189168 0.8034572580 0.6164900255 871 8.554139e-01 0.1972295512 0.7005497531 0.0770570743 686 8.585011e-01 0.8547907402 0.2620523099 0.5892018785 737 8.588443e-01 0.9415523992 0.5739862656 0.4065142951 389 8.602515e-01 0.6154405328 0.8474637247 0.1897592787 413 8.602877e-01 0.9649217611 0.4278149011 0.6996458429 16 8.603934e-01 0.6499685571 0.0334368318 0.2284856928 57 8.604390e-01 0.9255892264 0.0945856438 0.0556332492 187 8.632969e-01 0.0554656587 0.4497285939 0.7110322975 670 8.634045e-01 0.4504141961 0.0596322319 0.6350105603 329 8.654683e-01 0.3775968975 0.4527079093 0.8482642409 460 8.673849e-01 0.9115750396 0.2022457095 0.7877140816 131 8.718964e-01 0.3022752106 0.8906823441 0.4865382037 580 8.719081e-01 0.2934395284 0.1269136174 0.1446142751 931 8.798525e-01 0.5901282073 0.5315822545 0.5918912594 373 8.852438e-01 0.8638734382 0.3335147051 0.2591761104 632 8.873558e-01 0.5421606125 0.5577641414 0.2906716580 624 8.894493e-01 0.2068498603 0.9244352188 0.7331217912 979 8.908999e-01 0.4830600517 0.1168714568 0.4697852931 635 8.915720e-01 0.4979555777 0.9321299151 0.2257209374 786 8.925482e-01 0.2998972184 0.0247932493 0.6654331372 854 8.937822e-01 0.9471857212 0.1472545639 0.6740317815 182 8.954368e-01 0.3528975544 0.5985792372 0.2827785718 858 8.987779e-01 0.2401946220 0.1981164475 0.1134824855 94 8.993633e-01 0.5273659474 0.0290648802 0.4695062756 395 9.001396e-01 0.3231995006 0.3213722091 0.8301668824 830 9.008943e-01 0.2165649768 0.3863033375 0.7041533920 438 9.011308e-01 0.5051807642 0.4292937180 0.2583896192 936 9.022868e-01 0.4661556680 0.7181719975 0.2186214246 127 9.029183e-01 0.2481017341 0.2927445266 0.1579304296 302 9.034141e-01 0.1343819159 0.0731258711 0.7925460532 19 9.043985e-01 0.7074642747 0.6381588187 0.2703690410 567 9.054918e-01 0.7612596711 0.5404912441 0.2484524762 58 9.066961e-01 0.8559975526 0.3260994980 0.8240574540 801 9.086291e-01 0.5013623438 0.8082443818 0.9393593415 416 9.094970e-01 0.0105680444 0.5633843390 0.6027238476 673 9.122854e-01 0.8019275083 0.4476978888 0.8779064436 960 9.123199e-01 0.3938624107 0.5286954148 0.4011641196 180 9.131844e-01 0.1186505777 0.4178859165 0.0958404092 614 9.146212e-01 0.5416983927 0.8122150493 0.2591377443 264 9.151860e-01 0.1030903771 0.9152455269 0.6830093600 912 9.156627e-01 0.3239184842 0.2046534528 0.5718477706 128 9.157535e-01 0.1482616905 0.7406861333 0.1696961420 864 9.180876e-01 0.8116302644 0.1234603578 0.2929377335 252 9.202459e-01 0.7754298418 0.9740118007 0.6950658283 542 9.203973e-01 0.3947809346 0.4643013154 0.9594274510 163 9.208058e-01 0.8819207975 0.5441987668 0.0196973770 371 9.216068e-01 0.7891150382 0.8372787703 0.0929246983 869 9.219011e-01 0.7919305307 0.0561114762 0.3290940716 720 9.220503e-01 0.6996939583 0.5771783197 0.1536133504 93 9.238368e-01 0.8139522299 0.5625555650 0.0188053132 736 9.241094e-01 0.6224616412 0.6907229791 0.9551087250 596 9.257952e-01 0.3851618001 0.5466658408 0.9998504703 356 9.267076e-01 0.8627272977 0.7053277765 0.0375258685 881 9.285382e-01 0.5191491793 0.1660548819 0.9338567317 214 9.286826e-01 0.9889966541 0.5269069371 0.9331521899 79 9.294050e-01 0.2867120111 0.4003361980 0.5434607770 22 9.302678e-01 0.0267390190 0.2984235329 0.6759171598 84 9.304352e-01 0.9687618057 0.1957632760 0.3184175068 339 9.309300e-01 0.9500578986 0.5204108951 0.9770452881 154 9.316496e-01 0.5062587992 0.9050525215 0.4264295292 511 9.318651e-01 0.0316710474 0.5556789797 0.5542635238 8 9.330266e-01 0.5996393552 0.1144119080 0.5123339933 59 9.341763e-01 0.3880641835 0.4178205393 0.6832093613 144 9.347553e-01 0.8251091221 0.4343012879 0.7604828055 625 9.375833e-01 0.3778728812 0.4766846879 0.5857761013 495 9.380244e-01 0.8910623183 0.5428369888 0.1924875798 116 9.401800e-01 0.6989513070 0.1898130747 0.0256870585 362 9.407680e-01 0.8272972556 0.5830404791 0.3058422999 125 9.418647e-01 0.2285985686 0.4386081048 0.6437031622 342 9.429780e-01 0.4240404828 0.9256313515 0.9959103644 215 9.435852e-01 0.6709398830 0.2594770531 0.4301793771 958 9.436314e-01 0.0022947276 0.5996681515 0.5466331611 750 9.440340e-01 0.6923751987 0.1462924418 0.7102696304 707 9.442515e-01 0.5301736786 0.0217455663 0.6137777702 6 9.465808e-01 0.1321919002 0.9642079063 0.0106674032 890 9.471895e-01 0.7377531750 0.7245785198 0.9717308118 566 9.474061e-01 0.4575275097 0.7229013266 0.8053930239 36 9.477427e-01 0.1271537659 0.8370112474 0.5033601515 627 9.478164e-01 0.0064144006 0.7303986286 0.1139918147 470 9.493792e-01 0.3616794958 0.6526809835 0.4169320252 757 9.497536e-01 0.9967181843 0.7824474410 0.8584247737 534 9.501419e-01 0.6390762145 0.5191639541 0.0375868985 92 9.506781e-01 0.8422753196 0.2326187629 0.3374200875 56 9.512035e-01 0.9973773425 0.3881836624 0.1071123448 522 9.519232e-01 0.2566480397 0.2819686020 0.1216525591 855 9.549711e-01 0.1809952792 0.3846041984 0.8197695767 478 9.573579e-01 0.8896628858 0.5876979744 0.8644074905 299 9.578075e-01 0.3640551963 0.3195550458 0.4844390657 978 9.588078e-01 0.0026292088 0.6364456320 0.2139793041 943 9.590627e-01 0.1992323883 0.1011316613 0.4234703849 410 9.600066e-01 0.1502293691 0.2545138474 0.5607979293 865 9.621612e-01 0.7022034277 0.1808796821 0.4974558749 415 9.636859e-01 0.4880572709 0.9564860468 0.4172343994 610 9.656254e-01 0.3322822081 0.5395499817 0.1961248489 412 9.663225e-01 0.8784005581 0.1177603554 0.1781791658 404 9.676273e-01 0.1463643739 0.6859388584 0.8283225275 486 9.700480e-01 0.7213446472 0.7836504765 0.9138441072 158 9.703417e-01 0.7202781925 0.4724573833 0.5607556552 730 9.704365e-01 0.1728176344 0.1903355829 0.0986674940 741 9.712680e-01 0.0532556248 0.7394646904 0.1547837462 789 9.716603e-01 0.0978035424 0.8540011973 0.2401124882 351 9.748567e-01 0.8269599262 0.4585275194 0.0160149978 12 9.761003e-01 0.2921082692 0.7710617441 0.2266206592 370 9.767044e-01 0.6709633351 0.6675661448 0.2184705033 527 9.775923e-01 0.7519204454 0.8651312224 0.7083046834 420 9.780004e-01 0.2643208704 0.8263567369 0.7606477893 953 9.787241e-01 0.7583340618 0.0853616341 0.6030265328 545 9.795723e-01 0.9054452714 0.7865849975 0.7885798116 168 9.797689e-01 0.3946641390 0.6334771330 0.6213491932 891 9.812785e-01 0.4585702720 0.0016538552 0.7442474423 961 9.814512e-01 0.5959760868 0.1882749950 0.2779168459 97 9.833093e-01 0.0266579669 0.9691282359 0.9607176126 418 9.842301e-01 0.5712654323 0.7981498616 0.0216022818 616 9.844846e-01 0.6427024952 0.4253862344 0.2759629737 126 9.849503e-01 0.7485080631 0.6450622142 0.9833274896 587 9.851147e-01 0.3733169329 0.6954850936 0.2028214347 426 9.860808e-01 0.4446820407 0.9295317468 0.2347590241 38 9.870220e-01 0.2977459133 0.3165021646 0.1984177802 922 9.872592e-01 0.9825267738 0.5609763565 0.6135865250 254 9.873621e-01 0.0089440597 0.5473776746 0.8885872513 650 9.875903e-01 0.1436544911 0.4789102755 0.6419834672 276 9.887185e-01 0.6629624350 0.4162972015 0.1310563025 876 9.887674e-01 0.5000619232 0.3573094157 0.1268163959 231 9.892501e-01 0.3850566579 0.6298543336 0.0072366130 661 9.901462e-01 0.3397307894 0.1784861020 0.1922769179 152 9.902760e-01 0.5242095138 0.5966212377 0.3967671560 240 9.915536e-01 0.6732728919 0.2480303932 0.5564433485 270 9.924114e-01 0.4641520588 0.6984564601 0.5133074266 713 9.926625e-01 0.2295191360 0.9857337854 0.9854747527 35 9.928012e-01 0.0392448856 0.1567591096 0.0315715638 848 9.947761e-01 0.9395159914 0.1398117112 0.2550433211 793 9.996020e-01 0.3849962784 0.4771569117 0.1785570637","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sorting-dataframes.html"},{"title":"Special Characters In Strings","loc":"http://chrisalbon.com/r-stats/special-characters-in-strings.html","text":"There are some special characters that can be added to strings by placing certain characters in the string. To view these characters we use cat() because the default print() converts these escape characters into regular characters. # Add a tab cat ( \"cat\\tdog\" ) cat dog # Add a line break cat ( \"cat\\ndog\" ) cat dog","tags":"R Stats","url":"http://chrisalbon.com/r-stats/special-characters-in-strings.html"},{"title":"Split, Apply, Combine","loc":"http://chrisalbon.com/r-stats/split-apply-combine.html","text":"Original source: Learning R # generate fake data with multiple lines for a single observation war.name <- c ( \"WWII\" , \"WWII\" , \"WWI\" , \"WWI\" , \"Franco-Prussian\" , \"Franco-Prussian\" , \"Franco-Prussian\" , \"Boer War\" , \"Boer War\" , \"Boer War\" ) deaths <- c ( 938 , 9480 , 2049 , 1039 , 3928 , 9202 , 10933 , 40293 , 10394 , 20394 ) casualties <- data.frame ( war.name , deaths ); casualties war.name deaths 1 WWII 938 2 WWII 9480 3 WWI 2049 4 WWI 1039 5 Franco-Prussian 3928 6 Franco-Prussian 9202 7 Franco-Prussian 10933 8 Boer War 40293 9 Boer War 10394 10 Boer War 20394 Using the split function we can calculate the average deaths for each war, even though they are split on different lines. ## split the deaths by war.name casualties.by.war <- with ( casualties , split ( deaths , war.name )) ## calculate the list mean war deaths for each war list.of.means.by.war <- lapply ( casualties.by.war , mean ) # convert the list into a vector mean.by.war <- unlist ( list.of.means.by.war )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/split-apply-combine.html"},{"title":"Split, Apply, Recombine","loc":"http://chrisalbon.com/r-stats/split-apply-recombine.html","text":"# split up the data by gear number data <- split ( mtcars , mtcars $ gear ) # apply a function (in this case a linear model) fits <- lapply ( data , function ( x ) return ( lm ( x $ mpg ~ x $ disp ) $ coef )) # recombine the function applied in the line aboove again do.call ( rbind , fits ) (Intercept) x$disp 3 24.51557 -0.02577046 4 39.56753 -0.12221268 5 31.66095 -0.05077512","tags":"R Stats","url":"http://chrisalbon.com/r-stats/split-apply-recombine.html"},{"title":"Data can be split up by levels of a factor","loc":"http://chrisalbon.com/r-stats/split-data-by-factor-levels.html","text":"(i.e. categories in a nominal variable) district <- factor ( c ( \"NORTH\" , \"NORTHWEST\" , \"CENTRAL\" , \"SOUTH\" , \"SOUTHWEST\" , \"WESTERN\" )) number.crimes <- runif ( 6 ) year <- c ( 2009 , 2009 , 2009 , 2010 , 2010 , 2009 ) crime <- data.frame ( year , district , number.crimes ) # split up the crime data by district split ( crime , district ) $CENTRAL year district number.crimes 3 2009 CENTRAL 0.9478017 $NORTH year district number.crimes 1 2009 NORTH 0.6461916 $NORTHWEST year district number.crimes 2 2009 NORTHWEST 0.1150372 $SOUTH year district number.crimes 4 2010 SOUTH 0.8126506 $SOUTHWEST year district number.crimes 5 2010 SOUTHWEST 0.6700562 $WESTERN year district number.crimes 6 2009 WESTERN 0.5597461 Imagine if you also can crime data for 2009, you can use the combination of split() and sapply() to create a nice 2x2 table # create 2009 crime data, for this example we are keeping the data the same for both years numberCrimes09 <- number.crimes # add a column in bmore.crime for numberCrime09 crime <- cbind ( numberCrimes09 , crime ) # display the top few rows of bmore.crime to check everything looks good head ( crime ) #find the combined sum of the both annual crime total columns, by district sapply ( split ( crime [, c ( 'numberCrimes09' , 'number.crimes' )], district ), sum ) numberCrimes09 year district number.crimes 1 0.6461916 2009 NORTH 0.6461916 2 0.1150372 2009 NORTHWEST 0.1150372 3 0.9478017 2009 CENTRAL 0.9478017 4 0.8126506 2010 SOUTH 0.8126506 5 0.6700562 2010 SOUTHWEST 0.6700562 6 0.5597461 2009 WESTERN 0.5597461 CENTRAL NORTH NORTHWEST SOUTH SOUTHWEST WESTERN 1.8956034 1.2923833 0.2300744 1.6253013 1.3401124 1.1194923","tags":"R Stats","url":"http://chrisalbon.com/r-stats/split-data-by-factor-levels.html"},{"title":"Simple Random Sampling Of Rows","loc":"http://chrisalbon.com/r-stats/srs-of-rows.html","text":"# create a dataframe with simulated values x <- runif ( 1000 ) y <- runif ( 1000 ) z <- runif ( 1000 ) a <- runif ( 1000 ) df <- data.frame ( x , y , z , a ) # create a vector of weighs w <- runif ( 1000 ) # sample 10 rows of the dataframe at pseudorandom, without replacement sample <- df [ sample ( 1 : nrow ( df ), 10 , replace = F ),] sample x y z a 500 0.44885287 0.7228785 0.11504159 0.054208551 980 0.08392782 0.4568137 0.04393736 0.639204705 445 0.48527409 0.9822628 0.37823768 0.410292231 734 0.54769226 0.8992187 0.58030521 0.178938602 177 0.43245083 0.9172805 0.02955689 0.743852472 266 0.94394635 0.7638428 0.43960561 0.942814638 256 0.77091541 0.2714964 0.49622060 0.009319224 944 0.92682962 0.4278643 0.87805822 0.283003822 492 0.84598479 0.7312344 0.02114653 0.366224907 409 0.71072941 0.8280846 0.03118358 0.196112987 # sample 10 rows of the dataframe at pseudorandom, without replacement, with the selection of reach row weighted by w (note w doesn't need to add up to 1) sample.weighted <- df [ sample ( 1 : nrow ( df ), 10 , replace = F , prob = w ),] sample.weighted x y z a 588 0.42359291 0.7747600 0.3443440 0.7502456 417 0.23503932 0.2448383 0.8961582 0.2380380 997 0.05755856 0.1587866 0.8144725 0.3183274 789 0.46962589 0.5453313 0.3952169 0.5651571 646 0.74493118 0.5151418 0.8717189 0.8330892 231 0.60350137 0.2860977 0.8010951 0.7727321 957 0.11192911 0.8962056 0.1051003 0.7203017 947 0.38530976 0.7359524 0.1379156 0.7036205 101 0.32523590 0.8928368 0.2648167 0.8808521 208 0.84037450 0.8692373 0.5897262 0.2414655","tags":"R Stats","url":"http://chrisalbon.com/r-stats/srs-of-rows.html"},{"title":"Stacked Area Graph In ggplot2","loc":"http://chrisalbon.com/r-stats/stacked-area-graph.html","text":"Original source: r graphics cookbook # load the ggplot2 library library ( ggplot2 ) # load the gcookbook library library ( gcookbook ) # load the plyr library library ( plyr ) # create the ggplot2 data, using Year and thousands (of people) and filled by age group. ggplot ( uspopage , aes ( x = Year , y = Thousands , fill = AgeGroup )) + # add a area plot layer geom_area () + # change the colors and reserve the legend order (to match to stacking order) scale_fill_brewer ( palette = \"Blues\" , breaks = rev ( levels ( uspopage $ AgeGroup ))) To reverse the stacking order we use plyr's desc() (decsending order) function # create the reversed stack ggplot2 data, using Year and thousands (of people) and filled by age group, grouped by reserve agegroup ggplot ( uspopage , aes ( x = Year , y = Thousands , fill = AgeGroup , order = desc ( AgeGroup ))) + # add a area plot layer geom_area () + # change the colors and reserve the legend order (to match to stacking order) scale_fill_brewer ( palette = \"Blues\" , breaks = levels ( uspopage $ AgeGroup ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/stacked-area-graph.html"},{"title":"Stacked Area Plot","loc":"http://chrisalbon.com/r-stats/stacked-area-plot.html","text":"Source: http://www.r-bloggers.com/a-nifty-area-plot-or-a-bootleg-of-a-ggplot-geom/ # load the ggplot2 package library ( ggplot2 ) # load the reshape package library ( reshape ) # set the random seed set.seed ( 3 ) # create a sequence t.step <- seq ( 0 , 20 ) # create some group names grps <- letters [ 1 : 10 ] # create simulated data for group values across time grp.dat <- runif ( length ( t.step ) * length ( grps ), 5 , 15 ) # create data frame grp.dat <- matrix ( grp.dat , nrow = length ( t.step ), ncol = length ( grps )) grp.dat <- data.frame ( grp.dat , row.names = t.step ) names ( grp.dat ) <- grps #reshape the data p.dat <- data.frame ( step = row.names ( grp.dat ), grp.dat , stringsAsFactors = F ) p.dat <- melt ( p.dat , id = 'step' ) p.dat $ step <- as.numeric ( p.dat $ step ) #create plots p <- ggplot ( p.dat , aes ( x = step , y = value )) + theme ( legend.position = \"none\" ) p + geom_area ( aes ( fill = variable )) p + geom_area ( aes ( fill = variable ), position = 'fill' )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/stacked-area-plot.html"},{"title":"String Formatting","loc":"http://chrisalbon.com/python/string_formatting.html","text":"Import the sys module import sys Print a string with 1 digit and one string. 'This is %d %s bird!' % ( 1 , 'dead' ) 'This is 1dead bird!' Print a dictionary based string ' %(number)d more %(food)s ' % { 'number' : 1 , 'food' : 'burger' } '1 more burger' Print a string about my laptop. 'My {1[kind]} runs {0.platform}' . format ( sys , { 'kind' : 'laptop' }) 'My laptop runs darwin' Codes %s string %r repr string %c character (integer or string) %d decimal %i integer %x hex integer %X same as X but with uppercase %e floating point lowercase %E floating point uppercase %f floating point decimal lowercase %F floating point decimal uppercase %g floating point e or f %G floating point E or F %% literal %","tags":"Python","url":"http://chrisalbon.com/python/string_formatting.html"},{"title":"String Indexing","loc":"http://chrisalbon.com/python/string_indexing.html","text":"Create a string string = 'Strings are defined as ordered collections of characters.' Print the entire string string [:] 'Strings are defined as ordered collections of characters.' Print the first three characters string [ 0 : 2 ] 'St' Print the first three characters string [: 2 ] 'St' Print the last three characters string [ - 3 :] 'rs.' Print the third to fifth character string [ 2 : 4 ] 'ri' Print the first to the tenth character, skipping every other character string [ 0 : 9 : 2 ] 'Srnsa' Print the string in reverse string [:: - 1 ] '.sretcarahc fo snoitcelloc deredro sa denifed era sgnirtS'","tags":"Python","url":"http://chrisalbon.com/python/string_indexing.html"},{"title":"String Operations","loc":"http://chrisalbon.com/python/string_operations.html","text":"Python 3 has three string types str() is for unicode bytes() is for binary data bytesarray() mutable variable of bytes Create some simulated text. string = 'The quick brown fox jumped over the lazy brown bear.' Capitalize the first letter. string_capitalized = string . capitalize () string_capitalized 'The quick brown fox jumped over the lazy brown bear.' Center the string with periods on either side, for a total of 79 characters string_centered = string . center ( 79 , '.' ) string_centered '..............The quick brown fox jumped over the lazy brown bear..............' Count the number of e's between the fifth and last character string_counted = string . count ( 'e' , 4 , len ( string )) string_counted 4 Locate any e's between the fifth and last character string_find = string . find ( 'e' , 4 , len ( string )) string_find 24 Are all characters are alphabet? string_isalpha = string . isalpha () string_isalpha False Are all characters digits? string_isdigit = string . isdigit () string_isdigit False Are all characters lower case? string_islower = string . islower () string_islower False Are all chracters alphanumeric? string_isalnum = string . isalnum () string_isalnum False Are all characters whitespaces? string_isalnum = string . isspace () string_isalnum False Is the string properly titlespaced? string_istitle = string . istitle () string_istitle False Are all the characters uppercase? string_isupper = string . isupper () string_isupper False Return the lengths of string len ( string ) 52 Convert string to lower case string_lower = string . lower () string_lower 'the quick brown fox jumped over the lazy brown bear.' Convert string to lower case string_upper = string . upper () string_upper 'THE QUICK BROWN FOX JUMPED OVER THE LAZY BROWN BEAR.' Convert string to title case string_title = string . title () string_title 'The Quick Brown Fox Jumped Over The Lazy Brown Bear.' Convert string the inverted case string_swapcase = string . swapcase () string_swapcase 'tHE QUICK BROWN FOX JUMPED OVER THE LAZY BROWN BEAR.' Remove all leading whitespaces (i.e. to the left) string_lstrip = string . lstrip () string_lstrip 'The quick brown fox jumped over the lazy brown bear.' Remove all leading and trailing whitespaces (i.e. to the left and right) string_strip = string . strip () string_strip 'The quick brown fox jumped over the lazy brown bear.' Remove all trailing whitespaces (i.e. to the right) string_rstrip = string . rstrip () string_rstrip 'The quick brown fox jumped over the lazy brown bear.' Replace lower case e's with upper case E's, to a maximum of 4 string_replace = string . replace ( 'e' , 'E' , 4 ) string_replace 'ThE quick brown fox jumpEd ovEr thE lazy brown bear.'","tags":"Python","url":"http://chrisalbon.com/python/string_operations.html"},{"title":"Strings","loc":"http://chrisalbon.com/r-stats/strings.html","text":"Strings are text data stored in character vectors. Paste function can \"paste\" text into string elements # Create a character vector with three strings meals <- c ( \"cheeseburger\" , \"soup\" , \"sandwich\" ) # Paste \"bacon\" onto the end of each string element bacon.meals <- paste ( meals , \"bacon\" ); bacon.meals [1] \"cheeseburger bacon\" \"soup bacon\" \"sandwich bacon\" bacon.meals.dash <- paste ( meals , \"bacon\" , sep = \"-\" ); bacon.meals.dash ; bacon.meals.dash [1] \"cheeseburger-bacon\" \"soup-bacon\" \"sandwich-bacon\" [1] \"cheeseburger-bacon\" \"soup-bacon\" \"sandwich-bacon\" # We can also collapse the whole vector into a single string mouthful <- paste ( meals , collapse = \"\" ); mouthful [1] \"cheeseburgersoupsandwich\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/strings.html"},{"title":"Converting Strings To Logical","loc":"http://chrisalbon.com/r-stats/strings-to-logical.html","text":"# Create a string with Y and N string elements answers <- c ( \"Y\" , \"Y\" , \"N\" , \"Y\" , \"N\" ) # write a function that converts \"Y\" to TRUE and \"N\" to FALSE yn_to_logical <- function ( x ) { y <- rep.int ( NA , length ( x )) y [ x == \"Y\" ] <- TRUE y [ x == \"N\" ] <- FALSE y } # run the function on the data yn_to_logical ( answers ) [1] TRUE TRUE FALSE TRUE FALSE","tags":"R Stats","url":"http://chrisalbon.com/r-stats/strings-to-logical.html"},{"title":"Converting Strings To Datetime","loc":"http://chrisalbon.com/python/strings_to_datetime.html","text":"Import modules from datetime import datetime from dateutil.parser import parse import pandas as pd Create a string variable with the war start time war_start = '2011-01-03' Convert the string to datetime format datetime . strptime ( war_start , '%Y-%m- %d ' ) datetime.datetime(2011, 1, 3, 0, 0) Create a list of strings as dates attack_dates = [ '7/2/2011' , '8/6/2012' , '11/13/2013' , '5/26/2011' , '5/2/2001' ] Convert attack_dates strings into datetime format [ datetime . strptime ( x , '%m/ %d /%Y' ) for x in attack_dates ] [datetime.datetime(2011, 7, 2, 0, 0), datetime.datetime(2012, 8, 6, 0, 0), datetime.datetime(2013, 11, 13, 0, 0), datetime.datetime(2011, 5, 26, 0, 0), datetime.datetime(2001, 5, 2, 0, 0)] Use parse() to attempt to auto-convert common string formats parse ( war_start ) datetime.datetime(2011, 1, 3, 0, 0) Use parse() on every element of the attack_dates string [ parse ( x ) for x in attack_dates ] [datetime.datetime(2011, 7, 2, 0, 0), datetime.datetime(2012, 8, 6, 0, 0), datetime.datetime(2013, 11, 13, 0, 0), datetime.datetime(2011, 5, 26, 0, 0), datetime.datetime(2001, 5, 2, 0, 0)] Use parse, but designate that the day is first parse ( war_start , dayfirst = True ) datetime.datetime(2011, 3, 1, 0, 0) Create a dataframe data = { 'date' : [ '2014-05-01 18:47:05.069722' , '2014-05-01 18:47:05.119994' , '2014-05-02 18:47:05.178768' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.230071' , '2014-05-02 18:47:05.280592' , '2014-05-03 18:47:05.332662' , '2014-05-03 18:47:05.385109' , '2014-05-04 18:47:05.436523' , '2014-05-04 18:47:05.486877' ], 'value' : [ 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 ]} df = pd . DataFrame ( data , columns = [ 'date' , 'value' ]) print ( df ) date value 0 2014-05-01 18:47:05.069722 1 1 2014-05-01 18:47:05.119994 1 2 2014-05-02 18:47:05.178768 1 3 2014-05-02 18:47:05.230071 1 4 2014-05-02 18:47:05.230071 1 5 2014-05-02 18:47:05.280592 1 6 2014-05-03 18:47:05.332662 1 7 2014-05-03 18:47:05.385109 1 8 2014-05-04 18:47:05.436523 1 9 2014-05-04 18:47:05.486877 1 Convert df['date'] from string to datetime pd . to_datetime ( df [ 'date' ]) 0 2014-05-01 18:47:05.069722 1 2014-05-01 18:47:05.119994 2 2014-05-02 18:47:05.178768 3 2014-05-02 18:47:05.230071 4 2014-05-02 18:47:05.230071 5 2014-05-02 18:47:05.280592 6 2014-05-03 18:47:05.332662 7 2014-05-03 18:47:05.385109 8 2014-05-04 18:47:05.436523 9 2014-05-04 18:47:05.486877 Name: date, dtype: datetime64[ns]","tags":"Python","url":"http://chrisalbon.com/python/strings_to_datetime.html"},{"title":"Styling Axes","loc":"http://chrisalbon.com/r-stats/styling-axes.html","text":"Original source: r graphics cookbook # load the gcookbook package for the data library ( gcookbook ) # load the ggplot2 package library ( ggplot2 ) Swapping Axes # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # swap the axes coord_flip () # Reverse The Axis ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # reverse the y-axis scale_y_reverse () Set The Range Of An Axis # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # set the y axis to range from 0 to 10 scale_y_continuous ( limits = c ( 0 , 10 )) Zoom In # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # zoom in to the plot so the plot displays only between y=5 and y=6.5 coord_cartesian ( ylim = c ( 5 , 6.5 )) Rearrange The Display Ordering Of Categorical Data # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # Display the three boxplots in a custom order scale_x_discrete ( limits = c ( \"trt1\" , \"ctrl\" , \"trt2\" )) Set The Axis Tick Marks # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # Set the tick marks at certain points scale_y_continuous ( breaks = c ( 4 , 4.25 , 4.5 , 5 , 6 , 8 )) # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # Remove the tick marks scale_y_continuous ( breaks = NULL ) # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # Set the tick marks at certain points scale_y_continuous ( breaks = c ( 4 , 4.25 , 4.5 , 5 , 6 , 8 ), # set custom tick labels labels = c ( \"Super\\nTiny\" , \"Tiny\" , \"Really\\nshort\" , \"Short\" , \"Medium\" , \"Tallish\" )) + scale_x_discrete ( breaks = c ( \"ctrl\" , \"trt1\" , \"trt2\" ), # set custom tick labels labels = c ( \"Control\" , \"Treatment 1\" , \"Treatment 2\" )) + # set the axis label angle theme ( axis.text.x = element_text ( angle = 30 , hjust = 1 , vjust = 1 , family = \"Times\" , face = \"italic\" , colour = \"darkred\" )) Set The Axis Labels # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # write a custom xlab xlab ( \"Age in years\" ) + # wrote a custom ylab ylab ( \"Height in inches\" ) Remove The Axis Labels # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # remove the x label theme ( axis.title.x = element_blank ()) Style the Axis Label # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # style the x axis theme ( axis.title.x = element_text ( face = \"italic\" , colour = \"darkred\" , size = 14 )) Drawing Axis Lines # create the ggplot2 data ggplot ( PlantGrowth , aes ( x = group , y = weight )) + # plot the boxplots geom_boxplot () + # style whole plot as black and white theme_bw () + # draw axis line with a strong line and the x-y corner being a square theme ( panel.border = element_blank (), axis.line = element_line ( colour = \"black\" , size = 2 , lineend = \"square\" ))","tags":"R Stats","url":"http://chrisalbon.com/r-stats/styling-axes.html"},{"title":"Subsetting Data","loc":"http://chrisalbon.com/r-stats/subsets.html","text":"Original source: http://rforpublichealth.blogspot.com/2012/10/quick-and-easy-subsetting.html # create some simulated data ID <- 1 : 10 Age <- c ( 26 , 65 , 15 , 7 , 88 , 43 , 28 , 66 , 45 , 12 ) Sex <- c ( 1 , 0 , 1 , 1 , 0 , 1 , 1 , 1 , 0 , 1 ) Weight <- c ( 132 , 122 , 184 , 145 , 118 , NA , 128 , 154 , 166 , 164 ) Height <- c ( 60 , 63 , 57 , 59 , 64 , NA , 67 , 65 , NA , 60 ) Married <- c ( 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 1 ) # create a dataframe of the simulated data mydata <- data.frame ( ID , Age , Sex , Weight , Height , Married ) mydata ID Age Sex Weight Height Married 1 1 26 1 132 60 0 2 2 65 0 122 63 0 3 3 15 1 184 57 0 4 4 7 1 145 59 0 5 5 88 0 118 64 0 6 6 43 1 NA NA 0 7 7 28 1 128 67 1 8 8 66 1 154 65 1 9 9 45 0 166 NA 0 10 10 12 1 164 60 1 # create a new variable called sub.data that is the subset of mydata, containing all rows where Age is less than 50 and sex is 0 and the columns ID, Age, and Weight sub.data <- subset ( mydata , Age > 50 & Sex == 0 , select = c ( ID , Age , Weight )) sub.data ID Age Weight 2 2 65 122 5 5 88 118","tags":"R Stats","url":"http://chrisalbon.com/r-stats/subsets.html"},{"title":"Summarize A Variable","loc":"http://chrisalbon.com/r-stats/summarize.html","text":"# Create two variables of 50 observations percent.sms <- runif ( 50 ) location <- state.name # Create a dataframe of those two variables usa <- data.frame ( location , percent.sms ) # Summarize the dataframe summary ( usa ) location percent.sms Alabama : 1 Min. :0.0002011 Alaska : 1 1st Qu.:0.2562950 Arizona : 1 Median :0.5068584 Arkansas : 1 Mean :0.4832696 California: 1 3rd Qu.:0.7120861 Colorado : 1 Max. :0.9816996 (Other) :44 # View the top few rows of the dataframe head ( usa ) location percent.sms 1 Alabama 0.8962870 2 Alaska 0.5507513 3 Arizona 0.5061370 4 Arkansas 0.9779650 5 California 0.6913123 6 Colorado 0.4087530 # View the structure of the dataframe str ( usa ) 'data.frame': 50 obs. of 2 variables: $ location : Factor w/ 50 levels \"Alabama\",\"Alaska\",..: 1 2 3 4 5 6 7 8 9 10 ... $ percent.sms: num 0.896 0.551 0.506 0.978 0.691 ... # View the attributes of the dataframe attributes ( usa ) $names [1] \"location\" \"percent.sms\" $row.names [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 $class [1] \"data.frame\" # View the dataframe View ( usa )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/summarize.html"},{"title":"Swapping variable values","loc":"http://chrisalbon.com/python/swapping_variable_values.html","text":"Note: Originally from: Learning Python Setup the originally variables and their values one = 1 two = 2 View the original variables 'one =' , one , 'two =' , two ('one =', 1, 'two =', 2) Swap the values one , two = two , one View the swapped values, notice how the values for each variable have changed 'one =' , one , 'two =' , two ('one =', 2, 'two =', 1)","tags":"Python","url":"http://chrisalbon.com/python/swapping_variable_values.html"},{"title":"Sweep Over An Dataframe","loc":"http://chrisalbon.com/r-stats/sweep-over-a-df.html","text":"Original source: the r book # create a dataframe with simulated values x <- runif ( 100 ) y <- runif ( 100 ) z <- runif ( 100 ) a <- runif ( 100 ) data <- data.frame ( x , y , z , a ) rm ( x , y , z , a ) # Add 10 to the first column, 20 to the second column, 30 to the third column, 40 to the fourth column. Columns are denoted with the \"2\" sweep ( data , 2 , c ( 10 , 20 , 30 , 40 ), \"+\" ) x y z a 1 10.32730 20.32033 30.25435 40.35822 2 10.25235 20.01114 30.92937 40.41954 3 10.35791 20.53621 30.51055 40.05875 4 10.11858 20.23212 30.17816 40.27137 5 10.88790 20.99958 30.60182 40.20856 6 10.52219 20.43288 30.90038 40.11521 7 10.46691 20.69662 30.42834 40.56927 8 10.26317 20.73948 30.38198 40.44033 9 10.22629 20.50134 30.52825 40.14349 10 10.35842 20.30308 30.59516 40.86080 11 10.96218 20.40019 30.67054 40.23832 12 10.69216 20.56607 30.13535 40.20793 13 10.88016 20.44120 30.96720 40.95606 14 10.88235 20.17180 30.73084 40.43032 15 10.03777 20.98083 30.44728 40.52844 16 10.88287 20.11302 30.20767 40.27113 17 10.64171 20.70630 30.77119 40.32028 18 10.03359 20.09994 30.69826 40.25316 19 10.47795 20.13006 30.31004 40.16694 20 10.32405 20.63986 30.27047 40.78593 21 10.97502 20.14313 30.03588 40.05490 22 10.90446 20.05095 30.08117 40.76391 23 10.71511 20.06212 30.10469 40.06789 24 10.92001 20.61362 30.90136 40.76595 25 10.60598 20.24580 30.33061 40.92882 26 10.22627 20.07714 30.82103 40.21735 27 10.35566 20.06530 30.93909 40.91805 28 10.40162 20.17826 30.79633 40.42704 29 10.17046 20.57119 30.40088 40.23268 30 10.63219 20.70219 30.36659 40.32080 31 10.89678 20.47896 30.73640 40.11156 32 10.92914 20.15821 30.98486 40.57949 33 10.87023 20.70825 30.05313 40.73828 34 10.33950 20.39289 30.76355 40.92854 35 10.67925 20.77507 30.00954 40.67655 36 10.80570 20.03596 30.47324 40.91835 37 10.58231 20.69349 30.19653 40.60302 38 10.95704 20.28324 30.62937 40.02016 39 10.45236 20.76670 30.73899 40.25480 40 10.10766 20.58719 30.86107 40.13780 41 10.00444 20.68638 30.62684 40.45284 42 10.42376 20.43846 30.14995 40.13457 43 10.27377 20.55315 30.12707 40.28277 44 10.92104 20.36958 30.41070 40.17274 45 10.02524 20.78526 30.28361 40.76992 46 10.04650 20.72938 30.36932 40.88512 47 10.01691 20.65482 30.31852 40.33411 48 10.06295 20.15335 30.82940 40.57323 49 10.07610 20.60222 30.06898 40.34765 50 10.96166 20.44878 30.99801 40.92482 51 10.57523 20.20867 30.46799 40.36589 52 10.27001 20.36986 30.55987 40.46065 53 10.67929 20.36916 30.82538 40.89647 54 10.19466 20.69508 30.81382 40.97260 55 10.11495 20.53541 30.22398 40.07807 56 10.02986 20.48134 30.44946 40.51284 57 10.27442 20.33044 30.05853 40.47423 58 10.61156 20.30363 30.74917 40.66185 59 10.93385 20.73686 30.07704 40.67739 60 10.75203 20.77763 30.51821 40.94442 61 10.68343 20.27160 30.65755 40.09830 62 10.24891 20.00158 30.07434 40.76243 63 10.21683 20.22156 30.21365 40.52795 64 10.67541 20.40373 30.50563 40.57624 65 10.58832 20.82957 30.85743 40.50789 66 10.48336 20.41940 30.64045 40.20400 67 10.76902 20.67303 30.65093 40.35683 68 10.09627 20.73701 30.12208 40.94728 69 10.17750 20.46018 30.00964 40.71895 70 10.30874 20.49484 30.15179 40.87569 71 10.48646 20.73930 30.36417 40.01011 72 10.69185 20.28898 30.55007 40.92272 73 10.82433 20.13282 30.86382 40.58866 74 10.00377 20.70401 30.23805 40.19058 75 10.45241 20.81022 30.75468 40.08258 76 10.33948 20.02256 30.42336 40.25027 77 10.80330 20.70447 30.27600 40.52746 78 10.48710 20.73166 30.12767 40.30538 79 10.01032 20.49381 30.59134 40.34080 80 10.09500 20.88171 30.94042 40.79542 81 10.35373 20.75203 30.56967 40.75710 82 10.44146 20.40917 30.41113 40.89054 83 10.10264 20.94804 30.53518 40.59626 84 10.79171 20.83813 30.33770 40.19323 85 10.12137 20.77931 30.70078 40.07897 86 10.59251 20.52565 30.92419 40.31978 87 10.42809 20.43321 30.41216 40.98018 88 10.36346 20.97021 30.17975 40.33618 89 10.45994 20.89540 30.82994 40.31142 90 10.18585 20.59533 30.52392 40.63472 91 10.40786 20.57543 30.49794 40.08891 92 10.13158 20.29264 30.13490 40.06497 93 10.00396 20.77002 30.16856 40.35595 94 10.04839 20.46230 30.70012 40.34182 95 10.74123 20.42913 30.37626 40.24870 96 10.52422 20.26828 30.23467 40.23621 97 10.92283 20.11496 30.79288 40.93147 98 10.83929 20.60866 30.25455 40.10090 99 10.45022 20.43363 30.35828 40.81281 100 10.48697 20.86386 30.76933 40.27158","tags":"R Stats","url":"http://chrisalbon.com/r-stats/sweep-over-a-df.html"},{"title":"Switch Function","loc":"http://chrisalbon.com/r-stats/switch.html","text":"Original source: http://stackoverflow.com/questions/7825501/switch-statement-usage The switch function is the same as having a bunch of if statements. It allows R to trigger between different actions based on an input. # create fake data y <- runif ( 10 ) # Create a function that switches between different types of averages depending on the input. In this function \"type\" is input variable that the user enters to select which action is triggered. avg <- function ( x , type ) { switch ( type , mean = mean ( x ), median = median ( x ), trimmed = mean ( x , trim = .2 )) } # Trigger the function to calculate the mean avg ( y , \"mean\" ) [1] 0.6176063 # Trigger the function to calculate the median avg ( y , \"median\" ) [1] 0.6356017 # Trigger the function to calculate a trimmed mean avg ( y , \"trimmed\" ) [1] 0.6390694","tags":"R Stats","url":"http://chrisalbon.com/r-stats/switch.html"},{"title":"Testing if a number is odd or even","loc":"http://chrisalbon.com/r-stats/test-if-number-is-odd-or-even.html","text":"When you divide one number into another, sometimes you are left with a remainder. That remainder is called a modulo. # If you divide an odd number in half, it will always have a remainder of 1. 173 %% 2 # what is the modulo value (i.e. the remainder) of 143 when divided by 2 [1] 1 # If you divide an even number in half, it will always have a remainder of 0. 40 %% 2 # what is the modulo value (i.e. the remainder) of 143 when divided by 2 [1] 0","tags":"R Stats","url":"http://chrisalbon.com/r-stats/test-if-number-is-odd-or-even.html"},{"title":"Thematic Map","loc":"http://chrisalbon.com/r-stats/thematic-maps.html","text":"Original source: http://bcb.dfci.harvard.edu/~aedin/courses/R/CDC/maps.html # load the maps package library ( maps ) # load the ggmap package library ( ggmap ) # create a graphic \"grid\" for layout purposes of 2 rows and 1 column par ( mfrow = c ( 2 , 1 )) # load the US county map map ( \"county\" ) # load national unemployment data data ( unemp ) # load national FIPS county codes data ( county.fips ) # create a range of colors for the map fill colors = c ( \"#F1EEF6\" , \"#D4B9DA\" , \"#C994C7\" , \"#DF65B0\" , \"#DD1C77\" , \"#980043\" ) # create a colorBuckets object cutting up unemp's values into buckets according to a vector of cutpoints unemp $ colorBuckets <- as.numeric ( cut ( unemp $ unemp , c ( 0 , 2 , 4 , 6 , 8 , 10 , 100 ))) # match the colors in \"colors\" with those buckets colorsmatched <- unemp $ colorBuckets [ match ( county.fips $ fips , unemp $ fips )] # create a map with colors according to the bucket colors map ( \"county\" , col = colors [ colorsmatched ], fill = TRUE , resolution = 0 , lty = 0 , projection = \"polyconic\" ) # Add border around each State map ( \"state\" , col = \"white\" , fill = FALSE , add = TRUE , lty = 1 , lwd = 0.4 , projection = \"polyconic\" ) title ( \"unemployment by county, 2009\" ) # add some text for a legend. leg.txt <- c ( \"<2%\" , \"2-4%\" , \"4-6%\" , \"6-8%\" , \"8-10%\" , \">10%\" ) # add a legend legend ( \"topright\" , leg.txt , horiz = TRUE , fill = colors )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/thematic-maps.html"},{"title":"Time and Dates in R","loc":"http://chrisalbon.com/r-stats/time-and-dates.html","text":"Original source: http://rforpublichealth.blogspot.com/2013/07/lets-make-date-date-and-time-classes-in.html # create some simulated date data dates <- as.data.frame ( cbind ( c ( 1 , 3 , 6 , 11 , 4 , 12 , 5 , 3 ), c ( 30 , 14 , NA , NA , 16 , NA , 20 , 31 ), c ( 1980 , 1980 , 1980 , 1983 , 1983 , 1983 , 1986 , 1980 ), c ( 2 , NA , NA , NA , NA , 12 , 4 , NA ), c ( 2 , NA , NA , NA , NA , NA , 29 , NA ), c ( 1980 , NA , NA , 1985 , NA , 1983 , 1987 , NA ))) # add column names to the data frame colnames ( dates ) <- c ( \"birth_month\" , \"birth_day\" , \"birth_year\" , \"death_month\" , \"death_day\" , \"death_year\" ) Dates can be easily manipulated if they are converted into ISO format ISO format uses the function: ISOdate(year, month, day, hour = 12, min = 0, sec = 0, tz = \"GMT\") # create a new variable called DOB that converts the birth date data into ISO dates $ DOB <- ISOdate ( dates $ birth_year , dates $ birth_month , dates $ birth_day ) # ISOdate includes time by default, we can strip time from the DOB data using strptime() dates $ DOB <- strptime ( dates $ DOB , format = \"%Y-%m-%d\" ) # create date of death data combining the two commands we ran above dates $ DOD <- strptime ( ISOdate ( dates $ death_year , dates $ death_month , dates $ death_day ), format = \"%Y-%m-%d\" ) We can use the difftime() function to calculate the difference in two dates/times The format of difftime() is: difftime(time1, time2, tz,units = c(\"auto\", \"secs\", \"mins\", \"hours\",\"days\", \"weeks\")) # create a variable that is the difference between the birth and death date, ie their age (in days) dates $ Age.atdeath <- difftime ( dates $ DOD , dates $ DOB , unit = \"days\" ) # check if there were an infant mortalities dates $ Age.atdeath < 365 [1] TRUE NA NA NA NA NA TRUE NA Clever trick: if day/time data is missing, you can set the missing date/time data to fixed numbers. Comments inline. # create a variable DOB2, strip the time off dates $ DOB2 <- strptime ( # convert the data into ISO format ISOdate ( year = dates $ birth_year , # if month observation is missing, set month to 1, else set it to birth month month = ifelse ( is.na ( dates $ birth_month ), 1 , dates $ birth_month ), # if day observation is missing, set day to 1, else set it to birth day day = ifelse ( is.na ( dates $ birth_day ), 1 , dates $ birth_day )), # format to display the date format = \"%Y-%m-%d\" ) # create a variable DOD2, strip the time off dates $ DOD2 <- strptime ( # convert the data into ISO format ISOdate ( year = dates $ death_year , # if month observation is missing, set month to 1, else set it to death month month = ifelse ( is.na ( dates $ death_month ), 12 , dates $ death_month ), # if day observation is missing, set day to 1, else set it to death day day = ifelse ( is.na ( dates $ death_day ), 30 , dates $ death_day )), # format to display the date format = \"%Y-%m-%d\" ) # create a new variable that converts the differnce in time to numeric dates $ Ageatdeath_2 <- as.numeric ( difftime ( dates $ DOD2 , dates $ DOB2 , unit = \"days\" )) # view columns 1 through 6, and 10 through 12 dates [, c ( 1 : 6 , 10 : 12 )] birth_month birth_day birth_year death_month death_day death_year DOB2 1 1 30 1980 2 2 1980 1980-01-30 2 3 14 1980 NA NA NA 1980-03-14 3 6 NA 1980 NA NA NA 1980-06-01 4 11 NA 1983 NA NA 1985 1983-11-01 5 4 16 1983 NA NA NA 1983-04-16 6 12 NA 1983 12 NA 1983 1983-12-01 7 5 20 1986 4 29 1987 1986-05-20 8 3 31 1980 NA NA NA 1980-03-31 DOD2 Ageatdeath_2 1 1980-02-02 3 2 <NA> NA 3 <NA> NA 4 1985-12-30 790 5 <NA> NA 6 1983-12-30 29 7 1987-04-29 344 8 <NA> NA","tags":"R Stats","url":"http://chrisalbon.com/r-stats/time-and-dates.html"},{"title":"Truncate A String","loc":"http://chrisalbon.com/r-stats/truncate-a-string.html","text":"Source: http://www.r-bloggers.com/truncate-by-delimiter-in-r/ # create some simulated data patients <- data.frame ( uid = 1 : 3 , fullname = c ( \"Smith/John\" , \"Jackson/Smith\" , \"Joel/Billy\" )) patients $ lastname <- sub ( \"/.*\" , \"\" , patients $ fullname ); patients $ lastname [1] \"Smith\" \"Jackson\" \"Joel\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/truncate-a-string.html"},{"title":"Try, Except, and Finally","loc":"http://chrisalbon.com/python/try_except_finally.html","text":"Create data # Create some data scores = [ 23 , 453 , 54 , 235 , 74 , 234 ] Try something that doesn't work # Try to: try : # Add a list of integers and a string scores + 'A string of characters.' # If you get an error, set the error as 'e', except Exception as e : # print the error, e print ( 'Error:' , e ) # Then, finally : # print end program print ( 'End Program' ) Error: can only concatenate list (not \"str\") to list End Program Try something that works # Try to: try : # Print scores print ( 'Worked!' , scores ) # If you get an error, set the error as 'e', except Exception as e : # print the error, e print ( 'Error:' , e ) # Then, finally : # print end program print ( 'End program' ) Worked! [23, 453, 54, 235, 74, 234] End program","tags":"Python","url":"http://chrisalbon.com/python/try_except_finally.html"},{"title":"Tufte's Horizontal Bar Lines","loc":"http://chrisalbon.com/r-stats/tuftes-horizontal-bar-lines.html","text":"Original source: http://stackoverflow.com/questions/13701485/r-graphs-creating-tuftes-horizontal-bar-lines # load the ggplot2 package library ( ggplot2 ) # create the ggplot2 data ggplot ( msleep , aes ( x = order )) + # draw the bars stat_bin () + # draw the black and white theme theme_bw () + # draw white horizontal lines on the yintercept between 5 and 20, spaced every 5 geom_hline ( yintercept = seq ( 5 , 20 , 5 ), col = \"white\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/tuftes-horizontal-bar-lines.html"},{"title":"Changing Uppercase And Lowercase","loc":"http://chrisalbon.com/r-stats/upper-and-lower-case.html","text":"# create a character vector text <- c ( \"DSFGSsdffdsDSF\" , \"DdsFDSFsdfSsdf\" , \"DDDDDDDD\" , \"kkkkkkkk\" ) # convert to upper case toupper ( text ) [1] \"DSFGSSDFFDSDSF\" \"DDSFDSFSDFSSDF\" \"DDDDDDDD\" \"KKKKKKKK\" # convert to all lower case tolower ( text ) [1] \"dsfgssdffdsdsf\" \"ddsfdsfsdfssdf\" \"dddddddd\" \"kkkkkkkk\"","tags":"R Stats","url":"http://chrisalbon.com/r-stats/upper-and-lower-case.html"},{"title":"Violin Plots","loc":"http://chrisalbon.com/r-stats/violin-plots.html","text":"Original Source: http://www.statmethods.net/graphs/boxplot.html # Load package library ( vioplot ) Loading required package: sm Package 'sm', version 2.2-5.4: type help(sm) for summary information # Create some fake data about different types of casualties in a war fpwar <- c ( 234 , 234 , 643 , 74 , 323 , 67 , 34 , 78 , 434 ) wwi <- c ( 42 , 534 , 65 , 47 , 85 , 67 , 90 , 45 , 78 ) wwii <- c ( 345 , 2 , 25 , 345 , 35 , 373 , 463 , 46 , 85 ) # Create a violin of the number of deaths vioplot ( fpwar , wwi , wwii , names = c ( \"Franco-Prussian\" , \"WWI\" , \"WWII\" ), col = \"red3\" )","tags":"R Stats","url":"http://chrisalbon.com/r-stats/violin-plots.html"},{"title":"while Statement","loc":"http://chrisalbon.com/python/while_statements.html","text":"Note: Based on: A Byte of Python While loops until a condition is true. Import the random module. import random Create a variable of the true number of deaths of an event. deaths = 6 Create a variable that is denotes if the while loop for keep running. running = True while running is True while running : # Create a variable that randomly create a integer between 0 and 10. guess = random . randint ( 0 , 10 ) # if guess equals deaths, if guess == deaths : # then print this print ( 'Correct!' ) # and then also change running to False to stop the script running = False # else if guess is lower than deaths elif guess < deaths : # then print this print ( 'No, it is higher.' ) # if guess is none of the above else : # print this print ( 'No, it is lower' ) No, it is lower No, it is higher. Correct! By the output, you can see that the while script keeping generating guesses and checking them until guess matches deaths, in which case the script stops.","tags":"Python","url":"http://chrisalbon.com/python/while_statements.html"},{"title":"Monitor A Website For Changes With Python","loc":"http://chrisalbon.com/python/monitor_a_website.html","text":"In this snippet, we create a continous loop that, at set times, scrapes a website, checks to see if it contains some text and if so, emails me. Specifically I used this script to find when Venture Beat had published an article about my company. It should be noted that there are more efficient ways of setting scripts to run at certain times, notable cron. However, this is a quick and dirty solution. Note: I've commented out the last few lines of this tutorial, which attempts to send an email. Before running this code, uncomment those lines Preliminaries # Import requests (to download the page) import requests # Import BeautifulSoup (to parse what we download) from bs4 import BeautifulSoup # Import Time (to add a delay between the times the scape runs) import time # Import smtplib (to allow us to email) import smtplib Monitoring Script # This is a pretty simple script. The script downloads the homepage of VentureBeat, and if it finds some text, emails me. # If it does not find some text, it waits 60 seconds and downloads the homepage again. # while this is true (it is true by default), while True : # set the url as VentureBeat, url = \"http://Google.com/\" # set the headers like we are a browser, headers = { 'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36' } # download the homepage response = requests . get ( url , headers = headers ) # parse the downloaded homepage and grab all text, then, soup = BeautifulSoup ( response . text , \"lxml\" ) # if the number of times the word \"Google\" occurs on the page is less than 1, if str ( soup ) . find ( \"Google\" ) == - 1 : # wait 60 seconds, time . sleep ( 60 ) # continue with the script, continue # but if the word \"Google\" occurs any other number of times, else : # create an email message with just a subject line, msg = 'Subject: This is Chris \\' s script talking, CHECK GOOGLE!' # set the 'from' address, fromaddr = 'YOUR_EMAIL_ADDRESS' # set the 'to' addresses, toaddrs = [ 'AN_EMAIL_ADDRESS' , 'A_SECOND_EMAIL_ADDRESS' , 'A_THIRD_EMAIL_ADDRESS' ] # setup the email server, # server = smtplib.SMTP('smtp.gmail.com', 587) # server.starttls() # add my account login name and password, # server.login(\"YOUR_EMAIL_ADDRESS\", \"YOUR_PASSWORD\") # Print the email's contents print ( 'From: ' + fromaddr ) print ( 'To: ' + str ( toaddrs )) print ( 'Message: ' + msg ) # send the email # server.sendmail(fromaddr, toaddrs, msg) # disconnect from the server # server.quit() break From: YOUR_EMAIL_ADDRESS To: ['AN_EMAIL_ADDRESS', 'A_SECOND_EMAIL_ADDRESS', 'A_THIRD_EMAIL_ADDRESS'] Message: Subject: This is Chris's script talking, CHECK GOOGLE!","tags":"Python","url":"http://chrisalbon.com/python/monitor_a_website.html"},{"title":"Quickly Change A Column Of Strings In Pandas","loc":"http://chrisalbon.com/python/pandas_change_column_of_strings.html","text":"Often I need or want to change the case of all items in a column of strings (e.g. BRAZIL to Brazil, etc.). There are many ways to accomplish this but I have settled on this one as the easiest and quickest. # Import pandas import pandas as pd # Create a list of first names first_names = pd . Series ([ 'Steve Murrey' , 'Jane Fonda' , 'Sara McGully' , 'Mary Jane' ]) # print the column first_names 0 Steve Murrey 1 Jane Fonda 2 Sara McGully 3 Mary Jane dtype: object # print the column with lower case first_names . str . lower () 0 steve murrey 1 jane fonda 2 sara mcgully 3 mary jane dtype: object # print the column with upper case first_names . str . upper () 0 STEVE MURREY 1 JANE FONDA 2 SARA MCGULLY 3 MARY JANE dtype: object # print the column with title case first_names . str . title () 0 Steve Murrey 1 Jane Fonda 2 Sara Mcgully 3 Mary Jane dtype: object # print the column split across spaces first_names . str . split ( \" \" ) 0 [Steve, Murrey] 1 [Jane, Fonda] 2 [Sara, McGully] 3 [Mary, Jane] dtype: object # print the column with capitalized case first_names . str . capitalize () 0 Steve murrey 1 Jane fonda 2 Sara mcgully 3 Mary jane dtype: object You get the idea. Many more string methods are avaliable here","tags":"Python","url":"http://chrisalbon.com/python/pandas_change_column_of_strings.html"},{"title":"Multiline Strings","loc":"http://chrisalbon.com/javascript/multiline_strings.html","text":"There are various ways to include multiline strings in JavaScript. Here is the generally preferable way. // Create a series of strings, one per desired line. var multiline = \"Hello Steve\\n\" + \"Rock on.\\n\" + \"From Chris\" ; // Print the string console . log ( multiline ) Hello Steve Rock on. From Chris","tags":"Javascript","url":"http://chrisalbon.com/javascript/multiline_strings.html"},{"title":"Printing Variables Inside Strings","loc":"http://chrisalbon.com/javascript/printing_variables_inside_strings.html","text":"String formatting in JavaScript works very similar to other languages like Python 3. String concatenation allows for non-string variables to be incorporated into string variables. There are two basic ways of printing variables inside strings. Here is the first: // Create a variable called currentTemp let votingTime = 3 ; // Create a constant (variable) with a string that includes the value from currentTemp const message = \"Voters should go to polling station at \" + votingTime + \"pm\" ; // Print the string console . log ( message ) Voters should go to polling station at 3pm In the first way, you simply wrap the variable in strings. There is, however, a more modern way of achieving the same output: // Create a constant (variable) with a string that includes the value from currentTemp const message = `Voters should go to polling station at ${ votingTime } pm` ; // Print the string console . log ( message ) Voters should go to polling station at 3pm In the example above, instead of wrapping the variable in strings, we called the variable directly from inside a string.","tags":"Javascript","url":"http://chrisalbon.com/javascript/printing_variables_inside_strings.html"},{"title":"Group Data By Time","loc":"http://chrisalbon.com/python/pandas_group_data_by_time.html","text":"On March 13, 2016, version 0.18.0 of pandas was released, with significant changes in how the resampling function operates. This tutorial follows v0.18.0 and will not work for previous versions of pandas. First let's load the modules we care about Preliminaries # Import required packages import pandas as pd import datetime import numpy as np Next, let's create some sample data that we can group by time as an sample. In this example I am creating a dataframe with two columns with 365 rows. One column is a date, the second column is a numeric value. Create Data # Create a datetime variable for today base = datetime . datetime . today () # Create a list variable that creates 365 days of rows of datetime values date_list = [ base - datetime . timedelta ( days = x ) for x in range ( 0 , 365 )] # Create a list variable of 365 numeric values score_list = list ( np . random . randint ( low = 1 , high = 1000 , size = 365 )) # Create an empty dataframe df = pd . DataFrame () # Create a column from the datetime variable df [ 'datetime' ] = date_list # Convert that column into a datetime datatype df [ 'datetime' ] = pd . to_datetime ( df [ 'datetime' ]) # Set the datetime column as the index df . index = df [ 'datetime' ] # Create a column from the numeric score variable df [ 'score' ] = score_list # Let's take a took at the data df . head () datetime score datetime 2016-03-11 21:27:36.859714 2016-03-11 21:27:36.859714 459 2016-03-10 21:27:36.859714 2016-03-10 21:27:36.859714 153 2016-03-09 21:27:36.859714 2016-03-09 21:27:36.859714 458 2016-03-08 21:27:36.859714 2016-03-08 21:27:36.859714 310 2016-03-07 21:27:36.859714 2016-03-07 21:27:36.859714 376 Group Data By Date In pandas, the most common way to group by time is to use the .resample() function. In v0.18.0 this function is two-stage. This means that df.resample('M') creates an object to which we can apply other functions ( mean , count , sum , etc.) # Group the data by month, and take the mean for each group (i.e. each month) df . resample ( 'M' ) . mean () score datetime 2015-03-31 509.421053 2015-04-30 543.100000 2015-05-31 520.709677 2015-06-30 473.100000 2015-07-31 521.677419 2015-08-31 410.580645 2015-09-30 491.933333 2015-10-31 447.322581 2015-11-30 488.166667 2015-12-31 473.193548 2016-01-31 444.129032 2016-02-29 555.965517 2016-03-31 445.545455 # Group the data by month, and take the sum for each group (i.e. each month) df . resample ( 'M' ) . sum () score datetime 2015-03-31 9679 2015-04-30 16293 2015-05-31 16142 2015-06-30 14193 2015-07-31 16172 2015-08-31 12728 2015-09-30 14758 2015-10-31 13867 2015-11-30 14645 2015-12-31 14669 2016-01-31 13768 2016-02-29 16123 2016-03-31 4901 Grouping Options There are many options for grouping. You can learn more about them in Pandas's timeseries docs , however, I have also listed them below for your convenience. Value Description B business day frequency C custom business day frequency (experimental) D calendar day frequency W weekly frequency M month end frequency BM business month end frequency CBM custom business month end frequency MS month start frequency BMS business month start frequency CBMS custom business month start frequency Q quarter end frequency BQ business quarter endfrequency QS quarter start frequency BQS business quarter start frequency A year end frequency BA business year end frequency AS year start frequency BAS business year start frequency BH business hour frequency H hourly frequency T minutely frequency S secondly frequency L milliseconds U microseconds N nanoseconds","tags":"Python","url":"http://chrisalbon.com/python/pandas_group_data_by_time.html"},{"title":"Break A List Into N-Sized Chunks","loc":"http://chrisalbon.com/python/break_list_into_chunks_of_equal_size.html","text":"In this snippet we take a list and break it up into n-size chunks. This is a very common practice when dealing with APIs that have a maximum request size. Credit for this nifty function goes to Ned Batchelder who posted it on StackOverflow . # Create a list of first names first_names = [ 'Steve' , 'Jane' , 'Sara' , 'Mary' , 'Jack' , 'Bob' , 'Bily' , 'Boni' , 'Chris' , 'Sori' , 'Will' , 'Won' , 'Li' ] # Create a function called \"chunks\" with two arguments, l and n: def chunks ( l , n ): # For item i in a range that is a length of l, for i in range ( 0 , len ( l ), n ): # Create an index range for l of n items: yield l [ i : i + n ] # Create a list that from the results of the function chunks: list ( chunks ( first_names , 5 )) [['Steve', 'Jane', 'Sara', 'Mary', 'Jack'], ['Bob', 'Bily', 'Boni', 'Chris', 'Sori'], ['Will', 'Won', 'Li']]","tags":"Python","url":"http://chrisalbon.com/python/break_list_into_chunks_of_equal_size.html"},{"title":"What I Learned Tracking My Time At Techstars","loc":"http://chrisalbon.com/articles/what-i-learned-tracking-my-time-at-techstars.html","text":"In the fall of 2015, Popily , a data exploration company I cofounded with two friends was offered a slot in the 2016 class of Techstars Cloud . Like most people in tech, I had heard about Techstars, but in truth I barely knew anything specific, particularly about the day-to-day of the program. Was Techstars a permanent hackathon fueled by Soylent and Adderall? Was it three months of guest speakers and sponsored happy hours? I watched Techstars's promotional videos , but the only impression I could glean was that all the founders worked 36 hours per day while having ample spare time to ride bikes around downtown Boulder. I also found some posts about people's experiences, but overall I was in the dark. So, when we joined Techstars Cloud in November, I did what I was trained to do: gather data. For three months, from November 2nd, 2015 to January 31st, 2016, I tracked how I spent every 15 minutes of every day and categorized each into one of seven activities: Non-Technical Work: Email, writing, diagramming, project management, PivotalTracker , etc. Technical Work: Coding, designing, data analysis, etc. Discussion: Team meetings, speaking events, meeting with mentors or investors, Techstars happy hours, etc. Sleep Travel: Driving or flying (when I couldn't do any work) Exercise: Running (which basically never happened) Personal: Time with family, cooking, hobbies, reading, housework etc. The full data is available on Github , however in this post I used Popily.com to explore the data and understand how I spent my time during Techstars and offer a few lessons learned along the way. I hope you find it useful. POPILY.host = \"popily.com\"; var chart = POPILY.chart('date-duration-activity-33', '-'); chart.setDimensions(853, 700); chart.draw(); Lesson 1: Techstars Is For Going All-In If there was one description of my time at Techstars it was that I worked, a lot -- during evenings, lunches, and holidays; in offices, cars, and AirBnBs; alone, with my team, and while holding my daughter. The combination of the environment and the looming Demo Day made the work all-consuming. I was responsible for the product, so it is unsurprising that the largest number of my hours was spent on technical tasks. In truth, even non-technical tasks were almost always related to product development: UX testing, QAing, or PivotalTracker. My laser focus on the product was sometimes at odds with the myriad of guest speakers and mentor meetings that, while interesting and useful, could suck me in and stall development. As a company we started using two strategies to keep the product moving forward. First, most of the meetings and events were handled by my cofounders, which left me time to focus to a greater degree on the product. Second, when we had a product update or launch goal, I spent weeks away from the Techstars offices so I could get the most work done on the product as possible. POPILY.host = \"popily.com\"; var chart = POPILY.chart('date-duration-activity-33', 's8fyuf'); chart.setDimensions(853, 700); chart.draw(); On average, during Techstars I worked 91.6 hours per week. I worked the most hours during the first week of the program, where I averaged 17.4 hours of work per day. This number will be unsurprising to anyone who has going though Techstars. The first three weeks of Techstars are called Mentor Madness and can essentially be described as a full day of back to back 20 minute meetings with every fancy and impressive founder you can imagine. You walk into a room, give a five minute pitch to anyone from the CEO of a boutique ad agency to a founder of a technology giant and hear their take about you, your company, your product, your market, whatever. After 20 minutes you walk into the next room and repeat the entire process again. POPILY.host = \"popily.com\"; var chart = POPILY.chart('activity-duration-34', '-'); chart.setDimensions(853, 600); chart.draw(); Mentor Madness was a grueling experience for all of us, not only because it took up so much of our energy (it takes a lot of focus to be engaged after seven almost-back-to-back meetings), but more importantly it made us have to defend, explain, and face so many fundamental assumptions we implicitly and explicitly made about our company, product, and strategy. In one meeting a mentor would argue that the go-to-market strategy is flawed and 20 minutes later the next mentor is arguing that we need to change our company's name. Those weeks were brutal. However, it was also probably one of the most important few weeks of our company. Because at the end of all those meetings, our day would just begin; from the last meeting to late at night my cofounders and I would consider, discuss, and debate a hundred points that were brought in the day. Why did the mentor hate the pricing model? Was she right or just old-fashioned? Should we change our name? Who is our customer? All the meetings and all the challenging questions forced us to discuss things which might otherwise be assumed or left unsaid. For those first few weeks I would get up early, spend an hour emailing, discuss some point or another about strategy or product, take meetings with mentors, discuss more over lunch, take more meetings, discuss over dinner and into the evening, shower, sleep, and start the whole thing over again. Two days during Mentor Madness contained a massive 13 and 14 hours of discussions. Those conversations alone made Techstars worthwhile. Unsurprisingly, the week with the least work was Christmas when I was traveling with my family and would only be able to sneak in a few hours of work per day after everyone fell asleep. Lesson 2: The Start Of Techstars Is About Strategy, The End Is About Execution Our company was not alone is spending the first weeks of the program focusing on planning and strategy. I think most of the companies in our class spent the first few weeks either going back to the drawing board or mapping out a plan ahead. However, in December, the mid-point of the program there was a general shift from planning to execution. Looking at hours spent on between technical and non-technical tasks, Techstars could be divided into two periods, before Christmas and after Christmas. In the first half of the program it felt like I spent every moment I wasn't in meetings writing, updating, or managing user stories. We had a few contract developers and I agonized over the ordering of user stories in PivotalTracker as to maximize the value we would get from every hour of their time. However, during the second half of the problem the planning took a back seat to building. POPILY.host = \"popily.com\"; var chart = POPILY.chart('date-duration-activity-33', 'z00udv'); chart.setDimensions(853, 700); chart.draw(); Between Christmas and our launch on January 19th was probably my happiest in the program, because it was a wonderful coding crunchtime. I would get up, select the next user story in the queue, complete it, QA, and repeat. It was -- frankly -- just fun. All the discussions around marketing, fundraising, or ten thousand other things were put aside (i.e. handled by my cofounders), leaving me free to build. This switch from planning to building is beautifully seen in the data: in the first half of the program I spent almost all my time on planning and strategy and in the second half I spent almost all my time building. While non-technical and technical work peaked around the beginning and the end of the program, meetings, networking, and discussions continued throughout. As I said previously, the Mentor Madness of the first three weeks took up a massive amount of time at the start of the program, but after those weeks my cofounders and I actively tried to give each other the space to get stuff done. Our primary tactic was to have regular short(ish) \"State Of The Union calls almost every day, leaving the major discussions to multi-day planning sessions every few weeks. These discussion can be easily seen as peaks in the chart below. This system might not work well for everyone, but it gave us the right balance of time to work out problems and time to get shit done. POPILY.host = \"popily.com\"; var chart = POPILY.chart('date-duration-activity-33', 'xpd6ys'); chart.setDimensions(853, 700); chart.draw(); Did all those hours matter? Completely and absolutely. Over the course of the program the product went from a tricycle to a Harley Davidson. There is more to do, but the leaps and bounds made during Techstars has been incredible. Lesson 3: Sleep Is Important! Kinda. As soon as I started tracking my time, I knew the first question everyone was going to ask: \"how much did you sleep?\" and because of the power of data I can give you a real answer: 5.5 hours of sleep per night. I will admit I was pretty pleased when I saw that number because while everyone at Techstars works their asses off 4.5 to 6.5 hours of sleep was my normal amount prior to Techstars. That said, on those few nights I received far less sleep than I needed my productivity suffered the next day. Looking at the data below, it is also notable to see that the amount of sleep I got every night remained relatively stable throughout the program. The only two deviations are the two times I made the twelve hour drive from my house to the Techstars offices overnight and the moderate dip in sleep I got in early January when I was enjoying my late night coding sessions. POPILY.host = \"popily.com\"; var chart = POPILY.chart('date-duration-activity-33', '2btet9'); chart.setDimensions(853, 700); chart.draw(); Lesson 4: There Is No Work-Life Balance I would love to say there was great work-life balance, but in reality was not any sort of work-life balance. There was regular fun events (e.g. happy hours, runs, etc.) during the program, but that isn't work-life balance, that is fun work events. The truth is that all the benefits of Techstars, the rapid advance of our product, the boost in users, the improved strategy, and everything else came at a price -- that for three months you lived your work and everything else from family to kids to friends took a backseat. During Techstars my average daily \"free time\" (i.e. not working or sleeping) was 2.7 hours. That is 2.7 hours per day for everything, from Christmas mornings to birthday parties to doctors appointments to cooking dinner. I am not complaining. I like working, and I am in a place in my life that I can let work take over for a while, but make no mistake: it did take over. I'd argue that to really get value from Techstars, it probably has to take over. POPILY.host = \"popily.com\"; var chart = POPILY.chart('date-duration-free-time-3', '-'); chart.setDimensions(853, 700); chart.draw(); As a more concrete example to the point above, of the three major holidays that took place during Techstars Cloud two of them were just a regular work day for me. POPILY.host = \"popily.com\"; var chart = POPILY.chart('holiday-duration-activity-2', '-'); chart.setDimensions(853, 700); chart.draw(); Final Thoughts On Techstars It has been incredibly valuable to every aspect of Popily , from product to strategy. The advice been useful, the connections have been valuable, and Demo Day (February 11th) will be awesome. However, at the end of the day none of that was why I enjoyed Techstars. The real reason is that for three months I worked with people I enjoy for long hours on hard problems -- and everything else took a back seat. That might not appeal to many people, but it appeals to me. In truth, not once during Techstars did I feel like I was \"working\". Rather, I was with my friends, trying to do something genuinely hard -- and I can't ask for more than that. (All the charts in this article were made using Popily. Want to create you own? Visit Popily.com Want to follow us founders? We are @chrisalbon , @vid_spandana , and @jonathonmorgan on Twitter.)","tags":"Articles","url":"http://chrisalbon.com/articles/what-i-learned-tracking-my-time-at-techstars.html"}]}