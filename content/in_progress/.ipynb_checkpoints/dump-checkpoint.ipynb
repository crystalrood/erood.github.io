{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pprint\n",
    "from nltk import Tree\n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv('clean_data/survey_data.csv')\n",
    "\n",
    "patterns = \"\"\"\n",
    "    NP: {<JJ>*<NN*>+}\n",
    "    {<JJ>*<NN*><CC>*<NN*>+}\n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "def find_nps(text):\n",
    "    prepared = prepare_text(text)\n",
    "    parsed = parsed_text_to_NP(prepared)\n",
    "    return parsed\n",
    "\n",
    "def list_of_noun_phrases(series):\n",
    "\n",
    "    nouns = []\n",
    "\n",
    "    for row in series.dropna():\n",
    "        nouns.append(find_nps(row))\n",
    "        \n",
    "    list_of_nouns = [i.lower() for row in nouns for i in row]\n",
    "    \n",
    "    list_of_list_of_nouns = [[x[0]] * x[1] for x in Counter(list_of_nouns).most_common(30)]\n",
    "    \n",
    "    flattened_list_of_nouns = [i.lower() for row in list_of_list_of_nouns for i in row]\n",
    "    \n",
    "    titlecased_list_of_nouns = [x.title() for x in flattened_list_of_nouns]\n",
    "    \n",
    "    return titlecased_list_of_nouns\n",
    "\n",
    "df_helped_women = pd.DataFrame()\n",
    "df_helped_women['What Helps Women?'] = list_of_noun_phrases(df['What Helps Women?'])\n",
    "\n",
    "df_barriers = pd.DataFrame()\n",
    "df_barriers['What Are Barriers To Women?'] = list_of_noun_phrases(df['What Are Barriers To Women?'])\n",
    "\n",
    "df_helped_you = pd.DataFrame()\n",
    "df_helped_you['What Helped You The Most?'] = list_of_noun_phrases(df['What Helped You The Most?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_zip_file(filepath):\n",
    "    \"\"\"Opens the Google Sheet exported zip files\"\"\"\n",
    "    file = zipfile.ZipFile(filepath)\n",
    "    for filename in file.namelist():\n",
    "        if '.html' in filename:\n",
    "            ifile = file.open(filename)\n",
    "            content = ifile.read()\n",
    "            return content\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import isodate\n",
    "import requests\n",
    "import collections\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "\n",
    "video_json_results = []\n",
    "\n",
    "for video_id in youtube_ids:\n",
    "    \n",
    "    videos_api = 'https://www.googleapis.com/youtube/v3/videos'\n",
    "    youtube_id = video_id\n",
    "    api_key = 'AIzaSyD60FFhwu15CRgXZE_KBaOxrct1oumVRhI'\n",
    "    query_parts = 'snippet,contentDetails,statistics,topicDetails'\n",
    "\n",
    "    url = '{0}?id={1}&key={2}&part={3}'.format(videos_api, youtube_id, api_key, query_parts)\n",
    "\n",
    "    feed = urllib.request.urlopen(url)\n",
    "    feed = feed.read().decode('utf-8')\n",
    "    video_json = json.loads(feed)\n",
    "    \n",
    "    video_json_results.append(video_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_language_google(string):\n",
    "    try:\n",
    "        string = string\n",
    "        google_api_key = 'AIzaSyD60FFhwu15CRgXZE_KBaOxrct1oumVRhI'\n",
    "\n",
    "        url = 'https://www.googleapis.com/language/translate/v2/detect?q={0}&key={1}'.format(string, google_api_key)\n",
    "\n",
    "        r = requests.get(url)\n",
    "\n",
    "        data = r.json()\n",
    "\n",
    "        confidence = data['data']['detections'][0][0]['confidence']\n",
    "        language_code = data['data']['detections'][0][0]['language']\n",
    "\n",
    "        country_object = pycountry.languages.get(iso639_1_code=language_code)\n",
    "\n",
    "        if confidence >= .70:\n",
    "            return (language_code, country_object.name)\n",
    "        else:\n",
    "            return (np.nan, np.nan)\n",
    "    except:\n",
    "        return (np.nan, np.nan)\n",
    "    \n",
    "    google_language_detect_data = [detect_language_google(x) for x in df_comments['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate_language(string, language):\n",
    "    string = string\n",
    "    source_lang = language\n",
    "    google_api_key = 'AIzaSyD60FFhwu15CRgXZE_KBaOxrct1oumVRhI'\n",
    "\n",
    "    url = 'https://www.googleapis.com/language/translate/v2?q={0}&target=en&format=text&source={1}&key={2}'.format(string, source_lang, google_api_key)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    return r.json()\n",
    "\n",
    "translated_text = []\n",
    "\n",
    "# For each row and index in all the comments:\n",
    "for index, row in df_comments.iterrows():\n",
    "    # Try to,\n",
    "    try:\n",
    "        # Make a Google Translate API Query\n",
    "        data = translate_language(row['Text'], row['Language Code'])\n",
    "        # Try to\n",
    "        try:\n",
    "            # Find the translation if it exists\n",
    "            translated_text.append(data['data']['translations'][0]['translatedText'])\n",
    "        # If get an error:\n",
    "        except Exception as e:\n",
    "            # If the error is this, then use the original text\n",
    "            if data['error']['message'] == 'Bad language pair: en|en':\n",
    "                translated_text.append(row['Text'])\n",
    "            # If the error is this,\n",
    "            elif data['error']['message'] == 'Invalid Value':\n",
    "                # Then mark as missing data\n",
    "                translated_text.append(np.nan)\n",
    "            else:\n",
    "                print('Error:', data, str(index))\n",
    "                translated_text.append(np.nan)\n",
    "    # If you get an error\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "        # Then mark as missing data\n",
    "        translated_text.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import isodate\n",
    "from bs4 import BeautifulSoup\n",
    "import bleach\n",
    "import re\n",
    "import requests\n",
    "import langdetect\n",
    "import pycountry\n",
    "import json\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import dateutil.parser\n",
    "from datetime import date, datetime\n",
    "import pytz\n",
    "\n",
    "def sentiment(x):\n",
    "    try:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        return sid.polarity_scores(x)['compound']\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_datetime_variables(x):\n",
    "    try:\n",
    "        year = dateutil.parser.parse('2016-05-16T17:12:54.000Z').year\n",
    "        month = dateutil.parser.parse('2016-05-16T17:12:54.000Z').month\n",
    "        day = dateutil.parser.parse('2016-05-16T17:12:54.000Z').day\n",
    "        weekday = dateutil.parser.parse('2016-05-16T17:12:54.000Z').strftime(\"%A\")\n",
    "        return year, month, day, weekday\n",
    "    \n",
    "    except:\n",
    "        return np.nan, np.nan, np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [('Winter', (date(Y,  1,  1),  date(Y,  3, 20))),\n",
    "           ('Spring', (date(Y,  3, 21),  date(Y,  6, 20))),\n",
    "           ('Summer', (date(Y,  6, 21),  date(Y,  9, 22))),\n",
    "           ('Autumn', (date(Y,  9, 23),  date(Y, 12, 20))),\n",
    "           ('Winter', (date(Y, 12, 21),  date(Y, 12, 31)))]\n",
    "\n",
    "def get_season(now):\n",
    "        if isinstance(now, datetime):\n",
    "            now = now.date()\n",
    "        now = now.replace(year=Y)\n",
    "        return next(season for season, (start, end) in seasons\n",
    "                    if start <= now <= end)\n",
    "    \n",
    "season = []\n",
    "\n",
    "for row in df_videos['Published']:\n",
    "    if type(row) == float:\n",
    "        season.append(np.nan)\n",
    "    else:\n",
    "        season.append(get_season(dateutil.parser.parse(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import urllib\n",
    "from time import gmtime, strftime\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import sys\n",
    "\n",
    "CONSUMER_KEY = 'MhDddOlABwBv41X2RMQAdsIQD'\n",
    "CONSUMER_SECRET = 'OwTjwXxCSgBBEzyOQyxa7uwBhFpJzX5OXcBNWMnnbwGUnrInuT'\n",
    "ACCESS_KEY = '3326958650-TYEjXkLW7V3VC805oM6KBOuv24bepyaNYLkL9f0'\n",
    "ACCESS_SECRET = 'FWnsnK5FqaUWIHXVzTxikOptklVQSVklW28UdKdiL9LR3'\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_KEY, ACCESS_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "data = urllib.urlopen('http://export.arxiv.org/api/query?search_query=all&sortBy=lastUpdatedDate&sortOrder=descending&max_results=25').read()\n",
    "soup = BeautifulSoup(data)\n",
    "hashtags = ['science', 'harrypotter']\n",
    "current_hour_gmt = int(strftime(\"%H\", gmtime()))\n",
    "entry = soup.findAll('entry')[current_hour_gmt-1]\n",
    "title = entry.find('title').string.replace('\\n ', '')\n",
    "\n",
    "question_words = ['Who', 'What', 'When', 'Where', 'Why', 'Does', 'How', 'Whom', 'Whose', 'Which']\n",
    "\n",
    "if title.split(\" \")[0] in question_words or title[-1:] == '?':\n",
    "    sys.exit(\"Title is likely a question, abandoning this run\")\n",
    "    \n",
    "harry_prefix = 'Harry Potter And The'\n",
    "title = str(title).title().replace('On ','').replace('From ','').replace('The ','')\n",
    "link = str(entry.find('link').get('href'))\n",
    "hashtag = '#' + random.choice(hashtags)\n",
    "\n",
    "tweet = harry_prefix + ' ' + title + ' ' + link + ' ' + hashtag\n",
    "\n",
    "if len(tweet) > 140:\n",
    "    tweet = harry_prefix + ' ' + title + ' ' + link\n",
    "    if tweet > 140:\n",
    "        tweet = harry_prefix + ' ' + title.split(':')[0] + ' ' + link\n",
    "            if tweet > 140:\n",
    "                sys.exit(\"Tweet too long and I can't shorten it.\")\n",
    "                \n",
    "# api.update_status(tweet)\n",
    "print(tweet)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
